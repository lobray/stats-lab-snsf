---
title: "Board Report"
author: "Leslie O'Bray"
date: "April 22, 2018"
output: pdf_document
---

```{r}
#################################################################
###########  Regression Analysis - Board reviews  ###############
#################################################################

library(vcd) 
library(corrplot)
library(caTools)
library(ROCR)
library(pROC)
library(car)
library(caret)
library(glmnet)

### Initialize data

load("/home/leslie/Desktop/StatsLab/snsf_data.RData")
source("/home/leslie/Desktop/StatsLab/stats-lab-snsf/Cleaning Functions.R")
source("/home/leslie/Desktop/StatsLab/stats-lab-snsf/Data for Regression.R")

```


Prepare the data and fit the full model:
```{r}

board_data <- prepare_data_board_log_regression(final.apps, internal = final.internal, external = final.external)
board_data$Ranking <- factor(board_data$Ranking,ordered=TRUE)
board_data$OverallGrade <- factor(board_data$Ranking, ordered=TRUE)


board_log_regression <- glm(board_data$IsApproved ~ Gender + Division + Age + IsContinuation + InstType + AmountRequested + 
                              Ranking + OverallGrade + Gender:Division + PercentFemale, family="binomial", data = board_data)


```


In fitting the regression, the summary shos that OverallGrade, Ranking, InstType and Age are all significant predictors.
```{r}
(summary(board_log_regression))
```


In checking for correlation among coefficients, we only don't see any alarming values (VIF > 5). 
```{r}
(vif(board_log_regression))
```


Check VariableImportance. We see that Ranking is orders of magnitude more impactful than any of the other variables.

First, calculate Pseudo R^2, create a function:

```{r}
# NOTE TO LESLIE: confirm pseudo r^2 metric

calc_pseudo_r <- function(log_regression_object) {
  (log_regression_object$null.deviance - log_regression_object$deviance) /
          (log_regression_object$df.null + log_regression_object$null.deviance - log_regression_object$deviance)

}

# Check pseudo r for model: 
calc_pseudo_r(board_log_regression)
```


```{r}
## IF GOING TO USE VARIMP, NEED TO UPDATE TO USE STANDARDIZED VARIABLES. BUT, WHAT DOES A STANDARDIZED BINARY VARIABLE EVEN MEAN? CHECK IF USEMODEL=F IS SUFFICIENT
(board_variable_importance <- varImp(board_log_regression, useModel=FALSE))
(board_variable_importance <- varImp(board_log_regression, nonpara=TRUE))



```

Alternative Method for seeing variable importance: For each predictor, permute the values. See difference in fit.



```{r permute data }

predictors <- c("Gender", "Division", "Age", "IsContinuation", "InstType", "AmountRequested", 
                "Ranking", "OverallGrade")

# Function to permute values
permute_values <- function(data_vector) {
  n <- length(data_vector)
  permuted_vector <- sample(data_vector, n, replace=F)
}


# Function to calculate variable importance via permutation
calc_variable_importance <- function(predictors = predictors, data = board_data, regression_object) {
  variable_imp_matrix <- matrix(0, nrow=length(predictors), ncol=2)
  variable_imp_matrix[,1] <- predictors
  base_pseudo_r <- calc_pseudo_r(regression_object)
  
  for (i in predictors) {
    permuted_board_data <- data
    permuted_predictor <- permute_values(permuted_board_data[,i])
    
    permuted_board_data[,i] <- permuted_predictor
    
    # run regression on permuted value
    permuted_board_log_regression <- glm(permuted_board_data$IsApproved ~ Gender + Division + Age +
                                           IsContinuation + InstType + AmountRequested +
                                           Ranking + OverallGrade + Gender:Division, family="binomial", data = permuted_board_data)
    
    
    # calculate pseudo r squared
    permuted_pseudo_r <- calc_pseudo_r(permuted_board_log_regression)
    variable_imp_matrix[which(variable_imp_matrix[,1] == i),2] <- (permuted_pseudo_r - base_pseudo_r)
  } 
  colnames(variable_imp_matrix) <- c("Variable", "Difference to Base Pseudo R^2")
  return(variable_imp_matrix)
}


aa <- as.data.frame(calc_variable_importance(predictors=predictors, data=board_data, regression_object=board_log_regression))

```
In this step, for each explanatory variable, I randomly permuted the values for that variable, and refitted the logistic regression model. I then compared the pseudo R^2 metric with the initial pseudo R^2 metric computed in the original model. In the output matrix, you can see how much the pseudo R^2 changed when that variable was permuted. In this case, we see that permuting Ranking had the biggest impact on the pseudo R^2 - decreasing it by 0.16. The next biggest impact was overall grade, which decreased the pseudo R^2 0.003. From this, we can conclude that Ranking is the most important explanatory variable in predicting IsApproved.

**QUESITON** Is my definition of pseudo R^2 okay? Make sure to be consistent across our work. 

Check diagnostics: residuals:
```{r}
plot(resid(board_log_regression, type="pearson")) 
plot(resid(board_log_regression, type="deviance"))

```

Checking the residuals, they seem to be expectation 0, with a few outliers. However, in looking at the deviance, there appears to be some structure in the data. Should this indicate we need a new model fit?

Do a bit of variable selection in order to optimize the AIC criterion:

```{r}
drop1(board_log_regression, test="F")
fit1 <- update(board_log_regression, .~.-InstType)
drop1(fit1, test="F")
fit2 <- update(fit1, .~.-Gender:Division)
drop1(fit2, test="F")
fit3 <- update(fit2, .~.-Division)
drop1(fit3)
fit4 <- update(fit3, .~.-AmountRequested)
drop1(fit4)

```

I stopped when removing Gender became the next target.

Do CV to see if model fit is decent:
```{r}
#####################################################################
#################### CV to assess model fit ##########################
#####################################################################


assess_board_model<- function(data=board_data, Div="All",
                      SplitRatio=0.8, cutoff=0.5 ){ 
  
  if (Div == "All"){ 
    final.data <- data
  }
  else {
    final.data<- subset(data,Division==Div, select = -(Division)) 
  }
  
  ### spliting the data into train and test set
  
  if (SplitRatio<1){
    split<-sample.split(final.data$IsApproved, SplitRatio = SplitRatio)
    Train<-subset(final.data, split=="TRUE")
    Test <-subset(final.data, split=="FALSE") 
  } else {
    Test<-Train<-final.data
  }
  
  #### fitting the model
  
  # Cutoff
  cutoff <- cutoff
  
  # Optimize the model
  
  Model <- glm(Train$IsApproved ~ .-(ProjectID),data=Train, 
               family="binomial")
  
  ### Testing the Treshold
  par(mfrow=c(1,2))
  predictor<-predict(Model, Test, type="response")
  ROCRPred<- prediction(predictor,Test$IsApproved)
  ROCRPerf<- performance(ROCRPred,"tpr","fpr")
  plot(ROCRPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1))
  
  ### with respect to accuracy
  ROCRACC<- performance(ROCRPred,"acc")
  plot(ROCRACC)  
  
  ### Confusion Matrices
  AccTable<-table(ActualValue=Test$IsApproved,Prediction=predictor>=cutoff)
  accuracy<-(sum(diag(AccTable))/sum(AccTable))
  
  ### Return
  print(paste("Regresion for External Reviews.   ", "Division: ", Div))
  return(list(Model= summary(Model), 
              #`Confidence Intervals`=confint(Model),
              `Confusion Matrix`=AccTable,
              `Percentage of data used for Training`=paste(SplitRatio*100,"%"),
              `Accuracy`=paste(round(accuracy,2)*100,"%")))
}


board.all <- assess_board_model(data=board_data, Div="All",
                        SplitRatio=1,cutoff=0.5)
board.all$Accuracy

```



