---
title: "Board Report"
author: "Leslie O'Bray"
date: "April 22, 2018"
output: pdf_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#################################################################
###########  Regression Analysis - Board reviews  ###############
#################################################################

library(vcd) 
library(corrplot)
library(caTools)
library(ROCR)
library(pROC)
library(car)
library(caret)
library(glmnet)
library(combinat)
library(psych)
library(biostatUZH)

### Initialize data

load("/home/leslie/Desktop/StatsLab/snsf_data.RData")
source("/home/leslie/Desktop/StatsLab/stats-lab-snsf/Cleaning Functions.R")
source("/home/leslie/Desktop/StatsLab/stats-lab-snsf/Data for Regression.R")

```


Prepare the data and fit the full model:
```{r, echo=FALSE, warning=FALSE}

board_data <- prepare_data_board_log_regression(apps=final.apps, internal = final.internal, external = final.external)

kappa.matrix <- table(board_data$OverallGrade, board_data$Ranking)
kappa.matrix.boardranking <- table(board_data$GradeFinal, board_data$Ranking)
kappa.applicant <- table(board_data$ExternalApplicantTrack, board_data$InternalApplicantTrack)
kappa.proposal <- table(board_data$ProposalCombined, board_data$ProjectAssessment)

custom.weights <- matrix(c(1, 0.8, 0.6, 0, 0, 0, 0, 1, 0.8, 0.6, 0, 0, 
         0.6, 0.8, 1, 0.8, 0.6, 0, 0, 0.6, 0.8, 1, 0.8, 0.6,
         0, 0, 0.6, 0.8, 1, 0.8, 0, 0, 0, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)
300/(sum(kappa.applicant))
pobs <- sum(diag(kappa.applicant))/sum(kappa.applicant)
pex <- 
kappa <- cohen.kappa(kappa.matrix, w=custom.weights)
# table(board_data$OverallGrade, board_data$IsApproved)
print("Cohen's Kappa, Ranking & OverallGrade")
print(kappa)

board_data$Ranking <- ifelse(board_data$Ranking %in% c(1,2,3), 3, board_data$Ranking)
board_data$Ranking <- factor(board_data$Ranking, ordered=T)
board_data$OverallGrade <- ifelse(board_data$OverallGrade %in% c(1,2,3), 3, board_data$OverallGrade)
board_data$OverallGrade <- factor(board_data$OverallGrade, ordered=T)

str(board_data)
### TEST COMMENT OUT LATER
board_data$Ranking <- factor(board_data$Ranking, ordered=F)
board_data$OverallGrade <- factor(board_data$OverallGrade, ordered=F)
########

board_log_regression <- glm(board_data$IsApproved ~ Gender + Division + Age + IsContinuation + InstType + log(AmountRequested) + 
                              Ranking + OverallGrade + Gender:Division + PercentFemale + 
                              PercentFemale:Gender, family="binomial", data = board_data)

```


In fitting the regression, the summary shos that OverallGrade, Ranking, InstType and Age are all significant predictors.
```{r, echo=FALSE, warning=FALSE}
(summary(board_log_regression))



```


In checking for correlation among coefficients, we only don't see any alarming values (VIF > 5). However, gender is nearly 5 (4.99), so we would like to investigate the correlation among variables. 

```{r, echo=FALSE, warning=FALSE}
(vif(board_log_regression))

x <- model.matrix(board_log_regression)
x <- x[,-1]
Xc <- cor(x)


# Plot the correlation matrix  
# correlation test
# mat : is a matrix of data
# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(x,method="spearman")
head(p.mat[, 1:5])
```

```{r, echo=FALSE, warning=FALSE}
corrplot(Xc, type="upper", order="hclust", 
         p.mat = p.mat, sig.level = 0.01,insig = "blank")
```


Check VariableImportance. We see that Ranking is orders of magnitude more impactful than any of the other variables.

First, calculate Pseudo R^2, create a function:

```{r, echo=FALSE, warning=FALSE}


calc_pseudo_r <- function(log_regression_object) {
  n <- dim(log_regression_object$data)[1]
  pseudo <- (1 - exp((log_regression_object$dev - log_regression_object$null)/n)) / (1-exp(-log_regression_object$null/n))
  return(pseudo)
}

# Check pseudo r for model: 
base_pseudo_r <- calc_pseudo_r(board_log_regression)


```

Alternative Method for seeing variable importance: For each predictor, permute the values. See difference in fit.



```{r permute data, echo=FALSE, warning=FALSE }

predictorNames <- c("Gender", "Division", "Age", "IsContinuation", "InstType", "AmountRequested", 
                "Ranking", "OverallGrade", "PercentFemale")

# predictorNames <- c("Gender", "Division", "Age", "IsContinuation", "InstType", "AmountRequested", 
#                 "Ranking", "OverallGrade", "PercentFemale", "InternalApplicantTrack", "ExternalApplicantTrack",
#                 "ProposalCombined", "ProjectAssessment")

refR2<-calc_pseudo_r(board_log_regression)
  PseudoRShuffle <- NULL
  shuffletimes <- 100  #number of interactions
  
  featuresMeanR2 <- c()
  for (feature in predictorNames) {
    featureR2 <- c()
    shuffledData <- board_data
    for (iter in 1:shuffletimes) {
      shuffledData[,feature] <- sample(shuffledData[,feature], length(shuffledData[,feature]))
      Model.tmp <- update(board_log_regression,.~.,data=shuffledData)
      featureR2 <- c(featureR2,calc_pseudo_r(Model.tmp))
    }
    featuresMeanR2 <- c(featuresMeanR2, mean(featureR2-refR2))
  }  
  
  PseudoRShuffle <- data.frame('feature'=predictorNames, 'importance'=featuresMeanR2)
  PseudoRShuffle <- PseudoRShuffle[order(PseudoRShuffle$importance, decreasing=TRUE),]
  print(PseudoRShuffle)


```
In this step, for each explanatory variable, I randomly permuted the values for that variable, and refitted the logistic regression model. I then compared the pseudo R^2 metric with the initial pseudo R^2 metric computed in the original model. In the output matrix, you can see how much the mean pseudo R^2 changed when that variable was permuted. In this case, we see that permuting Ranking had the biggest impact on the pseudo R^2 - decreasing it by 0.16. The next biggest impact was overall grade, which decreased the pseudo R^2 0.003. From this, we can conclude that Ranking is the most important explanatory variable in predicting IsApproved.



Check diagnostics: residuals. We see that we have smaller residuals for ranking 6 and 5, which intuitively makes sense that they get funded with consistency, and similarly we 1-3 dont get funded, and thus are also classified correctly. Our resiudals with level 4 have the largest average deviance, and we have a few large outliers with 3 & 5, which likely means a 3 got funded, or a 5 did not get funded, despite our classification of the opposite.  

```{r, echo=FALSE, warning=FALSE}

plot(resid(board_log_regression, type="deviance"), col=board_data$Ranking)
legend("topright", legend = 3:6, col=3:6, pch=1)

plot(resid(board_log_regression, type="pearson"), col=board_data$Ranking)

```

Checking the residuals, they seem to be expectation 0, with a few outliers. However, in looking at the deviance, there appears to be some structure in the data, explained by Ranking.

```{r, echo=FALSE, warning=FALSE}
xx <- predict(board_log_regression, type="link")
yy <- residuals(board_log_regression, type="deviance")
plot(xx, yy, pch=20, main="Tukey-Anscombe Plot")
# lines(loess.smooth(xx, yy, family="Gaussian"), col="red")
abline(h=0, lty=3, col="grey")

```



Do a bit of variable selection in order to optimize the AIC criterion:

```{r, echo=FALSE, warning=FALSE}
drop1(board_log_regression, test="Chisq")
fit1 <- update(board_log_regression, .~.-InstType)
drop1(fit1, test="Chisq")
fit2 <- update(fit1, .~.-Gender:Division)
drop1(fit2, test="Chisq")
fit3 <- update(fit2, .~.-Gender:PercentFemale)
drop1(fit3, test="Chisq")
fit4 <- update(fit3, .~.-PercentFemale)
drop1(fit4, test="Chisq")
fit5 <- update(fit4, .~.-Gender)
drop1(fit5, test="Chisq")
fit6 <- update(fit5, .~.-log(AmountRequested))
drop1(fit6, test="Chisq")
fit7 <- update(fit6, .~.-Division)
drop1(fit7)



print("PseudoR^2 in Smaller Model")
(small_model_pseudo_r <- calc_pseudo_r(fit7))

print("PseudoR^2 in Full Model")
base_pseudo_r

```
The final model purely uses Age, IsContinuation, Ranking, & OverallGrade as predictors. Since the pseudo R^2 in the small model is nearly identical to the pseudo R^2 in the full model, we prefer the smaller model. 

Let's check variable importance in the smaller model:
```{r}

predictorNames <- c("Age", "IsContinuation", "Ranking", "OverallGrade")

refR2<-calc_pseudo_r(fit7)
  PseudoRShuffle <- NULL
  shuffletimes <- 10000  #number of interactions
  
  featuresMeanR2 <- c()
  for (feature in predictorNames) {
    featureR2 <- c()
    shuffledData <- board_data
    for (iter in 1:shuffletimes) {
      shuffledData[,feature] <- sample(shuffledData[,feature], length(shuffledData[,feature]))
      Model.tmp <- update(fit7,.~.,data=shuffledData)
      featureR2 <- c(featureR2,calc_pseudo_r(Model.tmp))
    }
    featuresMeanR2 <- c(featuresMeanR2, mean(featureR2-refR2))
  }  
  
  PseudoRShuffle <- data.frame('feature'=predictorNames, 'importance'=featuresMeanR2)
  PseudoRShuffle <- PseudoRShuffle[order(PseudoRShuffle$importance, decreasing=TRUE),]
  print(PseudoRShuffle)

  ```
