---
title: "Board Report"
author: "Leslie O'Bray"
date: "April 22, 2018"
output: pdf_document
---

```{r}
#################################################################
###########  Regression Analysis - Board reviews  ###############
#################################################################

library(vcd) 
library(corrplot)
library(caTools)
library(ROCR)
library(pROC)
library(car)
library(caret)
library(glmnet)
library(combinat)
library(psych)
library(biostatUZH)

### Initialize data

load("/home/leslie/Desktop/StatsLab/snsf_data.RData")
source("/home/leslie/Desktop/StatsLab/stats-lab-snsf/Cleaning Functions.R")
source("/home/leslie/Desktop/StatsLab/stats-lab-snsf/Data for Regression.R")

```


Prepare the data and fit the full model:
```{r}

board_data <- prepare_data_board_log_regression(apps=final.apps, internal = final.internal, external = final.external)

kappa.matrix <- table(board_data$OverallGrade, board_data$Ranking)

custom.weights <- matrix(c(1, 0.8, 0.6, 0, 0, 0, 0, 1, 0.8, 0.6, 0, 0, 
         0.6, 0.8, 1, 0.8, 0.6, 0, 0, 0.6, 0.8, 1, 0.8, 0.6,
         0, 0, 0.6, 0.8, 1, 0.8, 0, 0, 0, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)

kappa <- cohen.kappa(kappa.matrix)
table(board_data$OverallGrade, board_data$IsApproved)


board_data$Ranking <- ifelse(board_data$Ranking %in% c(1,2,3), 3, board_data$Ranking)
board_data$OverallGrade <- ifelse(board_data$OverallGrade %in% c(1,2,3), 3, board_data$OverallGrade)


board_log_regression <- glm(board_data$IsApproved ~ Gender + Division + Age + IsContinuation + InstType + log(AmountRequested) + 
                              Ranking + OverallGrade + Gender:Division + PercentFemale + 
                              PercentFemale:Gender, family="binomial", data = board_data)


```


In fitting the regression, the summary shos that OverallGrade, Ranking, InstType and Age are all significant predictors.
```{r}
(summary(board_log_regression))


```


In checking for correlation among coefficients, we only don't see any alarming values (VIF > 5). However, gender is nearly 5 (4.99), so we would like to address this. 
```{r}
(vif(board_log_regression))
```


Check VariableImportance. We see that Ranking is orders of magnitude more impactful than any of the other variables.

First, calculate Pseudo R^2, create a function:

```{r}


calc_pseudo_r <- function(log_regression_object) {
  n <- dim(log_regression_object$data)[1]
  pseudo <- (1 - exp((log_regression_object$dev - log_regression_object$null)/n)) / (1-exp(-log_regression_object$null/n))
  return(pseudo)
}

# Check pseudo r for model: 
base_pseudo_r <- calc_pseudo_r(board_log_regression)


```

Alternative Method for seeing variable importance: For each predictor, permute the values. See difference in fit.



```{r permute data }

predictorNames <- c("Gender", "Division", "Age", "IsContinuation", "InstType", "AmountRequested", 
                "Ranking", "OverallGrade", "PercentFemale")

refR2<-calc_pseudo_r(board_log_regression)
  PseudoRShuffle <- NULL
  shuffletimes <- 50  #number of interactions
  
  featuresMeanR2 <- c()
  for (feature in predictorNames) {
    featureR2 <- c()
    shuffledData <- board_data
    for (iter in 1:shuffletimes) {
      shuffledData[,feature] <- sample(shuffledData[,feature], length(shuffledData[,feature]))
      Model.tmp <- update(board_log_regression,.~.,data=shuffledData)
      featureR2 <- c(featureR2,calc_pseudo_r(Model.tmp))
    }
    featuresMeanR2 <- c(featuresMeanR2, mean(featureR2-refR2))
  }  
  
  PseudoRShuffle <- data.frame('feature'=predictorNames, 'importance'=featuresMeanR2)
  PseudoRShuffle <- PseudoRShuffle[order(PseudoRShuffle$importance, decreasing=TRUE),]
  print(PseudoRShuffle)


```
In this step, for each explanatory variable, I randomly permuted the values for that variable, and refitted the logistic regression model. I then compared the pseudo R^2 metric with the initial pseudo R^2 metric computed in the original model. In the output matrix, you can see how much the mean pseudo R^2 changed when that variable was permuted. In this case, we see that permuting Ranking had the biggest impact on the pseudo R^2 - decreasing it by 0.16. The next biggest impact was overall grade, which decreased the pseudo R^2 0.003. From this, we can conclude that Ranking is the most important explanatory variable in predicting IsApproved.



Check diagnostics: residuals. We see that we have smaller residuals for ranking 6 and 5, which intuitively makes sense. But this is a violation of our assumption of independently and identically distributed residuals, as there is clear structure in the residuals.
```{r}

plot(resid(board_log_regression, type="deviance"), col=board_data$Ranking)
legend("topright", legend = 1:6, col=1:6, pch=1)
```

Checking the residuals, they seem to be expectation 0, with a few outliers. However, in looking at the deviance, there appears to be some structure in the data. Should this indicate we need a new model fit?

Do a bit of variable selection in order to optimize the AIC criterion:

```{r}
drop1(board_log_regression, test="Chisq")
fit1 <- update(board_log_regression, .~.-InstType)
drop1(fit1, test="Chisq")
fit2 <- update(fit1, .~.-Gender:Division)
drop1(fit2, test="Chisq")
fit3 <- update(fit2, .~.-Gender:PercentFemale)
drop1(fit3, test="Chisq")
fit4 <- update(fit3, .~.-PercentFemale)
drop1(fit4, test="Chisq")
fit5 <- update(fit4, .~.-Gender)
drop1(fit5, test="Chisq")
fit6 <- update(fit5, .~.-log(AmountRequested))
drop1(fit6, test="Chisq")
fit7 <- update(fit6, .~.-Division)
drop1(fit7)

```
The final model purely uses Age, IsContinuation, Ranking, & OverallGrade as predictors. 




