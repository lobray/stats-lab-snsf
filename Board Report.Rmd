---
title: "Board Report"
author: "Leslie O'Bray"
date: "April 22, 2018"
output: pdf_document
---

```{r}
#################################################################
###########  Regression Analysis - Board reviews  ###############
#################################################################

library(vcd) 
library(corrplot)
library(caTools)
library(ROCR)
library(pROC)
library(car)
library(caret)


### Initialize data

load("/home/leslie/Desktop/StatsLab/snsf_data.RData")
source("/home/leslie/Desktop/StatsLab/stats-lab-snsf/Cleaning Functions.R")
source("/home/leslie/Desktop/StatsLab/stats-lab-snsf/Data for Regression.R")

```


### Questions for Janine:
-For variable importance question, could use a random forest to assess variable importance? Or could standardize coefficients
-Multiple testing problem??


Prepare the data and fit the full model:
```{r}

board_data <- prepare_data_board_log_regression(final.apps, internal = final.internal, external = final.external)



board_log_regression <- glm(board_data$IsApproved ~ Gender + Division + Age + IsContinuation + InstType + AmountRequested + 
                              Ranking + OverallGrade + Gender:Division, family="binomial", data = board_data)

```


In fitting the regression, the summary shos that OverallGrade, Ranking, InstType and Age are all significant predictors.
```{r}
(summary(board_log_regression))
```


In checking for correlation among coefficients, we only don't see any alarming values (VIF > 5). 
```{r}
(vif(board_log_regression))
```


Check diagnostics: residuals:
```{r}
plot(resid(board_log_regression, type="pearson")) 
plot(resid(board_log_regression, type="deviance"))

```

Checking the residuals, they seem to be expectation 0, with a few outliers. However, in looking at the deviance, there appears to be some structure in the data. Should this indicate we need a new model fit?

Test out other models, to do a hierarchical model comparision:

```{r}
board_log_regression2 <- glm(board_data$IsApproved ~ Gender + Division + Age + AmountRequested + 
                            OverallGrade + Gender:Division, family="binomial", data = board_data)

board_log_regression1 <- glm(board_data$IsApproved ~ Gender + Division + Age + AmountRequested + 
                              Ranking + Gender:Division, family="binomial", data = board_data)

board_log_regression4 <- glm(board_data$IsApproved ~ Gender + Ranking + OverallGrade, family="binomial", data = board_data)

anova(board_log_regression2, board_log_regression, test="Chisq") # significantly different than small model
anova(board_log_regression1, board_log_regression, test="Chisq") # significantly different than small model
anova(board_log_regression4, board_log_regression, test="Chisq") # not significiantly different than small model

1-pchisq(board_log_regression1$dev-board_log_regression$dev, df=(board_log_regression1$df.res-board_log_regression$df.res))

```
Doing this model comparision, don't we have a big multiple testing problem? Should we avoid doing any kind of model comparison and instead 1) validate model fit, 2) assess performance, 3) compute variable importance?


Do CV to see if model fit is decent:
```{r}
#####################################################################
#################### CV to assess model fit ##########################
#####################################################################


assess_board_model<- function(data=board_data, Div="All",
                      SplitRatio=0.8, cutoff=0.5 ){ 
  
  if (Div == "All"){ 
    final.data <- data
  }
  else {
    final.data<- subset(data,Division==Div, select = -(Division)) 
  }
  
  ### spliting the data into train and test set
  
  if (SplitRatio<1){
    split<-sample.split(final.data$IsApproved, SplitRatio = SplitRatio)
    Train<-subset(final.data, split=="TRUE")
    Test <-subset(final.data, split=="FALSE") 
  } else {
    Test<-Train<-final.data
  }
  
  #### fitting the model
  
  # Cutoff
  cutoff <- cutoff
  
  # Optimize the model
  
  Model <- glm(Train$IsApproved ~ .-(ProjectID),data=Train, 
               family="binomial")
  
  ### Testing the Treshold
  par(mfrow=c(1,2))
  predictor<-predict(Model, Test, type="response")
  ROCRPred<- prediction(predictor,Test$IsApproved)
  ROCRPerf<- performance(ROCRPred,"tpr","fpr")
  plot(ROCRPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1))
  
  ### with respect to accuracy
  ROCRACC<- performance(ROCRPred,"acc")
  plot(ROCRACC)  
  
  ### Confusion Matrices
  AccTable<-table(ActualValue=Test$IsApproved,Prediction=predictor>=cutoff)
  accuracy<-(sum(diag(AccTable))/sum(AccTable))
  
  ### Return
  print(paste("Regresion for External Reviews.   ", "Division: ", Div))
  return(list(Model= summary(Model), 
              #`Confidence Intervals`=confint(Model),
              `Confusion Matrix`=AccTable,
              `Percentage of data used for Training`=paste(SplitRatio*100,"%"),
              `Accuracy`=paste(round(accuracy,2)*100,"%")))
}


board.all <- assess_board_model(data=board_data, Div="All",
                        SplitRatio=1,cutoff=0.5)


```



