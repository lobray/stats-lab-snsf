---
title: "SNSF Report"
author: "Chiara Gilardi, Leslie O’Bray, Carla Schärer and Tommaso Portaluri"
date: "13 June 2018"
graphics: yes
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, prompt = FALSE,comment = "   ")
```

```{r Load Functions and Data }
# setwd()
load("SNFS Data/snsf_data.RData") # carla's directory, comment out
#load("/home/leslie/Desktop/StatsLab/snsf_data.RData") # leslie's directory, comment out when other is using
source("Cleaning Functions.R")
source("Data for Regression.R")

# install.packages("biostatUZH", repos="http://R-Forge.R-project.org")

library(biostatUZH)
library(psy)
library(psych)
library(ggplot2)
library(gridExtra)
library(coin)
library(ggmosaic)
library(effects)
library(xtable)
library(kableExtra)
library(ordinal)
library(MASS)

```


## Introduction
The Swiss National Science Foundation (SNF) is a research funding agency which disseminates yearly, on behalf of the Swiss Government, billions of CHF to the best researchers in Switzerland. This report contains a statistical analysis performed on three data sets provided by SNF, containing information on the applications for funding received in 2016, the corresponding and the scores given by both internal and external evaluators.  

The analysis performed for SNF had a three-fold aim, corresponding to the following three research questions:
1) Is gender bias occurring at any stage of the SNSF evaluation process? Is the gender of the main applicant influencing the rating of the application?
2) To what extent the different steps of the evaluation and the different criteria within each step determine the final funding decision?
3) When an application is approved, but the budget requested is cut, how can we explain this?

The SNSF evaluation procedure is a multi-step process (involving external reviewers, internal referees, and an internal board) which takes into consideration both the track record of the applicant and the quality of the project (see Appendix for a more detailed description of the evaluation procedure).

Several studies (Witteman et al., 2017; Solans-Domenech et al., 2017) have shown that female applicants' projects get higher score when the application is blinded. Moreover,  female applicants receive usually higher grades for projects and lower grades for track record. Hence, after investigating the gender dimension to identify possible biases in the evaluation procedure, the focus of the analysis will be the relative importance of of the criteria for funding (applicant's track record vs. quality of the proposal) and, also, of each step of the evaluation procedure (which opinion is more likely to determine the final decision – the external referee's or the board's?). Possible interactions between the gender dimension and the second research question will also be investigated (for instance, by taking into account also the gender of evaluator or the percentage of female referees).

## Data Description

We have three data sets: Applications, External Reviewers and Internal Referees. They contain respectively information about the SNSF project funding applications, the evaluation of the applications by external peer reviewers and the evaluation of the proposals by external the internal referee and co-referee (when available). For a full description of the data & variables, please see the Appendix. 



# Cleaning the Data

We decide to work with only complete applications, i.e. project for which we have information from all the three data sets.

To avoid a temporal trend, we are only considering application from 2016. 

In both the external and internal step, we encountered applications which had several reviews per application. For the sake of our analysis, in these scenarios we computed the mean grade for each criteria, so that each application had a "single" score for each criteria assessed on. In doing so we also introduced a new variable, PercentFemale, which calculated the percent of female reviewers out of all reviewers of a single application (ranging from 0 to 1).

All applications with a grade were converted to an ordinal factor.

Specific to each data set, this are the detailed considerations:

### Applications

We decide to consider only the MainDiscipline2 because for MainDiscipline we have 118 levels, while for the other only 21. 

There is one application for which we do not know the gender of the applicant, and therefore we decided to omit that observation from the analysis.




```{r Table Professorship and AcademicAge}
# For Professorship

tbl<-table(applications$ResponsibleApplicantProfessorshipType, useNA = "always")
NA.prof<-sum(is.na(applications$ResponsibleApplicantProfessorshipType))/dim(applications)[1]

Prof<-data.frame(tbl)
colnames(Prof)<-c("Type","Frequency")
# For AcademicAge
NA.AC<-sum(is.na(applications$ResponsibleApplicantAcademicAgeAtSubmission))/dim(applications)[1]
```
We will also not consider the variables "CallTitle", "Professorship", "AcademicAge". The first one, because we consider it has nothing to add to the model. The two last, due to the fact that there are a considerable number of NA's on those variables (around `r round(NA.prof*100)`% of the observations).
```{r}
kable(Prof,"latex", booktabs = T) %>%
  kable_styling(position = "center")
```


### External Reviewers

Reviewers always have the option to choose not to consider or to give the grade "0" when reviewing an application. Some might be mistakes, in others cases there might be a conflict of interest, or they might be very ambivalent about the project. Therefore, we did not considered observations with this grades. 

One of the questions evaluated in the applications is "Broader impact (forms part of the assessment of scientific relevance, originality and topicality)". For the time frame we are considering, in all the applications this grade was NA. Hence, we omit this variable from our model.

* **ProposalCombined**: We created a new variable to summarize the assessment of the scientific proposal in the external review step. This is a simple mean of the grade given for Suitability and Scientific Relevance. This helped to isolate the effect of the grade given to the scientific proposal, versus the applicant track record, as well as to ensure easier comparison with the internal review step.
* **PercentFemale**: As previously mentioned, we introduced a new variable calculating the percent of reviewers of each application that is female. 

### Internal Referees

There were 22 observations (1 for the time frame we are dealing with) for which only demographic information was available, no grades were given. We decide to omit those observations.

Also we decide to not consider the Referee role as a variable in our model, as the majority of the evaluations has only one referee.

```{r Table RefereeRole}
Ref<-data.frame(table(referee_grades$RefereeRole, useNA = "always"))
colnames(Ref)<-c("Type","Frequency")
kable(Ref,"latex", booktabs = T) %>%
  kable_styling(position = "center")

```
* **PercentFemale**: As previously mentioned, we introduced a new variable calculating the percent of reviewers of each application that is female. 


# Exploratory Analysis

```{r data to use for analysis, echo=F, message=F}


#### External datasets to use
external_regression_data <- prepare_data_external_log_regression(apps=final.apps, external=final.external)
ex.f <- external_regression_data[external_regression_data$Gender=="f",]
ex.m <- external_regression_data[external_regression_data$Gender=="m",]
ex.div1 <- external_regression_data[external_regression_data$Division=="Div 1",] # particularly bad for Div 1
ex.div2 <- external_regression_data[external_regression_data$Division=="Div 2",]
ex.div3 <- external_regression_data[external_regression_data$Division=="Div 3",]


#### internal datasets to use 
internal_regression_data<-prepare_data_internal_log_regression(apps=final.apps, internal=final.internal)
in.f <- internal_regression_data[internal_regression_data$Gender=="f",]
in.m <- internal_regression_data[internal_regression_data$Gender=="m",]
in.div1 <- internal_regression_data[internal_regression_data$Division=="Div 1",] # particularly bad for Div 1
in.div2 <- internal_regression_data[internal_regression_data$Division=="Div 2",]
in.div3 <- internal_regression_data[internal_regression_data$Division=="Div 3",]



```

In our exploratory analysis, we discovered a few interesting insights, that relates to the findings we will discuss from our analysis. 

### Distribution of Grades between the External & Internal Review Step

Since the external & internal step both assess candidates on the same criteria (the strength of the scientific proposal, and the strength of the applicant), on the same ordinal scale (from poor to outstanding), we were interested to see if the distribution of grades are the same. We would expect different distributions for the Overall Grade vs the Ranking, since those have two different measurements, however we were interested to see if for the same absolute ranking, the external and internal reviewers had different perspectives on the application. After combining the Suitability & Scientific Relevance grades given to a candidate in the external review step, we can compare the average grade given for the Scientific Proposal in the two steps, as well as the grade given for the Applicant Track Record in both steps. 

We see that the External Reviewers are more generous with their grades; for the strength of the Scientific Proposal, 48% of proposals are considered "excellent" or "outstanding", versus only 28% in the internal review step. 

```{r scientific grade distribution external vs internal}
ProposalCombined <- external_regression_data$ProposalCombined
ex_proposal <- as.data.frame(prop.table(table(ProposalCombined)))
ex_proposal$Freq <- as.numeric(ex_proposal$Freq)
colnames(ex_proposal)[1] <- "ProposalScore"
p8 <- ggplot(ex_proposal, aes(x=ProposalScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("External Scientific Proposal Grade Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5)) 
  scale_y_continuous(limits = c(0, 0.45))

in_proposal <- as.data.frame(prop.table(table(internal_regression_data$ProjectAssessment)))
in_proposal$Freq <- as.numeric(in_proposal$Freq)
colnames(in_proposal)[1] <- "ProposalScore"
p9  <- ggplot(in_proposal, aes(x=ProposalScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("Internal Scientific Proposal Grade Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5)) +
  scale_y_continuous(limits = c(0, 0.41))

grid.arrange(p8, p9, ncol=2)
```

Similarly we see the same pattern with Applicant Track Record: 66% of Applicant Track records are considered "excellent" or "outstanding" by the External Reviewers, versus merely 50% by the Internal Reviewers. 

```{r}
ApplicantTrack <- external_regression_data$ApplicantTrack
ex_proposal <- as.data.frame(prop.table(table(ApplicantTrack)))
ex_proposal$Freq <- as.numeric(ex_proposal$Freq)
colnames(ex_proposal)[1] <- "ApplicantScore"
p8 <- ggplot(ex_proposal, aes(x=ApplicantScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("External Applicant Track Grade Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5)) 
  scale_y_continuous(limits = c(0, 0.45))

in_proposal <- as.data.frame(prop.table(table(internal_regression_data$ApplicantTrack)))
in_proposal$Freq <- as.numeric(in_proposal$Freq)
colnames(in_proposal)[1] <- "ApplicantScore"
p9  <- ggplot(in_proposal, aes(x=ApplicantScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("Internal Applicant Track Grade Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5)) +
  scale_y_continuous(limits = c(0, 0.43))

grid.arrange(p8, p9, ncol=2)
```

Since we noticed this discrepancy, we wanted to quantify how differently the grades were to one another. To assess the agreement between the two steps, for the same criteria, we used Cohen's Kappa. Cohen's Kappa measures the proportion of agreement between two raters assessing something on an ordinal scale, accounting for the fact that there will always be some proportion by random chance. An important specification of Cohen's Kappa is the weight given to the measurements. If the external & internal reviewers both assessed the Applicant Track Record as "excellent", that would be considered full agreement. However, we want to allocate partial credit if the rating is a level close to it. We used a linear weight up to distance 2, and after that gave no credit. (In this example, if one rater gave an "outstanding" or "very good", that would be considered a distance of one and be weighted by 0.8. If the second rater assessed the Applicant Track to be "good", which is a distance of two away from excellent, that would be weighted as 0.6). Anything with a distance of 3 or more (in this example, if the second rater gave a rating of "average"), we allocated no weight, as the difference between average and excellent is quite large.

From this, we found that there was just moderate agreement between the two steps when using the weighted kappa, for both the grades given for the Scientific Proposal & the Applicant Track Record.

```{r cohens kappa}
board_data <- prepare_data_board_log_regression(apps=final.apps, internal = final.internal, external = final.external)

custom.weights <- matrix(c(1, 0.8, 0.6, 0, 0, 0, 0, 1, 0.8, 0.6, 0, 0, 
         0.6, 0.8, 1, 0.8, 0.6, 0, 0, 0.6, 0.8, 1, 0.8, 0.6,
         0, 0, 0.6, 0.8, 1, 0.8, 0, 0, 0, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)


kappa.applicant <- table(board_data$ExternalApplicantTrack, board_data$InternalApplicantTrack)

print("Cohen's Kappa for Applicant Track Record")
kappa.app <- cohen.kappa(kappa.applicant, w=custom.weights)
kappa.app

print("Cohen's Kappa for Scientific Proposal")
kappa.proposal <- table(board_data$ProposalCombined, board_data$ProjectAssessment)
kappa.pro <- cohen.kappa(kappa.applicant, w=custom.weights)
kappa.pro

```



### Impact of Internal Reviewers on Funding

We wanted to understand if this discrepancy between grades had an impact on whether an application is funded. To do this, we visualized the summary grade given to an application, and whether that application is funded or not. As we can see here, there are several applications with an OverallGrade of "excellent" or "outstanding" that end up not approved. It highlights that not only do the Internal Reviewers give tougher grades in general than the external step, but they also consider some "excellent" and "outstanding" applications by the external reviewers to be not of the quality that deserves funding. This trend is true in all divisions and both genders, please refer to the appendix to see the specific graphic. 

Our conclusion for this is that the internal step is very consequential, and the difference in the rating they give translates into differences in whether an application gets funded or not. 

```{r}
plot_mirror_barplot <- function(dataset, variable1, variable2="IsApproved", plot_title="Plot Title", title_size=8) {
  table_data_frame <- as.data.frame(table(dataset[,variable1], dataset[,"IsApproved"]))
  colnames(table_data_frame) <- c(variable1, variable2, "Frequency")
  levels(table_data_frame[,variable1]) <- list("Outstanding"="6", "Excellent"="5", 
                                               "Very Good"="4", "Good"="3", "Average"="2", "Poor"="1")
  table_data_frame$Frequency[table_data_frame[,"IsApproved"]==0] <- -table_data_frame$Frequency[table_data_frame[,"IsApproved"]==0]
  
  ggplot(table_data_frame, aes_string(x=variable1, y="Frequency", fill=variable2)) + 
    geom_bar(stat="identity", position="identity") + 
    scale_y_continuous(breaks=seq(-100,100,by=50),labels=abs(seq(-100,100,by=50))) +
    coord_flip() +
    ggtitle(plot_title) + 
    theme(plot.title = element_text(size = title_size)) 
    # scale_fill_manual(values = c("firebrick","darkseagreen4"))
    
}

p4 <- plot_mirror_barplot(dataset=external_regression_data, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; All Data")

p4


```


### Distribution of Grades by the Gender of the Reviewer

The third interesting insight we found was when we investigated the impact of the gender of the person reviewing the data. We look at the relative frequencies of grades given by male and female reviewers, to applicants, regardless of gender. Within the external step in particular, we found that female reviewers give proportionally fewer "excellent" and "outstanding" grades, compared to their male counterparts. Within the internal step, we did not notice a particular difference, though we will consider the impact of the gender of the reviewer more rigorously in our analysis. 

```{r}

####################### EXTERNAL

tmp.external.data <- merge(final.apps[,c("Gender", "ProjectID")], external_reviews, by="ProjectID")

# just females
tmp.external.data.f <- tmp.external.data[tmp.external.data[,"Gender"]=="f",]

# just males
tmp.external.data.m <- tmp.external.data[tmp.external.data[,"Gender"]=="m",]

r.tab.2<-prop.table(table(tmp.external.data$OverallGrade, tmp.external.data$ReviewerGender),2)
r.tab.dataframe.2 <- as.data.frame(r.tab.2)    
colnames(r.tab.dataframe.2) <- c("OverallGrade", "ReviewerGender", "Freq")

# How men and women allocate overallgrades

p10 <- ggplot(r.tab.dataframe.2,aes(x=OverallGrade,y=Freq,fill=factor(ReviewerGender)))+
  geom_bar(stat="identity",position="dodge")+
  xlab("OverallGrade")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
  ggtitle("External Step: Female vs. Male Reviewers Distribution of OverallGrades") +
  scale_fill_manual(values=c("orchid4", "darkorange1")) + 
  scale_y_continuous(limits = c(0, 0.40))

####################### INTERNAL

tmp.internal.data <- merge(final.apps[,c("Gender", "ProjectID")], final.internal, by="ProjectID")

# just females
tmp.internal.data.f <- tmp.internal.data[tmp.internal.data[,"Gender.x"]=="f",]

# just males
tmp.internal.data.m <- tmp.internal.data[tmp.internal.data[,"Gender.x"]=="m",]

r.tab.2<-prop.table(table(tmp.internal.data$Ranking, tmp.internal.data$RefereeGender),2)
r.tab.dataframe.2 <- as.data.frame(r.tab.2)    
colnames(r.tab.dataframe.2) <- c("Ranking", "RefereeGender", "Freq")


p11 <- ggplot(r.tab.dataframe.2,aes(x=Ranking,y=Freq,fill=factor(RefereeGender)))+
  geom_bar(stat="identity",position="dodge")+
  xlab("Ranking")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
  ggtitle("Internal Step:Female vs. Male Reviewers Distribution of Rankings") +
  scale_fill_manual(values=c("orchid4", "darkorange1")) + 
  scale_y_continuous(limits = c(0, 0.4))

p10
p11

```

# Gender Bias

To see if gender has an influence in any of the steps of the evaluation process, we did several things. For the external and internal steps, we first fit a logistic regression with the function glm in R, where we used IsApproved (a binary variable) as a response and demographic information of the applicant, project information and the given grades as predictors. The aim of this regression is to see if gender has an influence on the final decision from the perspective of each step.

As the final decision is determined by the different grades in the process, in order to see if gender has an influence on any of them, we fitted an Ordinal regression with the function clm of the package "ordinal" on each grade with demographic data and project information as predictors.

Finally, we estimated the relative importance of each of the predictors in the model as a last check, in order to see the importance of gender in all the models.

## Analysis

### External Step
*Logistic Regression*

Regression data:  To perform the analysis, we combined in one data frame information about the applications (IsApproved, Age, Gender, Division, IsContinuation, PreviousRequest, InstType, log(AmountRequested), Semester) and about the grades given by the external reviewers (ApplicantTrack, ScientificRelevance, Suitability, OverallGrade, ProposalCombined, PercentFemale). 
    
As there are almost no application approved that have grades smaller than "good", we decide to aggregate grades "poor", "average" and "good" to avoid perfect separation problems. All grades are considered as ordered factors.
                         
```{r}
# Obtain Regression Data
external_regression_data<-prepare_data_external_log_regression(final.apps,final.external)

# combine grades 1,2,3 and log AmountRequested
external_regression_data$logAmount<-log(external_regression_data$AmountRequested)
data<- subset(external_regression_data,select = -c(ProjectID, OverallGrade, AmountRequested,ProposalCombined))
data$ApplicantTrack<-ifelse(data$ApplicantTrack<=3,3,data$ApplicantTrack)
data$ApplicantTrack<-factor(data$ApplicantTrack, ordered=TRUE)
data$ScientificRelevance<-ifelse(data$ScientificRelevance<=3,3,data$ScientificRelevance)
data$ScientificRelevance<-factor(data$ScientificRelevance, ordered=TRUE)
data$Suitability<-ifelse(data$Suitability<=3,3,data$Suitability)
data$Suitability<-factor(data$Suitability, ordered=TRUE)

# Fit first model with all variables and interactions
  Model <- glm(IsApproved ~ .+Gender:Division+
               Gender:PercentFemale+Gender:ApplicantTrack+InstType:Division ,data=data, 
             family="binomial")
  PsR2<-(1-exp((Model$dev-Model$null)/1623))/(1-exp(-Model$null/1623))
```
We first fitted a full model with all the variables and the interactions between Gender and Division, PercentFemale and ApplicantTrack. Also we considered the interaction between InstType and Division. we didn't considered OverallGrade as it is highly correlated with the grades of the applicant and the project. we achieved a pseudo-R^2 value of  `r round(PsR2,4)`, indicating that percent of the variation in Y can not be explained very well by the model. 

```{r}
log.ext.Model <- glm(IsApproved ~ ApplicantTrack + ScientificRelevance + Suitability + 
              PercentFemale + Age + Gender + Division + IsContinuation + InstType + 
              Semester + Division:InstType, data=data, family="binomial")

PsR2.small<-(1-exp((log.ext.Model$dev-log.ext.Model$null)/1623))/(1-exp(-log.ext.Model$null/1623))
```


When selecting the variables with the AIC criteria in order to work with a small and effective model, we end up with the following predictors: ApplicantTrack, ScientificRelevance, Suitability, PercentFemale, Age, Gender, Division, IsContinuation, InstType, Semester, Division:InstType. No interaction with Gender where significant. The pseudo R^2 for this model is `r round(PsR2.small,4)` , i.e. this smaller model explains basically the same variance of the data than the former one. Non of them reveal that the information from the external reviewers explain the final decision correctly. This fact will be explored in more detail later on. 

```{r}
# Obtain all the effects        
  eff.fit <- allEffects(log.ext.Model)
  
# Obtain Gender effect as a separate data frame 
  eff<-Effect("Gender",log.ext.Model)
  eff<-eff$fit
  prob<- round((exp(eff)/(1+exp(eff))),3)
```

As we are interested in the effect of gender in each step of the evaluation process, we looked at the difference of the probability of being accepted between male and female given the final model. Female have a probability `r prob[2,1]`, while male have `r prob[1,1]`. Although there is a small difference, our analysis suggest that this difference is not significant.
```{r fig.align="center", fig.height=3}
 plot(Effect("Gender",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
```

In order to understand wich of the variables in this model are the more influencial, we ran a permutation test. We randomly permuted the values of all predictors (one at the time) and refit the the model. We did this 1000 times for each predictor, and calculated the average change in pseudo R^2 when we permuted that particular variable. If permuting a variable changes the pseudo r^2 a lot, this means that that variable was an important predictor in our regression. As shown in the next table, Gender is the second least important variable in this model.
```{r Logistic-external-relative-importance, out.width='50%',fig.align="center"}
# Relative variable importance 

  load("Ext.logistic_variable_importance.Rda")
  kable(PseudoRShuffle,"latex", booktabs = T) %>%
    kable_styling(position = "center")
  
  n<-dim(PseudoRShuffle)[1]
  PseudoRShuffle$Importance<-abs(PseudoRShuffle$Importance)
  mycol=colorRampPalette(c("blue","red"))(n)
  
  par(las=2,mfrow=c(1,1),mai=c(1,2,1,1))
  barplot(PseudoRShuffle[,2], names.arg = PseudoRShuffle[,1],
          horiz = TRUE,col = mycol,cex.names = 0.7,
          main="Variable Importance for Approval",
          xlab = "Percent of variation on the Pseudo R",
          cex.axis = 0.6,
          cex.main=0.8)
```
\pagebreak

*Ordinal Regression*

To look into the variables that influence the different grades in this part of the evaluation process, and see if the gender of the main applicant has an influence on it, we ran an ordinal regression with the function clm of the package ordinal in R. We did this for both, the applicant grade (ApplicantTrack), and the average of the grades given to the project (ProposalCombined). Also, we want to investigate which grades are more influential in the Overall Grade, which is the summary grade given by the external reviewers. In the next few paragraphs we will look into the analysis of this two regressions.

```{r}

Model.Prop<- clm(ProposalCombined ~ Gender + Division + PercentFemale + IsContinuation + 
                    InstType + logAmount ,data=external_regression_data)

Prop.nogender<- clm(ProposalCombined ~  Division + PercentFemale + IsContinuation + InstType + logAmount ,data=external_regression_data)
                            
p.value<-anova(Model.Prop,Prop.nogender)[2,6]
```

* **Project Assessment**: After fitting a full model with ProposalCombined as a response variable and different interactions, and then selecting from this model the significant variables with the AIC criteria and the help of the drop1() function in R, we end up with a model with the following predictors: Gender, Division, PercentFemale, IsContinuation, InstType and log(AmountRequested). If we fit the same model without Gender and compare it with the anova() function to the one with gender, we get a p.value of `r p.value`, meaning that for the grades given to the project, gender is not important. This is to be expected, as the project is being evaluated and not the applicant.

```{r}

gender.prob<-Effect("Gender", mod=Model.Prop)
gender.prob<-gender.prob$prob
gender.prob<-data.frame(Male=gender.prob[1,],Female=gender.prob[2,])
rownames(gender.prob)<-c("poor","average","good", "very good",
                        "excellent","outstanding")

gender.prob<-cbind(round(gender.prob,3),Difference=round((gender.prob[,1]-gender.prob[,2]),3))
       
```

    Nevertheless we kept gender as a predictor to be able to show the effect it has in the grades. For this purpose we estimated the cumulative probability of falling in the different grades for each gende. The result is presented in the next table. Overall the average difference is `r round(mean(abs(gender.prob[,3])),5)`. We see here as well, that there is no evidence of gender influencing the probability of achiving a certain grade.
```{r out.width='50%',fig.align="center"}
kable(gender.prob,"latex", booktabs = T) %>%
    kable_styling(position = "center")

ggplot(gender.prob) +
          geom_line(aes(1:6, Male, colour = "Male")) +
          geom_line(aes(1:6, Female, colour = "Female"))+
          theme(legend.title=element_blank(),legend.position = "bottom")+
          xlab("")+ylab("Cumulative Probability")+
          ggtitle("Cumulative Probability for ProposalCombined")
```
```{r}
load("External_Project_variable_Importance.Rda")
```
    We did also here our permutation test where we permuted each variable 1000 times, and used the percentage of variation of the AIC as a measure of good fit. Gender is the least important variable, in average it increments the AIC `r round(Shuffle.Result.Prop[6,3],2)`%.
    
```{r out.width='50%', fig.align="center"}
kable(Shuffle.Result.Prop,"latex", booktabs = T) %>%
    kable_styling(position = "center")

n<-dim(Shuffle.Result.Prop)[1] 
mycol=colorRampPalette(c("red","green"))(n)
       par(las=2)
       barplot(Shuffle.Result.Prop[,3], names.arg = Shuffle.Result.Prop[,1],
               horiz = TRUE,col = mycol,cex.names = 0.7,
               main="Variable Importance of Combined Project Assessment",
               xlab = "Percent of variation on the AIC",
               xlim=c(0,0.7),
               cex.axis = 0.6,
               cex.main=0.8)
```

```{r}
  Model.App<- clm(ApplicantTrack ~ Gender + Division + PercentFemale + IsContinuation + 
                              InstType + logAmount +Gender:PercentFemale,
                             data=external_regression_data)
  App.nogender<- clm(ApplicantTrack ~ Division + PercentFemale + IsContinuation +
                               InstType + logAmount +Gender:PercentFemale,
                             data=external_regression_data)
                            
  p.value<-anova(Model.App,App.nogender)[2,6]
             
```

* **Applicant Track assessment**: The final model we used has ApplicantTrack as a response variable, and the following predictors: Gender, Division, PercentFemale, IsContinuation, InstType, log(AmountRequested) and the interaction between Gender and PercentFemale. Again we fitted the same model without Gender and compare it with the anova() function to the one with gender, we get a p.value of `r p.value`, meaning that for the grades given to the main applicant, gender needs to be considered in the model. In the next table we present part of the summary for this model, to see the full summary refer to the Appendix.
```{r}
kable(coeff<-round(summary(Model.App)$coefficients,4),"latex", booktabs = T)%>%
    kable_styling(position = "center")
```

```{r}

gender.prob.App<-Effect("Gender", mod=Model.App)
gender.prob.App<-gender.prob.App$prob
gender.prob.App<-data.frame(Male=gender.prob.App[1,],Female=gender.prob.App[2,])
rownames(gender.prob.App)<-c("poor","average","good", "very good",
                        "excellent","outstanding")

gender.prob.App<-cbind(round(gender.prob.App,3),Difference=round((gender.prob.App[,1]-gender.prob.App[,2]),3))
       
```
The predicted probabilities of achieving certain grade for male and female is shown in the next table. The average difference of the cumulative probability is here as well close to zero,`r round(mean(abs(gender.prob.App[,3])),5)`,  but the greater difference are in grades "outstanding", which are the ones that are more likely to be approved. 
```{r out.width='50%',fig.align="center"}
kable(gender.prob.App,"latex", booktabs = T) %>%
    kable_styling(position = "center")

ggplot(gender.prob.App) +
          geom_line(aes(1:6, Male, colour = "Male")) +
          geom_line(aes(1:6, Female, colour = "Female"))+
          theme(legend.title=element_blank(),legend.position = "bottom")+
          xlab("")+ylab("Cumulative Probability")+
          ggtitle("Cumulative Probability for Applicant Track")
```
```{r}
load("External_Applicant_variable_importance.Rda")
```
Though, gender is significant to the model, when estimating its relative importance in comparison with the other variables, by permuting each variable 1000 times, and using the average percent variation on the AIC as a measure of goodness of fit, turns out that gender is the least important variable in the model, in average it increments the AIC `r round(Shuffle.Result.App[6,3],2)`%.
    
```{r out.width='50%', fig.align="center"}
kable(Shuffle.Result.App,"latex", booktabs = T) %>%
    kable_styling(position = "center")

n<-dim(Shuffle.Result.App)[1] 
mycol=colorRampPalette(c("red","blue"))(n)
       par(las=2)
       barplot(Shuffle.Result.App[,3], names.arg = Shuffle.Result.App[,1],
               horiz = TRUE,col = mycol,cex.names = 0.7,
               main="Variable Importance of Applicant Assessment",
               xlab = "Percent of variation on the AIC",
               xlim = c(0,1.2), 
               cex.axis = 0.6,
               cex.main=0.8)
```


```{r}

Model.Overall<-clm(OverallGrade ~ Gender + Division + PercentFemale + IsContinuation + 
                         InstType + logAmount + Gender:PercentFemale , data=external_regression_data)
Over.nogender<- clm(OverallGrade ~ Division + PercentFemale + IsContinuation + 
                         InstType + logAmount + Gender:PercentFemale , data=external_regression_data)
                            
p.value<-anova(Model.Overall,Over.nogender)[2,6]
```

* **Overall Grade**: The final model, after variable selection has OverallGrade as a Response, and predictors: Gender, Division, PercentFemale, IsContinuation, InstType, log(AmountRequested) and the interaction between Gender and PercentFemale. We are not considering here applicant grades and project grades, as we want to see the influence of the demographic data and the project information in this grade. Later on we will study which of this grades are more important in determinaiting the Overall Grade. Comparing this model with the one without gender suggest that gender is significant to the model, p.value of `r p.value`. 
```{r}
kable(coeff<-round(summary(Model.Overall)$coefficients,4),"latex", booktabs = T)%>%
    kable_styling(position = "center")
```

```{r}

gender.prob.Overall<-Effect("Gender", mod=Model.Overall)
gender.prob.Overall<-gender.prob.Overall$prob
gender.prob.Overall<-data.frame(Male=gender.prob.Overall[1,],Female=gender.prob.Overall[2,])
rownames(gender.prob.Overall)<-c("poor","average","good", "very good",
                        "excellent","outstanding")

gender.prob.Overall<-cbind(round(gender.prob.Overall,5),Difference=round((gender.prob.Overall[,1]-gender.prob.Overall[,2]),5))
       
```
The predicted probabilities of achieving certain grade for male and female is shown in the next table. The average difference of the cumulative probability is here as well close to zero,`r round(mean(abs(gender.prob.Overall[,3])),5)`,  the greater difference are in grades "Outstandig".
```{r out.width='50%',fig.align="center"}
kable(gender.prob.Overall,"latex", booktabs = T) %>%
    kable_styling(position = "center")

ggplot(gender.prob.Overall) +
          geom_line(aes(1:6, Male, colour = "Male")) +
          geom_line(aes(1:6, Female, colour = "Female"))+
          theme(legend.title=element_blank(),legend.position = "bottom")+
          xlab("")+ylab("Cumulative Probability")+
          ggtitle("Cumulative Probability for the Overall Grade")
```  
```{r}
load("External_Overall_variable_importance.Rda")
```
Once more, though gender is significant to the model, when estimating its relative importance in comparison with the other variables, by permuting each variable 1000 times, and using the average percent variation on the AIC as a measure of goodness of fit, turns out that gender is the least important variable in the model, in average it increments the AIC `r round(Ov.Shuffle.Result[6,3],2)`%.
    
```{r out.width='50%', fig.align="center"}
kable(Ov.Shuffle.Result,"latex", booktabs = T) %>%
    kable_styling(position = "center")

n<-dim(Ov.Shuffle.Result)[1] 
mycol=colorRampPalette(c("red","blue"))(n)
       par(las=2)
       barplot(Ov.Shuffle.Result[,3], names.arg = Ov.Shuffle.Result[,1],
               horiz = TRUE,col = mycol,cex.names = 0.7,
               main="Variable Importance of Applicant Assessment",
               xlab = "Percent of variation on the AIC",
               xlim = c(0,1), 
               cex.axis = 0.6,
               cex.main=0.8)
```

## Results


# Relative Importance of the Different Steps

Our second research question was to assess the relative importance of each step in the process, and the relative importance of each criteria within each step.

## Analysis

### Most Important Step

```{r board regression}

board_data1 <- prepare_data_board_log_regression(apps=final.apps, internal = final.internal, external = final.external)

board_data1$Ranking <- ifelse(board_data1$Ranking %in% c(1,2,3), 3, board_data1$Ranking)
board_data1$Ranking <- factor(board_data1$Ranking, ordered=T)
board_data1$OverallGrade <- ifelse(board_data1$OverallGrade %in% c(1,2,3), 3, board_data1$OverallGrade)
board_data1$OverallGrade <- factor(board_data1$OverallGrade, ordered=T)

board_log_regression <- glm(board_data1$IsApproved ~ Gender + Division + Age + IsContinuation + InstType + log(AmountRequested) + 
                              Ranking + OverallGrade + Gender:Division + PercentFemale + 
                              PercentFemale:Gender, family="binomial", data = board_data1)

calc_pseudo_r <- function(log_regression_object) {
  n <- dim(log_regression_object$data)[1]
  pseudo <- (1 - exp((log_regression_object$dev - log_regression_object$null)/n)) / (1-exp(-log_regression_object$null/n))
  return(pseudo)
}

full.board.psuedo.r <- calc_pseudo_r(board_log_regression)

```


To approach the question of which step in the process is most important, we first fit a logistic regression with IsApproved as our binary response variable using the glm function in R. We fit a full model with all potential demographic predictors and interactions across the different steps of the process, and the summary grade given to an application in the external (OverallGrade) and internal (Ranking) step. To address the first part of the question (relative importance of each step in the process), we used only the summary grade in each step due to the correlation between the individual grades given within each step and the summary grade given. With the full model (predictors: Gender, Division, Age, IsContinuation, InstType, log(AmountRequested), PercentFemale, Ranking, OverallGrade, Gender:Division, PercentFemale:Gender), we achieved a pseudo-R^2 value of  `r round(full.board.psuedo.r,4)`, indicating that percent of the variation in Y can be explained by the model. 

```{r}
board_log_regression_small <- glm(board_data1$IsApproved ~ Age + IsContinuation + Ranking + OverallGrade, family="binomial", data = board_data1)

board_r_small <- calc_pseudo_r(board_log_regression_small)
```

As our goal was to explain the most important factors, we then did backwards variable selection using the AIC. This left us with a model with only 4 predictors: Ranking, OverallGrade, Age, and IsContinuation. The pseudo R^2 measure of this model is `r round(board_r_small,4)`, which indicates that this simplified model nearly explains exactly as much variance in the data as the full model, and so we can be content to use just the small model.

Now that we've reduced our model to 4 predictors, we wanted to understand exactly how important each of those predictors are to the final funding decision. To do this, we performed permutation tests on the variables. For each of these 4 predictors, we randomly permuted the values of that predictor and refit the the model. We did this 1000 times for each predictor, and calculated the average change in pseudo R^2 when we permuted that particular variable. If permuting a variable changes the pseudo r^2 a lot, this means that that variable was an important predictor in our regression.

The results from this permutation test were the following: permuting Ranking decreased the pseudo R^2 on average by 0.3028. Since the pseudo R^2 metric is between 0 and 1, this is a big difference. Permuting the OverallGrade decreased the pseudo R^2 on average by 0.0078, showing that the Ranking is much more important than the OverallGrade. Permuting Age and IsContinuation had very small impact, with an average decrease of 0.0021 and 0.0012 respectively. From this regression, and the subsequent permutation test, we can conclude that the Internal Step is by far the most important step in the process.

### Most Important Criteria Within Each Step

The second aspect of this question was to identify what was the most important criteria within each step. To understand this, we again did a permutation test of the different predictors determining the summary grade given in each the external and the internal review step. We used the Ordinal Regressions from earlier: one for the external OverallGrade using the demographic data, Scientific Proposal grade, and Applicant Track grade as predictors, and a second one predicting the Ranking using the demographic data, Scientific Proposal grade, and Applicant Track grade as predictors. 

In each of these two regressions, we again computed the variable importance by permuting the values of the predictors one at a time. Since it is an Ordinal Regression, and there is no R^2 equivalent to measure the goodness of fit, we assessed goodness of fit based on the percent of variation in the AIC, another measure of goodness of fit. For both the external and the internal step, we found permuting the grade given to the Scientific Proposal by far had the biggest impact on the quality of the regression. This led us to conclude that the grade given to the Scientific Proposal far outweighs the grade given to the Applicant Track Record, or any of the demographic predictors, in explaining the overall grade given to an application.


## Results


# Budget Cuts

## Analysis

## Results

# Conclusion

# Appendix

## Detailed Data Description

### Applications
  * **AmountRequested**: Rounded to the next 10k CHF
  * **AmountGranted**: Rounded to the next 10k CHF
  * **IsApproved**: 1 if the application was approved, 0 if it was rejected
  * **GradeFinal**: Comparative ranking of the application as determined by the evaluation body (the division of the National Research Council). A: "belongs to the 10% best percent"; AB: "10% are worse, 75% are better"; B: "50% are worse, 25% are better"; BC: " 25% are worse, 50% are better"; C "10% are worse, 75% are better"; D: "90% of the applications are better"
  
  * **Division**: Evaluation Body in which the application was evaluated. Division 1 evaluates Social Sciences and Humanities; Division 2 Mathematics, Natural Sciences and Engineering; Division 3 Biology and Medicine
  
  * **MainDiscipline**: as chosen by the applicant from the SNSF discipline list
  * **MainDisciplineLevel2**: category in the SNF discipline list grouping disciplines into fields of research
  * **CallTitle**: Call for proposals under which the application was submitted. Applications from the same Call are evaluated together, i.e. in competition to each other
  * **CallEndDate**: Submission deadline of the Call
  * **ResponsibleApplicantAcademicAgeAtSubmission**: Years since the applicant's PhD at time of submission; data only available since mid 2016
  * **ResponsibleApplicantAgeAtSubmission**: Biological age of the applicant at time of submission; data only available since mid 2016
  * **ResponsibleApplicantProfessorshipType**: employment situation of the applicant at time of submission; data only available since mid 2016
  * **Gender**: of the main applicant
  * **NationalityIsoCode**: Nationality of the main applicant
  * **IsHasPreviousProjectRequested**: 0 if it is the applicant's first application at the SNSF, 1 if not
  * **InstType**: Type of institution where the applicant is employed
  * **IsContinuation**: 1 if the project is a thematic continuation of a previously approved project, 0 if not
  * **ProjectID**: Anonymized identifier of the application

### Referee Grades
  * **Question**: Evaluation criterion
  * **QuestionRating**: The (co-)referee's assessment of the evaluation criterion
  * **OverallRanking**: The (co-)referee's overall comparative ranking of the application. A: "belongs to the 10% best percent"; same scale as the GradeFinal
  * **RefereeRole**: Some applications have one referee evaluation, some have two. The role indicates who was the primary and who was the secondary referee (also called co-referee) 
  * **RefereeGender**
  * **IDs**: Anonymized identifiers of the application, the referee and the evaluation by the referee

### Reviews
  * **Question**: Evaluation criterion
  * **QuestionRating**: The external reviewer's assessment of the evaluation criterion
  * **OverallGrade**: The external reviewer's overall assessment of the application
  * **SourcePerson**: Who suggested the reviewer?
  * **Gender**
  * **Country**: where the reviewer is located. Not always known
  * **EmailEnding**: ending of the reviewer's email address. Might be used as an approximation of the country where the reviewer is located in cases where this data is missing
  * **IDs**: Anonymized identifiers of the application, the reviewer and the review
  
## Exploratory Analysis

## OverallGrade vs. IsApproved, by Division


```{r}
p5 <- plot_mirror_barplot(dataset=ex.div1, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Division 1")
p6 <- plot_mirror_barplot(dataset=ex.div2, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Division 2")
p7 <- plot_mirror_barplot(dataset=ex.div3, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Division 3")


p5
p6
p7


```

## OverallGrade vs. IsApproved, by Gender


```{r}

p8 <- plot_mirror_barplot(dataset=ex.f, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Female Applicants")
p9 <- plot_mirror_barplot(dataset=ex.m, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Male Applicants")

p8
p9
```
\newpage

## External Logistic Regression

* Summary of the final model
```{r}
summary(log.ext.Model)
```
```{r plot-effects, out.width='50%',fig.show="hold",fig.ncol=2, fig.cap="Effects of the external logistic regression"}

plot(Effect("ApplicantTrack",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("ScientificRelevance",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("Suitability",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("PercentFemale",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("Age",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("Division",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("IsContinuation",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("Semester",log.ext.Model),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
```

## Ordinal external Regression

### Project grades (ProposalCombined)

Summary of the final model:
    ```{r}
summary(Model.Prop)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.Prop$beta,confint(Model.Prop))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```
\pagebreak

### Applicant Track
Summary of the final model:
```{r}
summary(Model.App)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.App$beta,confint(Model.App))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```


### Overall Grade
Summary of the final model:
```{r}
summary(Model.Overall)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.Overall$beta,confint(Model.Overall))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```
