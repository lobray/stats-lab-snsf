---
title: "SNSF Report"
author: "Chiara Gilardi, Leslie O’Bray, Carla Schärer  and Tommaso Portaluri"
date: "5 April 2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, prompt = "    ")
```
```{r Load Functions and Data }
# setwd()
# load("SNFS Data/snsf_data.RData")
load("/home/leslie/Desktop/StatsLab/snsf_data.RData") # leslie's directory, comment out when other is using
source("Cleaning Functions.R")
source("Data for Regression.R")

# install.packages("biostatUZH", repos="http://R-Forge.R-project.org")

library(biostatUZH)
library(psy)
library(psych)
library(ggplot2)
library(gridExtra)
library(coin)
library(ggmosaic)

```


## Introduction
The Swiss National Science Foundation (SNF) is a research funding agency which disseminates yearlt, on behalf of the Swiss Government, billions of CHF to the best researchers in Switzerland. This report contains a statistical analysis performed on three datasets provided by SNF, cointaing information on the applications for funding received in 2016, the corresponding and the scores given by both internal and external evaluators.  

The analysis performed for SNF had a two-fold aim, corresponding to the following two research questions:
1) Is gender bias occurring at any stage of the SNSF evaluation process? Is the gender of the main applicant influencing the rating of the application?
2) To what extent the different steps of the evaluation and the different criteria determine the final funding decision?
The SNSF evaluation procedure is indeed a multi-step process (involving external reviewers, internal referees, and an internal board) which takes into consideration both the track record of the applicant and the quality of the project (see Appendinx for a more detailed description of the evaluation procedure).

Several studies (Witteman et al., 2017; Solans-Domenech et al., 2017) have shown that female applicants' projects get higher score when the application is blinded. Moreover,  female applicants receive usually higher grades for projects and lower grades for track record. Hence, after investigating the gender dimension to identify possible biases in the evaluation procedure, the focus of the analysis will be the relative importance of of the criteria for funding (applicant's track record vs. quality of the proposal) and, also, of each step of the evaluation procedure (which opinion is more likely to determine the final decision – the external referee's or the board's?). Possible interactions between the gender dimension and the second research question will also be investigated (for instance, by taking into account also the gender of evaluator or the percentage of female referees).

## Data Description

We have three data sets: Applications, External Reviewers and Internal Referees. They contain respectively information about the SNSF project funding applications, the evaluation of the applications by external peer reviewers and the evaluation of the proposals by external the internal referee and co-referee (when available). For a full description of the data & variables, please see the Appendix. 



# Cleaning the Data

We decide to work with only complete applications, i.e. project for which we have information from all the three data sets.

As we have only information from reviewers and referees since 2016, we are considering applications only from that year. 

Specific to each data set, this are the detailed considerations:

### Applications

We decide to consider only the MainDiscipline2 because for MainDiscipline we have 118 levels, while for the other only 21. 

There is one application for which we do not know the gender of the applicant, and therefore we decided to omit that observation from the analysis.

We will also not consider the variables "CallTitle", "Professorship", "AcademicAge". The first one, because we consider it has nothing to add to the model. The two last, due to the fact that there are a considerable number of NA's on those variables (around 93% of the observations).

```{r Table Professorship and AcademicAge}
# For Professorship
table(applications$ResponsibleApplicantProfessorshipType, useNA = "always")
sum(is.na(applications$ResponsibleApplicantProfessorshipType))/dim(applications)[1]

# For AcademicAge
sum(is.na(applications$ResponsibleApplicantAcademicAgeAtSubmission))/dim(applications)[1]
```

### External Reviewers

Reviewers always have the option to choose not to consider or to give the grade "0" when reviewing an application. Some might be mistakes, in others cases there might be a conflict of interest, or they might be very ambivalent about the project. Therefore, we did not considered observations with this grades. 

One of the questions evaluated in the applications is "Broader impact (forms part of the assessment of scientific relevance, originality and topicality)". For the time frame we are considering, in all the applications this grade was NA. Hence, we omit this variable from our model.

### Internal Referees

There were 22 observations (1 for the time frame we are dealing with) for which only demographic information was available, no grades were given. We decide to omit those observations.

Also we decide to not consider the Referee role as a variable in our model, as the majority of the evaluations has only one referee.

```{r Table RefereeRole}
table(referee_grades$RefereeRole, useNA = "always")
```




# Exploratory Analysis

```{r data to use for analysis}


#### External datasets to use
external_regression_data <- prepare_data_external_log_regression(apps=final.apps, external=final.external)
ex.f <- external_regression_data[external_regression_data$Gender=="f",]
ex.m <- external_regression_data[external_regression_data$Gender=="m",]
ex.div1 <- external_regression_data[external_regression_data$Division=="Div 1",] # particularly bad for Div 1
ex.div2 <- external_regression_data[external_regression_data$Division=="Div 2",]
ex.div3 <- external_regression_data[external_regression_data$Division=="Div 3",]


#### internal datasets to use 
internal_regression_data<-prepare_data_internal_log_regression(apps=final.apps, internal=final.internal)
in.f <- internal_regression_data[internal_regression_data$Gender=="f",]
in.m <- internal_regression_data[internal_regression_data$Gender=="m",]
in.div1 <- internal_regression_data[internal_regression_data$Division=="Div 1",] # particularly bad for Div 1
in.div2 <- internal_regression_data[internal_regression_data$Division=="Div 2",]
in.div3 <- internal_regression_data[internal_regression_data$Division=="Div 3",]



```


## Exploratory Analysis: measure agreement between stages of the process

First, let's check the histogram of the ApplicantTrack in the internal vs. external process. We see that the external reviewers score candidates higher in Outstanding & Excellent, whereas the internal process ranks more as "Very Good". This is consistent when looking just at women. 

```{r}


ex_applicant_track <- as.data.frame(prop.table(table(external_regression_data$ApplicantTrack)))
ex_applicant_track$Freq <- as.numeric(ex_applicant_track$Freq)
colnames(ex_applicant_track)[1] <- "ApplicantTrackScore"
ggplot(ex_applicant_track, aes(x=ApplicantTrackScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("External Applicant Track Frequency Distribution") + 
  geom_text(aes(label=round(Freq, 2),vjust=1.5))


in_applicant_track <- as.data.frame(prop.table(table(internal_regression_data$ApplicantTrack)))
in_applicant_track$Freq <- as.numeric(in_applicant_track$Freq)
colnames(in_applicant_track)[1] <- "ApplicantTrackScore"
ggplot(in_applicant_track, aes(x=ApplicantTrackScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("Internal Applicant Track Frequency Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5))


# prop.table(table(internal_regression_data$ApplicantTrack))

## Check if gender makes a difference
# prop.table(table(ex.f$ApplicantTrack))
# prop.table(table(in.f$ApplicantTrack))

```

Now if we examine if this is also true in the proposal, which it is. In the top three categories, the external reviewer gave 91% of proposals, versus on 60% in the internal review step. In the OverallGrade, external reviewers rated 89% of candidates in the top three, versus only 61% of internal rankings being in the top three categories. For applicant track, external reviews rated 95% of candidates in the top three categories, while internal rated 88% in the top three. It appears there is much more influence of the science proposal on the ranking/grade compared to the applicant track.

```{r, scientific proposal}

ProposalCombined <- calculate_combined_proposal_grade(final.external)$ProposalCombined
ex_proposal <- as.data.frame(prop.table(table(ProposalCombined)))
ex_proposal$Freq <- as.numeric(ex_proposal$Freq)
colnames(ex_proposal)[1] <- "ProposalScore"
ggplot(ex_proposal, aes(x=ProposalScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("External Proposal Frequency Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5))

in_proposal <- as.data.frame(prop.table(table(internal_regression_data$ProjectAssessment)))
in_proposal$Freq <- as.numeric(in_proposal$Freq)
colnames(in_proposal)[1] <- "ProposalScore"
ggplot(in_proposal, aes(x=ProposalScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("Internal Proposal Frequency Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5))



```


```{r, echo=FALSE}


sum(prop.table(table(external_regression_data$ApplicantTrack))[4:6])
sum(prop.table(table(internal_regression_data$ApplicantTrack))[4:6])


sum(prop.table(table(external_regression_data$ProposalCombined))[4:6])
sum(prop.table(table(internal_regression_data$ProjectAssessment))[4:6])

sum(prop.table(table(external_regression_data$OverallGrade))[4:6])
sum(prop.table(table(internal_regression_data$Ranking))[4:6])



```

We see almost moderate external and internal ratings of an applicant, and of the proposal. We see this using Cohen's kappas, which measures how closely two different raters assessed the candidate, and accounts for random aggrement. TO CHECK: make sure that the percent of grades (outstanding) are similar amongst the internal and external reviewers for each step. In the proposal, 37% of applicants recieved the same score from the internal and external reviewer. But this is again a flawed metric since it doesn't account for random agreement, which is why we use Cohen's Kappa.

### Agreement over the Applicant grade

```{r Cohens-Kappa}


calculate_applicant_kappa <- function(final.apps = final.apps, final.external = final.external, final.internal = final.internal) {
  # First, combine data of external & internal applicant rating
  kappa.internal.applicant <- prepare_data_internal_log_regression(final.apps,final.internal)
  kappa.external.applicant <- prepare_data_external_log_regression(final.apps, final.external)
  candidate.rating <- merge(x=kappa.external.applicant[,c("ProjectID", "ApplicantTrack")], 
                            y = kappa.internal.applicant[,c("ProjectID","ApplicantTrack")],
                            by="ProjectID")
  colnames(candidate.rating) <- c("ProjectID", "ApplicantTrackExt", "ApplicantTrackInt")
  
  # create matrix to show how external and internal raters rate applicant
  kappa.matrix.applicant <- matrix(0, nrow=6, ncol=6)
  colnames(kappa.matrix.applicant) <- 1:6
  rownames(kappa.matrix.applicant) <- 1:6

    # count number of times things agree/don't agree
  for (i in 1:nrow(candidate.rating)) {
    kappa.matrix.applicant[candidate.rating[i,][[2]],candidate.rating[i,][[3]]] <- 
      kappa.matrix.applicant[candidate.rating[i,][[2]],candidate.rating[i,][[3]]] + 1
  }

  # Define weight matrix
  linear.weights <- matrix(c(1, 0.8, 0.6, 0.4, 0.2, 0, 0.8, 1, 0.8, 0.6, 0.4, 0.2, 
         0.6, 0.8, 1, 0.8, 0.6, 0.4, 0.4, 0.6, 0.8, 1, 0.8, 0.6,
         0.2, 0.4, 0.6, 0.8, 1, 0.8, 0, 0.2, 0.4, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)


  applicant.kappa <- cohen.kappa(kappa.matrix.applicant, w=linear.weights)
}

print("Cohen's Kappa for the ApplicantTrack")
(applicant_kappa <- calculate_applicant_kappa(final.apps=final.apps, final.external=final.external, final.internal=final.internal))


```

### Agreement over the Project grade
```{r}

calculate_proposal_kappa <- function(final.apps = final.apps, final.external = final.external, final.internal = final.internal) {
  # First, combine data of external & internal applicant rating
  kappa.internal.proposal <- prepare_data_internal_log_regression(final.apps,final.internal)
  kappa.external.proposal <- prepare_data_external_log_regression(final.apps, final.external)
  kappa.external.proposal$ProposalCombined <- round((kappa.external.proposal$ScientificRelevance+
                                                            kappa.external.proposal$Suitability)/2, 0)
  proposal.rating <- merge(x=kappa.external.proposal[,c("ProjectID", "ProposalCombined")], 
                            y = kappa.internal.proposal[,c("ProjectID","ProjectAssessment")],
                            by="ProjectID")
  
  colnames(proposal.rating) <- c("ProjectID", "ProposalExt", "ProposalInt")
  
  # create matrix to show how external and internal raters rate applicant
  kappa.matrix.proposal <- matrix(0, nrow=6, ncol=6)
  colnames(kappa.matrix.proposal) <- 1:6
  rownames(kappa.matrix.proposal) <- 1:6

  # count number of times things agree/don't agree
  for (i in 1:nrow(proposal.rating)) {
    kappa.matrix.proposal[proposal.rating[i,][[2]],proposal.rating[i,][[3]]] <- 
      kappa.matrix.proposal[proposal.rating[i,][[2]],proposal.rating[i,][[3]]] + 1
  }
  print(sum(diag(kappa.matrix.proposal)) / 1643)
  # Define weight matrix
  linear.weights <- matrix(c(1, 0.8, 0.6, 0.4, 0.2, 0, 0.8, 1, 0.8, 0.6, 0.4, 0.2, 
         0.6, 0.8, 1, 0.8, 0.6, 0.4, 0.4, 0.6, 0.8, 1, 0.8, 0.6,
         0.2, 0.4, 0.6, 0.8, 1, 0.8, 0, 0.2, 0.4, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)
  
  custom.weights <- matrix(c(1, 0.8, 0.6, 0, 0, 0, 0, 1, 0.8, 0.6, 0, 0, 
         0.6, 0.8, 1, 0.8, 0.6, 0, 0, 0.6, 0.8, 1, 0.8, 0.6,
         0, 0, 0.6, 0.8, 1, 0.8, 0, 0, 0, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)


  proposal.kappa <- cohen.kappa(kappa.matrix.proposal, w=custom.weights) # try again with weights
  return(proposal.kappa)
}

print("Cohen's Kappa for the Scientific Proposal")
(proposal_kappa <- calculate_proposal_kappa(final.apps=final.apps, final.external=final.external, final.internal = final.internal))


```


```{r internal kappa with ranking}

library(psych)
library(biostatUZH)

calculate_ranking_kappa <- function(final.apps = final.apps, final.external = final.external, final.internal = final.internal) {
  # First, combine data of external & internal applicant rating
  kappa.internal.proposal <- prepare_data_internal_log_regression(final.apps,final.internal)
  
  proposal.rating <- kappa.internal.proposal[,c("ProjectID", "Ranking", "ApplicantTrack", "ProjectAssessment")]
  

  # create matrix to show how external and internal raters rate applicant
  kappa.matrix.proposal <- matrix(0, nrow=6, ncol=6)
  colnames(kappa.matrix.proposal) <- 1:6
  rownames(kappa.matrix.proposal) <- 1:6

  # count number of times things agree/don't agree
  kappa.matrix.project <- table(proposal.rating$Ranking, proposal.rating$ProjectAssessment)
  kappa.matrix.applicant <- table(proposal.rating$Ranking, proposal.rating$ApplicantTrack)
  
  # Define weight matrix
  linear.weights <- matrix(c(1, 0.8, 0.6, 0.4, 0.2, 0, 0.8, 1, 0.8, 0.6, 0.4, 0.2, 
         0.6, 0.8, 1, 0.8, 0.6, 0.4, 0.4, 0.6, 0.8, 1, 0.8, 0.6,
         0.2, 0.4, 0.6, 0.8, 1, 0.8, 0, 0.2, 0.4, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)
  
  custom.weights <- matrix(c(1, 0.8, 0.6, 0, 0, 0, 0, 1, 0.8, 0.6, 0, 0, 
         0.6, 0.8, 1, 0.8, 0.6, 0, 0, 0.6, 0.8, 1, 0.8, 0.6,
         0, 0, 0.6, 0.8, 1, 0.8, 0, 0, 0, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)


  proposal.kappa.project <- cohen.kappa(kappa.matrix.project, w=custom.weights) # try again with weights
  proposal.kappa.applicant <- cohen.kappa(kappa.matrix.applicant, w=custom.weights) # try again with weights
  return(list(project=proposal.kappa.project, applicant=proposal.kappa.applicant))
}

print("Cohen's Kappa for the Scientific Proposal")
(ranking_kappa <- calculate_ranking_kappa(final.apps=final.apps, final.external=final.external, final.internal = final.internal))
```

### External OverallGrade Cohen's Kappa with ApplicantTrack, ScientificProposal, & Suitability

```{r}
```{r internal kappa with ranking}

library(psych)
library(biostatUZH)

calculate_overallgrade_kappa <- function(final.apps = final.apps, final.external = final.external, final.internal = final.internal) {
  # First, combine data of external & internal applicant rating
  kappa.external.proposal <- prepare_data_external_log_regression(final.apps,final.external)
  
  proposal.rating <- kappa.external.proposal[,c("ProjectID", "OverallGrade", "ApplicantTrack", "ScientificRelevance", "Suitability")]
  

  # create matrix to show how external and internal raters rate applicant
  kappa.matrix.proposal <- matrix(0, nrow=6, ncol=6)
  colnames(kappa.matrix.proposal) <- 1:6
  rownames(kappa.matrix.proposal) <- 1:6

  # count number of times things agree/don't agree
  kappa.matrix.scientificrelevance <- table(proposal.rating$OverallGrade, proposal.rating$ScientificRelevance)
  kappa.matrix.suitability <- table(proposal.rating$OverallGrade, proposal.rating$Suitability)
  kappa.matrix.applicant <- table(proposal.rating$OverallGrade, proposal.rating$ApplicantTrack)

  # Define weight matrix
  linear.weights <- matrix(c(1, 0.8, 0.6, 0.4, 0.2, 0, 0.8, 1, 0.8, 0.6, 0.4, 0.2, 
         0.6, 0.8, 1, 0.8, 0.6, 0.4, 0.4, 0.6, 0.8, 1, 0.8, 0.6,
         0.2, 0.4, 0.6, 0.8, 1, 0.8, 0, 0.2, 0.4, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)
  
  custom.weights <- matrix(c(1, 0.8, 0.6, 0, 0, 0, 0, 1, 0.8, 0.6, 0, 0, 
         0.6, 0.8, 1, 0.8, 0.6, 0, 0, 0.6, 0.8, 1, 0.8, 0.6,
         0, 0, 0.6, 0.8, 1, 0.8, 0, 0, 0, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)


  proposal.kappa.scientificrelevance <- cohen.kappa(kappa.matrix.scientificrelevance, w=custom.weights) # try again with weights
  proposal.kappa.suitability <- cohen.kappa(kappa.matrix.suitability, w=custom.weights) # try again with weights
  proposal.kappa.applicant <- cohen.kappa(kappa.matrix.applicant, w=custom.weights) # try again with weights
  return(list(scientificrelevance=proposal.kappa.scientificrelevance, suitability=proposal.kappa.suitability,
              applicant=proposal.kappa.applicant))
}

print("Cohen's Kappa for the Scientific Proposal")
(overallgrade_kappa <- calculate_overallgrade_kappa(final.apps=final.apps, final.external=final.external, final.internal = final.internal))
```

```


## Analysis


## Results

## Conclusion

# Appendix

## Detailed Data Description

### Applications
  * **AmountRequested**: Rounded to the next 10k CHF
  * **AmountGranted**: Rounded to the next 10k CHF
  * **IsApproved**: 1 if the application was approved, 0 if it was rejected
  * **GradeFinal**: Comparative ranking of the application as determined by the evaluation body (the division of the National Research Council). A: "belongs to the 10% best percent"; AB: "10% are worse, 75% are better"; B: "50% are worse, 25% are better"; BC: " 25% are worse, 50% are better"; C "10% are worse, 75% are better"; D: "90% of the applications are better"
  
  * **Division**: Evaluation Body in which the application was evaluated. Division 1 evaluates Social Sciences and Humanities; Division 2 Mathematics, Natural Sciences and Engineering; Division 3 Biology and Medicine
  
  * **MainDiscipline**: as chosen by the applicant from the SNSF discipline list
  * **MainDisciplineLevel2**: category in the SNF discipline list grouping disciplines into fields of research
  * **CallTitle**: Call for proposals under which the application was submitted. Applications from the same Call are evaluated together, i.e. in competition to each other
  * **CallEndDate**: Submission deadline of the Call
  * **ResponsibleApplicantAcademicAgeAtSubmission**: Years since the applicant's PhD at time of submission; data only available since mid 2016
  * **ResponsibleApplicantAgeAtSubmission**: Biological age of the applicant at time of submission; data only available since mid 2016
  * **ResponsibleApplicantProfessorshipType**: employment situation of the applicant at time of submission; data only available since mid 2016
  * **Gender**: of the main applicant
  * **NationalityIsoCode**: Nationality of the main applicant
  * **IsHasPreviousProjectRequested**: 0 if it is the applicant's first application at the SNSF, 1 if not
  * **InstType**: Type of institution where the applicant is employed
  * **IsContinuation**: 1 if the project is a thematic continuation of a previously approved project, 0 if not
  * **ProjectID**: Anonymized identifier of the application

### Referee Grades
  * **Question**: Evaluation criterion
  * **QuestionRating**: The (co-)referee's assessment of the evaluation criterion
  * **OverallRanking**: The (co-)referee's overall comparative ranking of the application. A: "belongs to the 10% best percent"; same scale as the GradeFinal
  * **RefereeRole**: Some applications have one referee evaluation, some have two. The role indicates who was the primary and who was the secondary referee (also called co-referee) 
  * **RefereeGender**
  * **IDs**: Anonymized identifiers of the application, the referee and the evaluation by the referee

### Reviews
  * **Question**: Evaluation criterion
  * **QuestionRating**: The external reviewer's assessment of the evaluation criterion
  * **OverallGrade**: The external reviewer's overall assessment of the application
  * **SourcePerson**: Who suggested the reviewer?
  * **Gender**
  * **Country**: where the reviewer is located. Not always known
  * **EmailEnding**: ending of the reviewer's email address. Might be used as an approximation of the country where the reviewer is located in cases where this data is missing
  * **IDs**: Anonymized identifiers of the application, the reviewer and the review

