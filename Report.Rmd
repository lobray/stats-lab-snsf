---
title: "SNSF Report"
author: "Chiara Gilardi, Leslie O’Bray, Carla Schärer  and Tommaso Portaluri"
date: "5 April 2018"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, prompt = "    ")
```
```{r Load Functions and Data }

# load("SNFS Data/snsf_data.RData")
load("/home/leslie/Desktop/StatsLab/snsf_data.RData") # leslie's directory, comment out when other is using
source("Cleaning Functions.R")
source("Data for Regression.R")

# install.packages("biostatUZH", repos="http://R-Forge.R-project.org")

library(biostatUZH)
library(psy)
library(psych)

```


## Introduction

## Data Description

We have three data sets: Applications, External Reviewers and Internal Referees. They contain respectively information about the SNSF project funding applications, the evaluation of the applications by external peer reviewers and the evaluation of the proposals by external the internal referee and co-referee (when available). For a full description of the data & variables, please see the Appendix. 



# Cleaning the Data

We decide to work with only complete applications, i.e. project for which we have information from all the three data sets.

As we have only information from reviewers and referees since 2016, we are considering applications only from that year. 

Specific to each data set, this are the detailed considerations:

### Applications

We decide to consider only the MainDiscipline2 because for MainDiscipline we have 118 levels, while for the other only 21. 

There is one application for which we do not know the gender of the applicant, and therefore we decided to omit that observation from the analysis.

We will also not consider the variables "CallTitle", "Professorship", "AcademicAge". The first one, because we consider it has nothing to add to the model. The two last, due to the fact that there are a considerable number of NA's on those variables (around 93% of the observations).

```{r Table Professorship and AcademicAge}
# For Professorship
table(applications$ResponsibleApplicantProfessorshipType, useNA = "always")
sum(is.na(applications$ResponsibleApplicantProfessorshipType))/dim(applications)[1]

# For AcademicAge
sum(is.na(applications$ResponsibleApplicantAcademicAgeAtSubmission))/dim(applications)[1]
```

### External Reviewers

Reviewers always have the option to choose not to consider or to give the grade "0" when reviewing an application. Some might be mistakes, in others cases there might be a conflict of interest, or they might be very ambivalent about the project. Therefore, we did not considered observations with this grades. 

One of the questions evaluated in the applications is "Broader impact (forms part of the assessment of scientific relevance, originality and topicality)". For the time frame we are considering, in all the applications this grade was NA. Hence, we omit this variable from our model.

### Internal Referees

There were 22 observations (1 for the time frame we are dealing with) for which only demographic information was available, no grades were given. We decide to omit those observations.

Also we decide to not consider the Referee role as a variable in our model, as the majority of the evaluations has only one referee.

```{r Table RefereeRole}
table(referee_grades$RefereeRole, useNA = "always")
```




# Exploratory Analysis

```{r, echo=F}


external_regression_data <- prepare_data_external_log_regression(apps=final.apps, external=final.external)


# Look at the overall grades given to external reviewers and whether candidate is approved - like 50%. Then split by gender (Second line).
ex.f <- external_regression_data[external_regression_data$Gender=="f",]
ex.div1 <- external_regression_data[external_regression_data$Division=="Div 1",] # particularly bad for Div 1
ex.div2 <- external_regression_data[external_regression_data$Division=="Div 2",]
ex.div3 <- external_regression_data[external_regression_data$Division=="Div 3",]

(table(external_regression_data$OverallGrade, external_regression_data$IsApproved))
(table(ex.f$OverallGrade, ex.f$IsApproved))
(table(ex.div1$OverallGrade, ex.div1$IsApproved))
(table(ex.div2$OverallGrade, ex.div2$IsApproved))
(table(ex.div3$OverallGrade, ex.div3$IsApproved))

# External review of track record inversely used! 
(table(external_regression_data$ApplicantTrack, external_regression_data$IsApproved))
(table(ex.f$ApplicantTrack, ex.f$IsApproved))
(table(ex.div1$ApplicantTrack, ex.div1$IsApproved))
(table(ex.div2$ApplicantTrack, ex.div2$IsApproved))
(table(ex.div3$ApplicantTrack, ex.div3$IsApproved))


# External review of suitability and whether candidate is approved
(table(external_regression_data$Suitability, external_regression_data$IsApproved))
(table(ex.f$Suitability, ex.f$IsApproved))
(table(ex.div1$Suitability, ex.div1$IsApproved))
(table(ex.div2$Suitability, ex.div2$IsApproved))
(table(ex.div3$Suitability, ex.div3$IsApproved))

# External review of relevance and whether candidate is approved -- again like negatively used
(table(external_regression_data$ScientificRelevance, external_regression_data$IsApproved))
(table(ex.f$ScientificRelevance, ex.f$IsApproved))
(table(ex.div1$ScientificRelevance, ex.div1$IsApproved))
(table(ex.div2$ScientificRelevance, ex.div2$IsApproved))
(table(ex.div3$ScientificRelevance, ex.div3$IsApproved))

# Lets look at the internal data


internal_regression_data<-prepare_data_internal_log_regression(apps=final.apps, internal=final.internal)

in.f <- internal_regression_data[internal_regression_data$Gender=="f",]
in.div1 <- internal_regression_data[internal_regression_data$Division=="Div 1",] # particularly bad for Div 1
in.div2 <- internal_regression_data[internal_regression_data$Division=="Div 2",]
in.div3 <- internal_regression_data[internal_regression_data$Division=="Div 3",]

(table(internal_regression_data$Ranking, internal_regression_data$IsApproved))
(table(in.f$Ranking, in.f$IsApproved))
(table(in.div1$Ranking, in.div1$IsApproved))
(table(in.div2$Ranking, in.div2$IsApproved))
(table(in.div3$Ranking, in.div3$IsApproved))

# Internal review of track record inversely used! 
(table(internal_regression_data$ApplicantTrack, internal_regression_data$IsApproved))
(table(in.f$ApplicantTrack, in.f$IsApproved))
(table(in.div1$ApplicantTrack, in.div1$IsApproved))
(table(in.div2$ApplicantTrack, in.div2$IsApproved))
(table(in.div3$ApplicantTrack, in.div3$IsApproved))


# Internal review of proposal and whether candidate is approved -- again like negatively used
(table(internal_regression_data$ProjectAssessment, internal_regression_data$IsApproved))
(table(in.f$ProjectAssessment, in.f$IsApproved))
(table(in.div1$ProjectAssessment, in.div1$IsApproved))
(table(in.div2$ProjectAssessment, in.div2$IsApproved))
(table(in.div3$ProjectAssessment, in.div3$IsApproved))

```

#### Exploratory Analysis: measure agreement between stages of the process

```{r Cohens-Kappa}


calculate_applicant_kappa <- function(final.apps = final.apps, final.external = final.external, final.internal = final.internal) {
  # First, combine data of external & internal applicant rating
  kappa.internal.applicant <- prepare_data_internal_log_regression(final.apps,final.internal)
  kappa.external.applicant <- prepare_data_external_log_regression(final.apps, final.external)
  candidate.rating <- merge(x=kappa.external.applicant[,c("ProjectID", "ApplicantTrack")], 
                            y = kappa.internal.applicant[,c("ProjectID","ApplicantTrack")],
                            by="ProjectID")
  colnames(candidate.rating) <- c("ProjectID", "ApplicantTrackExt", "ApplicantTrackInt")
  
  # create matrix to show how external and internal raters rate applicant
  kappa.matrix.applicant <- matrix(0, nrow=6, ncol=6)
  colnames(kappa.matrix.applicant) <- 1:6
  rownames(kappa.matrix.applicant) <- 1:6

    # count number of times things agree/don't agree
  for (i in 1:nrow(candidate.rating)) {
    kappa.matrix.applicant[candidate.rating[i,][[2]],candidate.rating[i,][[3]]] <- 
      kappa.matrix.applicant[candidate.rating[i,][[2]],candidate.rating[i,][[3]]] + 1
  }

  # Define weight matrix
  # q.weights <- matrix(c(1, .75, 0, .75, 1, .75, 0, 0.75, 1),ncol=6, byrow=T)

  applicant.kappa <- cohen.kappa(kappa.matrix.applicant) # try again with weights
}

applicant_kappa <- calculate_applicant_kappa(final.apps=final.apps, final.external=final.external, final.internal=final.internal)

calculate_proposal_kappa <- function(final.apps = final.apps, final.external = final.external, final.internal = final.internal) {
  # First, combine data of external & internal applicant rating
  kappa.internal.proposal <- prepare_data_internal_log_regression(final.apps,final.internal)
  kappa.external.proposal <- prepare_data_external_log_regression(final.apps, final.external)
  kappa.external.proposal$ProposalCombined <- round((kappa.external.proposal$ScientificRelevance+
                                                            kappa.external.proposal$Suitability)/2, 0)
  proposal.rating <- merge(x=kappa.external.proposal[,c("ProjectID", "ProposalCombined")], 
                            y = kappa.internal.proposal[,c("ProjectID","ProjectAssessment")],
                            by="ProjectID")
  
  colnames(proposal.rating) <- c("ProjectID", "ProposalExt", "ProposalInt")
  
  # create matrix to show how external and internal raters rate applicant
  kappa.matrix.proposal <- matrix(0, nrow=6, ncol=6)
  colnames(kappa.matrix.proposal) <- 1:6
  rownames(kappa.matrix.proposal) <- 1:6

  # count number of times things agree/don't agree
  for (i in 1:nrow(proposal.rating)) {
    kappa.matrix.proposal[proposal.rating[i,][[2]],proposal.rating[i,][[3]]] <- 
      kappa.matrix.proposal[proposal.rating[i,][[2]],proposal.rating[i,][[3]]] + 1
  }

  # Define weight matrix
  # q.weights <- matrix(c(1, .75, 0, .75, 1, .75, 0, 0.75, 1),ncol=6, byrow=T)

  proposal.kappa <- cohen.kappa(kappa.matrix.proposal) # try again with weights
  return(proposal.kappa)
}


proposal_kappa <- calculate_proposal_kappa(final.apps=final.apps, final.external=final.external, final.internal = final.internal)
print(proposal_kappa)

```
We see almost no agreement between the external and internal ratings of an applicant, and of the proposal. We see this using Cohen's kappas, which measures how closely two different raters assessed the candidate, and accounts for random aggrement. TO CHECK: make sure that the percent of grades (outstanding) are similar amongst the internal and external reviewers for each step. 

Alternative way of displaying the tables above to save space in report:

#### External Reviewers
```{r}
# External Reviewers
library(expss)
ER<-apply_labels(external_regression_data,
                                   ProjectID="ProjectID",
                                   ApplicantTrack="Main Applicant Track",
                                   ApplicantTrack=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                              "excellent" = 5, "outstanding" = 6 ),  
                                   ScientificRelevance="ScientificRelevance",
                                   ScientificRelevance=c( "poor"= 1, "average"= 2, "good" = 3, 
                                                          "very good" = 4,"excellent" = 5, "outstanding" = 6 ),
                                   Suitability="Suitability",
                                   Suitability=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                                  "excellent" = 5, "outstanding" = 6 ),
                                   OverallGrade="Overall Reviewer grade",
                                   OverallGrade=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                                  "excellent" = 5, "outstanding" = 6 ),
                                   PercentFemale="Percentage of females in the reviewers",
                                   IsApproved="Result",
                                   IsApproved=c("Accepted"=1,"Rejected"=0),
                                   Age= "Age",
                                   Gender="Gender",
                                   Division="Division"
                                   )

# This only work if you knit to HTML

expss_output_rnotebook()
drop_empty_columns(calculate(ER,cro(OverallGrade, list(total(),
                                                       Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(ER,cro(ApplicantTrack,list(total(),
                                                        Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(ER,cro(ScientificRelevance, list(total(),
                                                              Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(ER,cro(Suitability, list(total(),
                                                      Gender%nest%IsApproved,Division%nest%IsApproved))))

```
#### Internal Reviewers

```{r}
# Internal Reviewers
library(expss)
IR<-apply_labels(internal_regression_data,
                                       ProjectID="ProjectID",
                                       IsApproved="Result",
                                       IsApproved=c("Accepted"=1,"Rejected"=0),
                                       Gender="Gender",
                                       Division="Division",
                                       Age= "Age",
                                       PercentFemale="Percentage of females in the reviewers",
                                       ApplicantTrack="Main Applicant Track",
                                       ApplicantTrack=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                                  "excellent" = 5, "outstanding" = 6 ),
                                       ProjectAssessment= "Project Assessment",
                                       ProjectAssessment=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                                  "excellent" = 5, "outstanding" = 6 ),
                                       Ranking="Overall Comparative Ranking",
                                       Ranking=c( "D"= 1, "C"= 2, "BC" = 3, "B" = 4, "AB" = 5, "A" = 6 )  
                                       )

# This only work if you knit to HTML

expss_output_rnotebook()
calculate(IR,cro(Ranking,IsApproved))
drop_empty_columns(calculate(IR,cro(Ranking, list(total(),Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(IR,cro(ApplicantTrack,list(total(),Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(IR,cro(ProjectAssessment, list(total(),Gender%nest%IsApproved,Division%nest%IsApproved))))


```



```{r Visual analysis}
#### LESLIE COMMENTED OUT THIS CHUNK JUST BECAUSE THERE IS AN ERROR PREVENTING THE DOCUMENT FROM KNITTING, AND HADNT FIGURED OUT WHERE THE BUG IS. 

# library(vcd)
# cotabplot(~ Gender+IsApproved, data=final.apps, shade=T)
# cotabplot(~ IsApproved + Gender | Division, data = final.apps, shade = TRUE)
# # Apparently there is no association between gender and approval (generally + in each division)
# 
# cotabplot(~ Gender + Division, data=final.apps, shade=T)
# # There are more women then men applying for Division 1 and the opposite for Division 2
# 
# cotabplot(~ IsContinuation + IsApproved, data=final.apps, shade=T)
# # If the project is a continuation it is more likely to be approved
# 
# id.approved <- which(final.apps$IsApproved==1)
# perc.f <- final.apps$AmountGranted[id.approved]/final.apps$AmountRequested[id.approved]*100
# boxplot(perc.f~final.apps$Division[id.approved])
# # Percentage of amount granted of the amount requested:
# # On average in Division1 applicants receive almost all the amount of money requested, 
# # even if there are lots of outliers. Division 2 is the one with the smallest percentage of
# # money granted.
# 
# boxplot(final.apps$AmountRequested~final.apps$Division)
# # Probably the previous result is due to the fact that in Division1, the AmountRequested
# # on average is smaller than in all the other divisions.
# 
# # EXTERNAL REVIEWERS
# cotabplot(~ Gender + OverallGrade, data=external_regression_data, shade=T)
# cotabplot(~ Gender + ApplicantTrack, data=external_regression_data, shade=T)
# cotabplot(~ Gender + ScientificRelevance, data=external_regression_data, shade=T)
# cotabplot(~ Gender + Suitability, data=external_regression_data, shade=T)
# # No gender bias for any criteria
# 
#     external_regression_data$SimplifiedOverallGrade <- ifelse(external_regression_data$OverallGrade < 4, 0, 1)
#     external_regression_data$SimplifiedProposal <- ifelse(external_regression_data$ProposalCombined < 4, 0, 1)
#     external_regression_data$SimplifiedApplicant <- ifelse(external_regression_data$ApplicantTrack < 4, 0, 1)
#     
# # Even using the simplified version with 0 and 1 there is no bias
# cotabplot(~ Gender + SimplifiedOverallGrade, data=external_regression_data, shade=T)
# cotabplot(~ Gender + SimplifiedApplicant, data=external_regression_data, shade=T)
# cotabplot(~ Gender + SimplifiedProposal, data=external_regression_data, shade=T)
# 
# # INTERNAL REVIEWERS
# # Here it seem that is more likely to get an high grade for the TrackRecord if 
# # the applicant is male!
# cotabplot(~ Gender + Ranking, data=internal_regression_data, shade=T)
# cotabplot(~ Gender + ApplicantTrack, data=internal_regression_data, shade=T)
# cotabplot(~ Gender + ProjectAssessment, data=internal_regression_data, shade=T)
```


## Analysis


## Results

## Conclusion

# Appendix

## Detailed Data Description

### Applications
  * **AmountRequested**: Rounded to the next 10k CHF
  * **AmountGranted**: Rounded to the next 10k CHF
  * **IsApproved**: 1 if the application was approved, 0 if it was rejected
  * **GradeFinal**: Comparative ranking of the application as determined by the evaluation body (the division of the National Research Council). A: "belongs to the 10% best percent"; AB: "10% are worse, 75% are better"; B: "50% are worse, 25% are better"; BC: " 25% are worse, 50% are better"; C "10% are worse, 75% are better"; D: "90% of the applications are better"
  
  * **Division**: Evaluation Body in which the application was evaluated. Division 1 evaluates Social Sciences and Humanities; Division 2 Mathematics, Natural Sciences and Engineering; Division 3 Biology and Medicine
  
  * **MainDiscipline**: as chosen by the applicant from the SNSF discipline list
  * **MainDisciplineLevel2**: category in the SNF discipline list grouping disciplines into fields of research
  * **CallTitle**: Call for proposals under which the application was submitted. Applications from the same Call are evaluated together, i.e. in competition to each other
  * **CallEndDate**: Submission deadline of the Call
  * **ResponsibleApplicantAcademicAgeAtSubmission**: Years since the applicant's PhD at time of submission; data only available since mid 2016
  * **ResponsibleApplicantAgeAtSubmission**: Biological age of the applicant at time of submission; data only available since mid 2016
  * **ResponsibleApplicantProfessorshipType**: employment situation of the applicant at time of submission; data only available since mid 2016
  * **Gender**: of the main applicant
  * **NationalityIsoCode**: Nationality of the main applicant
  * **IsHasPreviousProjectRequested**: 0 if it is the applicant's first application at the SNSF, 1 if not
  * **InstType**: Type of institution where the applicant is employed
  * **IsContinuation**: 1 if the project is a thematic continuation of a previously approved project, 0 if not
  * **ProjectID**: Anonymized identifier of the application

### Referee Grades
  * **Question**: Evaluation criterion
  * **QuestionRating**: The (co-)referee's assessment of the evaluation criterion
  * **OverallRanking**: The (co-)referee's overall comparative ranking of the application. A: "belongs to the 10% best percent"; same scale as the GradeFinal
  * **RefereeRole**: Some applications have one referee evaluation, some have two. The role indicates who was the primary and who was the secondary referee (also called co-referee) 
  * **RefereeGender**
  * **IDs**: Anonymized identifiers of the application, the referee and the evaluation by the referee

### Reviews
  * **Question**: Evaluation criterion
  * **QuestionRating**: The external reviewer's assessment of the evaluation criterion
  * **OverallGrade**: The external reviewer's overall assessment of the application
  * **SourcePerson**: Who suggested the reviewer?
  * **Gender**
  * **Country**: where the reviewer is located. Not always known
  * **EmailEnding**: ending of the reviewer's email address. Might be used as an approximation of the country where the reviewer is located in cases where this data is missing
  * **IDs**: Anonymized identifiers of the application, the reviewer and the review

