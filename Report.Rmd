---
title: "SNSF Report"
author: "Chiara Gilardi, Leslie O’Bray, Carla Schärer  and Tommaso Portaluri"
date: "5 April 2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, prompt = "    ")
```
```{r Load Functions and Data }
# setwd()
# load("SNFS Data/snsf_data.RData")
load("/home/leslie/Desktop/StatsLab/snsf_data.RData") # leslie's directory, comment out when other is using
source("Cleaning Functions.R")
source("Data for Regression.R")

# install.packages("biostatUZH", repos="http://R-Forge.R-project.org")

library(biostatUZH)
library(psy)
library(psych)
library(ggplot2)
library(gridExtra)
library(coin)
library(ggmosaic)

```


## Introduction
The Swiss National Science Foundation (SNF) is a research funding agency which disseminates yearlt, on behalf of the Swiss Government, billions of CHF to the best researchers in Switzerland. This report contains a statistical analysis performed on three datasets provided by SNF, cointaing information on the applications for funding received in 2016, the corresponding and the scores given by both internal and external evaluators.  

The analysis performed for SNF had a two-fold aim, corresponding to the following two research questions:
1) Is gender bias occurring at any stage of the SNSF evaluation process? Is the gender of the main applicant influencing the rating of the application?
2) To what extent the different steps of the evaluation and the different criteria determine the final funding decision?
The SNSF evaluation procedure is indeed a multi-step process (involving external reviewers, internal referees, and an internal board) which takes into consideration both the track record of the applicant and the quality of the project (see Appendinx for a more detailed description of the evaluation procedure).

Several studies (Witteman et al., 2017; Solans-Domenech et al., 2017) have shown that female applicants' projects get higher score when the application is blinded. Moreover,  female applicants receive usually higher grades for projects and lower grades for track record. Hence, after investigating the gender dimension to identify possible biases in the evaluation procedure, the focus of the analysis will be the relative importance of of the criteria for funding (applicant's track record vs. quality of the proposal) and, also, of each step of the evaluation procedure (which opinion is more likely to determine the final decision – the external referee's or the board's?). Possible interactions between the gender dimension and the second research question will also be investigated (for instance, by taking into account also the gender of evaluator or the percentage of female referees).

## Data Description

We have three data sets: Applications, External Reviewers and Internal Referees. They contain respectively information about the SNSF project funding applications, the evaluation of the applications by external peer reviewers and the evaluation of the proposals by external the internal referee and co-referee (when available). For a full description of the data & variables, please see the Appendix. 



# Cleaning the Data

We decide to work with only complete applications, i.e. project for which we have information from all the three data sets.

As we have only information from reviewers and referees since 2016, we are considering applications only from that year. 

Specific to each data set, this are the detailed considerations:

### Applications

We decide to consider only the MainDiscipline2 because for MainDiscipline we have 118 levels, while for the other only 21. 

There is one application for which we do not know the gender of the applicant, and therefore we decided to omit that observation from the analysis.

We will also not consider the variables "CallTitle", "Professorship", "AcademicAge". The first one, because we consider it has nothing to add to the model. The two last, due to the fact that there are a considerable number of NA's on those variables (around 93% of the observations).

```{r Table Professorship and AcademicAge}
# For Professorship
table(applications$ResponsibleApplicantProfessorshipType, useNA = "always")
sum(is.na(applications$ResponsibleApplicantProfessorshipType))/dim(applications)[1]

# For AcademicAge
sum(is.na(applications$ResponsibleApplicantAcademicAgeAtSubmission))/dim(applications)[1]
```

### External Reviewers

Reviewers always have the option to choose not to consider or to give the grade "0" when reviewing an application. Some might be mistakes, in others cases there might be a conflict of interest, or they might be very ambivalent about the project. Therefore, we did not considered observations with this grades. 

One of the questions evaluated in the applications is "Broader impact (forms part of the assessment of scientific relevance, originality and topicality)". For the time frame we are considering, in all the applications this grade was NA. Hence, we omit this variable from our model.

### Internal Referees

There were 22 observations (1 for the time frame we are dealing with) for which only demographic information was available, no grades were given. We decide to omit those observations.

Also we decide to not consider the Referee role as a variable in our model, as the majority of the evaluations has only one referee.

```{r Table RefereeRole}
table(referee_grades$RefereeRole, useNA = "always")
```




# Exploratory Analysis

```{r, echo=F}
########################################################################################################################
# Function to show mirrored bar plots of external criteria versus approved / not approved step
########################################################################################################################
plot_mirror_barplot <- function(dataset, variable1, variable2="IsApproved", plot_title="Plot Title", title_size=8) {
  table_data_frame <- as.data.frame(table(dataset[,variable1], dataset[,"IsApproved"]))
  colnames(table_data_frame) <- c(variable1, variable2, "Frequency")
  levels(table_data_frame[,variable1]) <- list("Outstanding"="6", "Excellent"="5", 
                                                "Very Good"="4", "Good"="3", "Average"="2", "Poor"="1")
  table_data_frame$Frequency[table_data_frame[,"IsApproved"]==0] <- -table_data_frame$Frequency[table_data_frame[,"IsApproved"]==0]
  
  ggplot(table_data_frame, aes_string(x=variable1, y="Frequency", fill=variable2)) + 
    geom_bar(stat="identity", position="identity") + 
    scale_y_continuous(breaks=seq(-100,100,by=50),labels=abs(seq(-100,100,by=50))) +
    coord_flip() +
    ggtitle(plot_title) + 
    theme(plot.title = element_text(size = title_size))
  
}
```
## Exploratory Analysis: Mirrored bar plots
### External Review Step: Relationship of the different grades on IsApproved

#### OverallGrade and IsApproved 
```{r, echo=F}

#### External datasets to use in mirror barplots
external_regression_data <- prepare_data_external_log_regression(apps=final.apps, external=final.external)
ex.f <- external_regression_data[external_regression_data$Gender=="f",]
ex.m <- external_regression_data[external_regression_data$Gender=="m",]
ex.div1 <- external_regression_data[external_regression_data$Division=="Div 1",] # particularly bad for Div 1
ex.div2 <- external_regression_data[external_regression_data$Division=="Div 2",]
ex.div3 <- external_regression_data[external_regression_data$Division=="Div 3",]


### Plot the overall effect, as well as by gender and then by division

p1 <- plot_mirror_barplot(dataset=external_regression_data, variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; All Data")
p2 <- plot_mirror_barplot(dataset=ex.f, variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved, Female")
p3 <- plot_mirror_barplot(dataset=ex.m, variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved, Male")
p4 <- plot_mirror_barplot(dataset=ex.div1, variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved, Div1")
p5 <- plot_mirror_barplot(dataset=ex.div2, variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved, Div2")
p6 <- plot_mirror_barplot(dataset=ex.div3, variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved, Div3")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)

```

#### ApplicantTrack and IsApproved 

``` {r, echo=F}


#### External datasets to use in mirror barplots
external_regression_data <- prepare_data_external_log_regression(apps=final.apps, external=final.external)
ex.f <- external_regression_data[external_regression_data$Gender=="f",]
ex.m <- external_regression_data[external_regression_data$Gender=="m",]
ex.div1 <- external_regression_data[external_regression_data$Division=="Div 1",] # particularly bad for Div 1
ex.div2 <- external_regression_data[external_regression_data$Division=="Div 2",]
ex.div3 <- external_regression_data[external_regression_data$Division=="Div 3",]

p1 <- plot_mirror_barplot(dataset=external_regression_data, variable1 = "ApplicantTrack", plot_title = "ApplicantTrack vs. Approved; All Data")
p2 <- plot_mirror_barplot(dataset=ex.f, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Female")
p3 <- plot_mirror_barplot(dataset=ex.m, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Male")
p4 <- plot_mirror_barplot(dataset=ex.div1, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs.Approved, Div1")
p5 <- plot_mirror_barplot(dataset=ex.div2, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Div2")
p6 <- plot_mirror_barplot(dataset=ex.div3, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Div3")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)

```


#### ScientificProposal and IsApproved 

``` {r, echo=F}


#### External datasets to use in mirror barplots
external_regression_data <- prepare_data_external_log_regression(apps=final.apps, external=final.external)
ex.f <- external_regression_data[external_regression_data$Gender=="f",]
ex.m <- external_regression_data[external_regression_data$Gender=="m",]
ex.div1 <- external_regression_data[external_regression_data$Division=="Div 1",] # particularly bad for Div 1
ex.div2 <- external_regression_data[external_regression_data$Division=="Div 2",]
ex.div3 <- external_regression_data[external_regression_data$Division=="Div 3",]

p1 <- plot_mirror_barplot(dataset=external_regression_data, variable1 = "ScientificRelevance", plot_title = "External Scientific Proposal vs. IsApproved; All Data")
p2 <- plot_mirror_barplot(dataset=ex.f, variable1 = "ScientificRelevance", plot_title = "External Scientific Proposal vs. IsApproved, Female")
p3 <- plot_mirror_barplot(dataset=ex.m, variable1 = "ScientificRelevance", plot_title = "External Scientific Proposal vs. IsApproved, Male")
p4 <- plot_mirror_barplot(dataset=ex.div1, variable1 = "ScientificRelevance", plot_title = "External Scientific Proposal vs. IsApproved, Div1")
p5 <- plot_mirror_barplot(dataset=ex.div2, variable1 = "ScientificRelevance", plot_title = "External Scientific Proposal vs. IsApproved, Div2")
p6 <- plot_mirror_barplot(dataset=ex.div3, variable1 = "ScientificRelevance", plot_title = "External Scientific Proposal vs. IsApproved, Div3")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)
```


#### Suitability and IsApproved

``` {r, echo=F}


#### External datasets to use in mirror barplots
external_regression_data <- prepare_data_external_log_regression(apps=final.apps, external=final.external)
ex.f <- external_regression_data[external_regression_data$Gender=="f",]
ex.m <- external_regression_data[external_regression_data$Gender=="m",]
ex.div1 <- external_regression_data[external_regression_data$Division=="Div 1",] # particularly bad for Div 1
ex.div2 <- external_regression_data[external_regression_data$Division=="Div 2",]
ex.div3 <- external_regression_data[external_regression_data$Division=="Div 3",]

p1 <- plot_mirror_barplot(dataset=external_regression_data, variable1 = "Suitability", plot_title = "External Suitability vs. IsApproved; All Data")
p2 <- plot_mirror_barplot(dataset=ex.f, variable1 = "Suitability", plot_title = "External Suitability vs. IsApproved, Female")
p3 <- plot_mirror_barplot(dataset=ex.m, variable1 = "Suitability", plot_title = "External Suitability vs. IsApproved, Male")
p4 <- plot_mirror_barplot(dataset=ex.div1, variable1 = "Suitability", plot_title = "External Suitability vs. IsApproved, Div1")
p5 <- plot_mirror_barplot(dataset=ex.div2, variable1 = "Suitability", plot_title = "External Suitability vs. IsApproved, Div2")
p6 <- plot_mirror_barplot(dataset=ex.div3, variable1 = "Suitability", plot_title = "External Suitability vs. IsApproved, Div3")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)

```


### Internal Review Step: Relationship of the different grades on IsApproved

#### Ranking and IsApproved
```{r, echo=F}

#### internal datasets to use in mirror barplots
internal_regression_data<-prepare_data_internal_log_regression(apps=final.apps, internal=final.internal)
in.f <- internal_regression_data[internal_regression_data$Gender=="f",]
in.m <- internal_regression_data[internal_regression_data$Gender=="m",]
in.div1 <- internal_regression_data[internal_regression_data$Division=="Div 1",] # particularly bad for Div 1
in.div2 <- internal_regression_data[internal_regression_data$Division=="Div 2",]
in.div3 <- internal_regression_data[internal_regression_data$Division=="Div 3",]


### Plot the Ranking effect, as well as by gender and then by division

p1 <- plot_mirror_barplot(dataset=internal_regression_data, variable1 = "Ranking", plot_title = "Ranking vs. Approved; All Data")
p2 <- plot_mirror_barplot(dataset=in.f, variable1 = "Ranking", plot_title = "Ranking vs. Approved, Female")
p3 <- plot_mirror_barplot(dataset=in.m, variable1 = "Ranking", plot_title = "Ranking vs. Approved, Male")
p4 <- plot_mirror_barplot(dataset=in.div1, variable1 = "Ranking", plot_title = "Ranking vs. Approved, Div1")
p5 <- plot_mirror_barplot(dataset=in.div2, variable1 = "Ranking", plot_title = "Ranking vs. Approved, Div2")
p6 <- plot_mirror_barplot(dataset=in.div3, variable1 = "Ranking", plot_title = "Ranking vs. Approved, Div3")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)

```

#### ApplicantTrack and IsApproved

``` {r, echo=F}


#### internal datasets to use in mirror barplots
internal_regression_data <- prepare_data_internal_log_regression(apps=final.apps, internal=final.internal)
in.f <- internal_regression_data[internal_regression_data$Gender=="f",]
in.m <- internal_regression_data[internal_regression_data$Gender=="m",]
in.div1 <- internal_regression_data[internal_regression_data$Division=="Div 1",] # particularly bad for Div 1
in.div2 <- internal_regression_data[internal_regression_data$Division=="Div 2",]
in.div3 <- internal_regression_data[internal_regression_data$Division=="Div 3",]

p1 <- plot_mirror_barplot(dataset=internal_regression_data, variable1 = "ApplicantTrack", plot_title = "ApplicantTrack vs. Approved; All Data")
p2 <- plot_mirror_barplot(dataset=in.f, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Female")
p3 <- plot_mirror_barplot(dataset=in.m, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Male")
p4 <- plot_mirror_barplot(dataset=in.div1, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Div1")
p5 <- plot_mirror_barplot(dataset=in.div2, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Div2")
p6 <- plot_mirror_barplot(dataset=in.div3, variable1 = "ApplicantTrack", plot_title = "Applicant Track vs. Approved, Div3")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)

```


#### ScientificProposal and IsApproved

``` {r, echo=F}


#### internal datasets to use in mirror barplots
internal_regression_data <- prepare_data_internal_log_regression(apps=final.apps, internal=final.internal)
in.f <- internal_regression_data[internal_regression_data$Gender=="f",]
in.m <- internal_regression_data[internal_regression_data$Gender=="m",]
in.div1 <- internal_regression_data[internal_regression_data$Division=="Div 1",] 
in.div2 <- internal_regression_data[internal_regression_data$Division=="Div 2",]
in.div3 <- internal_regression_data[internal_regression_data$Division=="Div 3",]

p1 <- plot_mirror_barplot(dataset=internal_regression_data, variable1 = "ProjectAssessment", plot_title = "Scientific Proposal vs. Approved; All Data")
p2 <- plot_mirror_barplot(dataset=in.f, variable1 = "ProjectAssessment", plot_title = "Scientific Proposal vs. Approved, Female")
p3 <- plot_mirror_barplot(dataset=in.m, variable1 = "ProjectAssessment", plot_title = "Scientific Proposal vs. Approved, Male")
p4 <- plot_mirror_barplot(dataset=in.div1, variable1 = "ProjectAssessment", plot_title = "Scientific Proposal vs. Approved, Div1")
p5 <- plot_mirror_barplot(dataset=in.div2, variable1 = "ProjectAssessment", plot_title = "Scientific Proposal vs. Approved, Div2")
p6 <- plot_mirror_barplot(dataset=in.div3, variable1 = "ProjectAssessment", plot_title = "Scientific Proposal vs. Approved, Div3")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2)
```



## Exploratory Analysis: measure agreement between stages of the process

First, let's check the histogram of the ApplicantTrack in the internal vs. external process. We see that the external reviewers score candidates higher in Outstanding & Excellent, whereas the internal process ranks more as "Very Good". This is consistent when looking just at women. 

```{r}


ex_applicant_track <- as.data.frame(prop.table(table(external_regression_data$ApplicantTrack)))
ex_applicant_track$Freq <- as.numeric(ex_applicant_track$Freq)
colnames(ex_applicant_track)[1] <- "ApplicantTrackScore"
ggplot(ex_applicant_track, aes(x=ApplicantTrackScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("External Applicant Track Frequency Distribution") + 
  geom_text(aes(label=round(Freq, 2),vjust=1.5))

in_applicant_track <- as.data.frame(prop.table(table(internal_regression_data$ApplicantTrack)))
in_applicant_track$Freq <- as.numeric(in_applicant_track$Freq)
colnames(in_applicant_track)[1] <- "ApplicantTrackScore"
ggplot(in_applicant_track, aes(x=ApplicantTrackScore, y= Freq)) +
  geom_histogram(stat="identity") +
  ggtitle("Internal Applicant Track Frequency Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5))


prop.table(table(internal_regression_data$ApplicantTrack))

## Check if gender makes a difference
prop.table(table(ex.f$ApplicantTrack))
prop.table(table(in.f$ApplicantTrack))

```

Now if we examine if this is also true in the proposal, which it is. In the top three categories, the external reviewer gave 91% of proposals, versus on 60% in the internal review step. In the OverallGrade, external reviewers rated 89% of candidates in the top three, versus only 61% of internal rankings being in the top three categories. For applicant track, external reviews rated 95% of candidates in the top three categories, while internal rated 88% in the top three. It appears there is much more influence of the science proposal on the ranking/grade compared to the applicant track.

```{r, scientific proposal}

# leslie NEEDS TO BE FIXED -- calculate sUMMARY METRIC BEFORE FACTOR! wON'T KNIT BECAUSE OF THIS. PROBLEM IS THAT WE CONVERTED BACK TO FACTOR. 
# 
# # Calculate summary metric for proposal 
# external_regression_data$ProposalCombined <-
#   round((external_regression_data$ScientificRelevance+external_regression_data$Suitability)/2, 0)
# 
# ex_proposal <- as.data.frame(prop.table(table(external_regression_data$ProposalCombined)))
# ex_proposal$Freq <- as.numeric(ex_proposal$Freq)
# colnames(ex_proposal)[1] <- "ProposalScore"
# ggplot(ex_proposal, aes(x=ProposalScore, y= Freq)) +
#   geom_histogram(stat="identity") +
#   ggtitle("External Proposal Frequency Distribution") +
#   geom_text(aes(label=round(Freq, 2),vjust=1.5))
# 
# in_proposal <- as.data.frame(prop.table(table(internal_regression_data$ProjectAssessment)))
# in_proposal$Freq <- as.numeric(in_proposal$Freq)
# colnames(in_proposal)[1] <- "ProposalScore"
# ggplot(in_proposal, aes(x=ProposalScore, y= Freq)) +
#   geom_histogram(stat="identity") +
#   ggtitle("Internal Proposal Frequency Distribution") +
#   geom_text(aes(label=round(Freq, 2),vjust=1.5))
# 


```


```{r, echo=FALSE}


sum(prop.table(table(external_regression_data$ApplicantTrack))[4:6])
sum(prop.table(table(internal_regression_data$ApplicantTrack))[4:6])


sum(prop.table(table(external_regression_data$ProposalCombined))[4:6])
sum(prop.table(table(internal_regression_data$ProjectAssessment))[4:6])

sum(prop.table(table(external_regression_data$OverallGrade))[4:6])
sum(prop.table(table(internal_regression_data$Ranking))[4:6])



```

We see almost moderate external and internal ratings of an applicant, and of the proposal. We see this using Cohen's kappas, which measures how closely two different raters assessed the candidate, and accounts for random aggrement. TO CHECK: make sure that the percent of grades (outstanding) are similar amongst the internal and external reviewers for each step. In the proposal, 37% of applicants recieved the same score from the internal and external reviewer. But this is again a flawed metric since it doesn't account for random agreement, which is why we use Cohen's Kappa.

### Agreement over the Applicant grade

```{r Cohens-Kappa}


calculate_applicant_kappa <- function(final.apps = final.apps, final.external = final.external, final.internal = final.internal) {
  # First, combine data of external & internal applicant rating
  kappa.internal.applicant <- prepare_data_internal_log_regression(final.apps,final.internal)
  kappa.external.applicant <- prepare_data_external_log_regression(final.apps, final.external)
  candidate.rating <- merge(x=kappa.external.applicant[,c("ProjectID", "ApplicantTrack")], 
                            y = kappa.internal.applicant[,c("ProjectID","ApplicantTrack")],
                            by="ProjectID")
  colnames(candidate.rating) <- c("ProjectID", "ApplicantTrackExt", "ApplicantTrackInt")
  
  # create matrix to show how external and internal raters rate applicant
  kappa.matrix.applicant <- matrix(0, nrow=6, ncol=6)
  colnames(kappa.matrix.applicant) <- 1:6
  rownames(kappa.matrix.applicant) <- 1:6

    # count number of times things agree/don't agree
  for (i in 1:nrow(candidate.rating)) {
    kappa.matrix.applicant[candidate.rating[i,][[2]],candidate.rating[i,][[3]]] <- 
      kappa.matrix.applicant[candidate.rating[i,][[2]],candidate.rating[i,][[3]]] + 1
  }

  # Define weight matrix
  linear.weights <- matrix(c(1, 0.8, 0.6, 0.4, 0.2, 0, 0.8, 1, 0.8, 0.6, 0.4, 0.2, 
         0.6, 0.8, 1, 0.8, 0.6, 0.4, 0.4, 0.6, 0.8, 1, 0.8, 0.6,
         0.2, 0.4, 0.6, 0.8, 1, 0.8, 0, 0.2, 0.4, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)


  applicant.kappa <- cohen.kappa(kappa.matrix.applicant, w=linear.weights)
}

print("Cohen's Kappa for the ApplicantTrack")
(applicant_kappa <- calculate_applicant_kappa(final.apps=final.apps, final.external=final.external, final.internal=final.internal))


```

### Agreement over the Project grade
```{r}

calculate_proposal_kappa <- function(final.apps = final.apps, final.external = final.external, final.internal = final.internal) {
  # First, combine data of external & internal applicant rating
  kappa.internal.proposal <- prepare_data_internal_log_regression(final.apps,final.internal)
  kappa.external.proposal <- prepare_data_external_log_regression(final.apps, final.external)
  kappa.external.proposal$ProposalCombined <- round((kappa.external.proposal$ScientificRelevance+
                                                            kappa.external.proposal$Suitability)/2, 0)
  proposal.rating <- merge(x=kappa.external.proposal[,c("ProjectID", "ProposalCombined")], 
                            y = kappa.internal.proposal[,c("ProjectID","ProjectAssessment")],
                            by="ProjectID")
  
  colnames(proposal.rating) <- c("ProjectID", "ProposalExt", "ProposalInt")
  
  # create matrix to show how external and internal raters rate applicant
  kappa.matrix.proposal <- matrix(0, nrow=6, ncol=6)
  colnames(kappa.matrix.proposal) <- 1:6
  rownames(kappa.matrix.proposal) <- 1:6

  # count number of times things agree/don't agree
  for (i in 1:nrow(proposal.rating)) {
    kappa.matrix.proposal[proposal.rating[i,][[2]],proposal.rating[i,][[3]]] <- 
      kappa.matrix.proposal[proposal.rating[i,][[2]],proposal.rating[i,][[3]]] + 1
  }
  print(sum(diag(kappa.matrix.proposal)) / 1643)
  # Define weight matrix
  linear.weights <- matrix(c(1, 0.8, 0.6, 0.4, 0.2, 0, 0.8, 1, 0.8, 0.6, 0.4, 0.2, 
         0.6, 0.8, 1, 0.8, 0.6, 0.4, 0.4, 0.6, 0.8, 1, 0.8, 0.6,
         0.2, 0.4, 0.6, 0.8, 1, 0.8, 0, 0.2, 0.4, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)
  
  custom.weights <- matrix(c(1, 0.8, 0.6, 0, 0, 0, 0, 1, 0.8, 0.6, 0, 0, 
         0.6, 0.8, 1, 0.8, 0.6, 0, 0, 0.6, 0.8, 1, 0.8, 0.6,
         0, 0, 0.6, 0.8, 1, 0.8, 0, 0, 0, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)


  proposal.kappa <- cohen.kappa(kappa.matrix.proposal, w=custom.weights) # try again with weights
  return(proposal.kappa)
}

print("Cohen's Kappa for the Scientific Proposal")
(proposal_kappa <- calculate_proposal_kappa(final.apps=final.apps, final.external=final.external, final.internal = final.internal))


```

## Exploratory Analysis: Distribution tables 
#### External Reviewers
```{r warnings=F, message=F}
# External Reviewers
library(expss)
ER<-apply_labels(external_regression_data,
                                   ProjectID="ProjectID",
                                   ApplicantTrack="Main Applicant Track",
                                   ApplicantTrack=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                              "excellent" = 5, "outstanding" = 6 ),  
                                   ScientificRelevance="ScientificRelevance",
                                   ScientificRelevance=c( "poor"= 1, "average"= 2, "good" = 3, 
                                                          "very good" = 4,"excellent" = 5, "outstanding" = 6 ),
                                   Suitability="Suitability",
                                   Suitability=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                                  "excellent" = 5, "outstanding" = 6 ),
                                   OverallGrade="Overall Reviewer grade",
                                   OverallGrade=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                                  "excellent" = 5, "outstanding" = 6 ),
                                   PercentFemale="Percentage of females in the reviewers",
                                   IsApproved="Result",
                                   IsApproved=c("Accepted"=1,"Rejected"=0),
                                   Age= "Age",
                                   Gender="Gender",
                                   Division="Division"
                                   )

# This only work if you knit to HTML

expss_output_rnotebook()
drop_empty_columns(calculate(ER,cro(OverallGrade, list(total(),
                                                       Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(ER,cro(ApplicantTrack,list(total(),
                                                        Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(ER,cro(ScientificRelevance, list(total(),
                                                              Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(ER,cro(Suitability, list(total(),
                                                      Gender%nest%IsApproved,Division%nest%IsApproved))))

```
#### Internal Reviewers

```{r message=F,warning=F}
# Internal Reviewers
library(expss)
IR<-apply_labels(internal_regression_data,
                                       ProjectID="ProjectID",
                                       IsApproved="Result",
                                       IsApproved=c("Accepted"=1,"Rejected"=0),
                                       Gender="Gender",
                                       Division="Division",
                                       Age= "Age",
                                       PercentFemale="Percentage of females in the reviewers",
                                       ApplicantTrack="Main Applicant Track",
                                       ApplicantTrack=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                                  "excellent" = 5, "outstanding" = 6 ),
                                       ProjectAssessment= "Project Assessment",
                                       ProjectAssessment=c( "poor"= 1, "average"= 2, "good" = 3, "very good" = 4,
                                                  "excellent" = 5, "outstanding" = 6 ),
                                       Ranking="Overall Comparative Ranking",
                                       Ranking=c( "D"= 1, "C"= 2, "BC" = 3, "B" = 4, "AB" = 5, "A" = 6 )  
                                       )

# This only work if you knit to HTML

expss_output_rnotebook()
drop_empty_columns(calculate(IR,cro(Ranking, list(total(),Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(IR,cro(ApplicantTrack,list(total(),Gender%nest%IsApproved,Division%nest%IsApproved))))
drop_empty_columns(calculate(IR,cro(ProjectAssessment, list(total(),Gender%nest%IsApproved,Division%nest%IsApproved))))


```


## Exploratory Analysis: Graphic exploration

### Applications
#### Is there evidence of gender bias?


Apparently there is no association between gender and approval (generally + in each division)
```{r echo=F}
library(vcd)
par(mfrow=c(1,2))
cotabplot(~ Gender+IsApproved, data=final.apps, shade=T)
cotabplot(~ IsApproved + Gender | Division, data = final.apps, shade = TRUE)
```

There are more women then men applying for Division 1 and the opposite for Division 2.
Also, if the project is a continuation it is more likely to be approved. It seems that there is no time effect: proposals are equally likely to be approved in April and October. Apparently proposals from ETH are more likely to be approved, especially than those from UAS/UTE. 

```{r echo=F}
par(mfrow=c(1,2))
cotabplot(~ Gender + Division, data=final.apps, shade=T)
cotabplot(~ IsContinuation + IsApproved, data=final.apps, shade=T)
cotabplot(~ IsApproved + Semester, data=final.apps, shade=TRUE)
cotabplot(~ IsApproved + InstType, data=final.apps, shade=TRUE)
```

#### How does the Amount requested relates to Amount granted?
On average in Division1 applicants receive almost all the amount of money requested,
 even if there are lots of outliers. Division 2 is the one with the smallest percentage of
 money granted.
```{r echo=F}
id.approved <- which(final.apps$IsApproved==1)
perc.f <- final.apps$AmountGranted[id.approved]/final.apps$AmountRequested[id.approved]*100
boxplot(perc.f~final.apps$Division[id.approved], main="Percentage of amount granted of the amount requested")
```

Probably the previous result is due to the fact that in Division1, the AmountRequested, on average, is smaller than in all the other divisions.
```{r echo=F}
boxplot(final.apps$AmountRequested~final.apps$Division, main="Amount requested per Division")
```

### External Reviewers
#### Is there evidence of gender bias?

#### Do males or females rate females differently in the OverallGrade?
```{r}

tmp.external.data <- merge(final.apps[,c("Gender", "ProjectID")], external_reviews, by="ProjectID")

# just females
tmp.external.data.f <- tmp.external.data[tmp.external.data[,"Gender"]=="f",]

# just males
tmp.external.data.m <- tmp.external.data[tmp.external.data[,"Gender"]=="m",]

r.tab<-prop.table(table(tmp.external.data$ReviewerGender, tmp.external.data$OverallGrade, tmp.external.data$Gender),1)

    #mycol<-colorRampPalette(c("red", "green"))(6)
    # barplot(r.tab, beside = TRUE, col = c("pink","lightblue"), 
    #         legend.text = c("Female", "Male"),
    #         main="Difference on how Female and Male grade")
r.tab.dataframe <- as.data.frame(r.tab)    
colnames(r.tab.dataframe) <- c("OverallGrade", "ReviewerGender", "Gender", "Freq")

ggplot(r.tab.dataframe,aes(x=OverallGrade,y=Freq,fill=factor(ReviewerGender)))+
  geom_bar(stat="identity",position="dodge")+
  scale_fill_discrete(name="ReviewerGender",
                      breaks=c(1, 2),
                      labels=c("Male", "Female"))+
  xlab("OverallGrade")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
  ggtitle("Difference in how Female & Male Allocate OverallGrades")

# How men and women allocate overallgrades
ggplot(r.tab.dataframe,aes(x=OverallGrade,y=Freq,fill=factor(ReviewerGender)))+
  geom_bar(stat="identity",position="dodge")+
  scale_fill_discrete(name="OverallGrade",
                      breaks=c(1, 2),
                      labels=c("Male", "Female"))+
  xlab("OverallGrade")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
  ggtitle("Difference in how Female & Male Allocate OverallGrades")



```

```{r}
# How men and women allocate overallgrades to female applicants

r.tab.f <- prop.table(table(tmp.external.data.f$ReviewerGender, tmp.external.data.f$OverallGrade),1)
r.tab.f.dataframe <- as.data.frame(r.tab.f)    
colnames(r.tab.f.dataframe) <- c("OverallGrade", "ReviewerGender", "Freq")


ggplot(r.tab.f.dataframe,aes(x=OverallGrade,y=Freq,fill=factor(ReviewerGender)))+
  geom_bar(stat="identity",position="dodge")+
  scale_fill_discrete(name="ReviewerGender",
                      breaks=c(1, 2),
                      labels=c("Male", "Female"))+
  xlab("OverallGrade")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
  ggtitle("Difference in how Female & Male Allocate OverallGrades to Female Applicants")
```

We see male and female reviewers rate female applicants almost identically. 
 
```{r}
# How men and women allocate overallgrades to male applicants

r.tab.m <- prop.table(table(tmp.external.data.m$ReviewerGender, tmp.external.data.m$OverallGrade),1)
r.tab.m.dataframe <- as.data.frame(r.tab.m)    
colnames(r.tab.m.dataframe) <- c("OverallGrade", "ReviewerGender", "Freq")

ggplot(r.tab.m.dataframe,aes(x=OverallGrade,y=Freq,fill=factor(ReviewerGender)))+
  geom_bar(stat="identity",position="dodge")+
  scale_fill_discrete(name="OverallGrade",
                      breaks=c(1, 2),
                      labels=c("Male", "Female"))+
  xlab("OverallGrade")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
  ggtitle("Difference in how Female & Male Allocate OverallGrades to Male Applicants")
```

We observe that female reviewers give fewer "Excellent" and "Outstanding" to male applicants, and give more "Very Goods."




Need to fix everywhere since ordinal variables cannot be used in chi squared test.

```{r}
par(mfrow=c(2,2))

test <- independence_test(Gender ~ OverallGrade, data=external_regression_data)
test2 <- lbl_test(Gender ~ OverallGrade, data=external_regression_data)

cotabplot(~ Gender + OverallGrade, data=external_regression_data, shade=T)
cotabplot(~ Gender + ApplicantTrack, data=external_regression_data, shade=T)
cotabplot(~ Gender + ScientificRelevance, data=external_regression_data, shade=T)
cotabplot(~ Gender + Suitability, data=external_regression_data, shade=T)
```


Even when combining Variables for Project Assessment, Applicant Track and the Overall Grade using a simplified version with 0 and 1, it dosen't seems that there is a gender bias in the external evaluation.

```{r echo=F}

#### commented out becuase of error HERE THAT PREVENTED KNITTING - WILL LOOK INTO THIS IN THE MORNING TO FIX
# par(mfrow=c(2,2))
#    external_regression_data$ProposalCombined <- round((external_regression_data$ScientificRelevance+
#                                                        external_regression_data$Suitability)/2, 0)
#    external_regression_data$SimplifiedOverallGrade <- ifelse(external_regression_data$OverallGrade < 4, 0, 1)
#    external_regression_data$SimplifiedProposal <- ifelse(external_regression_data$ProposalCombined < 4, 0, 1)
#    external_regression_data$SimplifiedApplicant <- ifelse(external_regression_data$ApplicantTrack < 4, 0, 1)


# cotabplot(~ Gender + SimplifiedOverallGrade, data=external_regression_data, shade=T)
# cotabplot(~ Gender + SimplifiedApplicant, data=external_regression_data, shade=T)
# cotabplot(~ Gender + SimplifiedProposal, data=external_regression_data, shade=T)
```

### Internal Reviewers
#### Is there evidence of gender bias?

We repeated the same analysis for the internal evaluation: it seems that it is more likely to get a low grade (from 1 to 3) for the both Track Record and Project Assessment if the applicant is female! There same holds for the Ranking.

```{r, echo=FALSE}
par(mfrow=c(2,2))

internal_regression_data$Ranking <- ifelse(internal_regression_data$Ranking < 4, 0, 1)
internal_regression_data$ProjectAssessment <- ifelse(internal_regression_data$ProjectAssessment < 4, 0, 1)
internal_regression_data$ApplicantTrack <- ifelse(internal_regression_data$ApplicantTrack < 4, 0, 1)

cotabplot(~ Gender + Ranking, data=internal_regression_data, shade=T)
cotabplot(~ Gender + ApplicantTrack, data=internal_regression_data, shade=T)
cotabplot(~ Gender + ProjectAssessment, data=internal_regression_data, shade=T)
```

## Analysis


## Results

## Conclusion

# Appendix

## Detailed Data Description

### Applications
  * **AmountRequested**: Rounded to the next 10k CHF
  * **AmountGranted**: Rounded to the next 10k CHF
  * **IsApproved**: 1 if the application was approved, 0 if it was rejected
  * **GradeFinal**: Comparative ranking of the application as determined by the evaluation body (the division of the National Research Council). A: "belongs to the 10% best percent"; AB: "10% are worse, 75% are better"; B: "50% are worse, 25% are better"; BC: " 25% are worse, 50% are better"; C "10% are worse, 75% are better"; D: "90% of the applications are better"
  
  * **Division**: Evaluation Body in which the application was evaluated. Division 1 evaluates Social Sciences and Humanities; Division 2 Mathematics, Natural Sciences and Engineering; Division 3 Biology and Medicine
  
  * **MainDiscipline**: as chosen by the applicant from the SNSF discipline list
  * **MainDisciplineLevel2**: category in the SNF discipline list grouping disciplines into fields of research
  * **CallTitle**: Call for proposals under which the application was submitted. Applications from the same Call are evaluated together, i.e. in competition to each other
  * **CallEndDate**: Submission deadline of the Call
  * **ResponsibleApplicantAcademicAgeAtSubmission**: Years since the applicant's PhD at time of submission; data only available since mid 2016
  * **ResponsibleApplicantAgeAtSubmission**: Biological age of the applicant at time of submission; data only available since mid 2016
  * **ResponsibleApplicantProfessorshipType**: employment situation of the applicant at time of submission; data only available since mid 2016
  * **Gender**: of the main applicant
  * **NationalityIsoCode**: Nationality of the main applicant
  * **IsHasPreviousProjectRequested**: 0 if it is the applicant's first application at the SNSF, 1 if not
  * **InstType**: Type of institution where the applicant is employed
  * **IsContinuation**: 1 if the project is a thematic continuation of a previously approved project, 0 if not
  * **ProjectID**: Anonymized identifier of the application

### Referee Grades
  * **Question**: Evaluation criterion
  * **QuestionRating**: The (co-)referee's assessment of the evaluation criterion
  * **OverallRanking**: The (co-)referee's overall comparative ranking of the application. A: "belongs to the 10% best percent"; same scale as the GradeFinal
  * **RefereeRole**: Some applications have one referee evaluation, some have two. The role indicates who was the primary and who was the secondary referee (also called co-referee) 
  * **RefereeGender**
  * **IDs**: Anonymized identifiers of the application, the referee and the evaluation by the referee

### Reviews
  * **Question**: Evaluation criterion
  * **QuestionRating**: The external reviewer's assessment of the evaluation criterion
  * **OverallGrade**: The external reviewer's overall assessment of the application
  * **SourcePerson**: Who suggested the reviewer?
  * **Gender**
  * **Country**: where the reviewer is located. Not always known
  * **EmailEnding**: ending of the reviewer's email address. Might be used as an approximation of the country where the reviewer is located in cases where this data is missing
  * **IDs**: Anonymized identifiers of the application, the reviewer and the review

