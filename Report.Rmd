---
title: "SNSF Report"
author: "Chiara Gilardi, Leslie O'Bray, Carla Schaerer and Tommaso Portaluri"
date: "13 June 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
graphics: yes
---

<style>
  body {
    text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, prompt = FALSE,comment = "   ")
```

```{r Load Functions and Data }
# setwd("~/StatLab")
#setwd("~/MSc Statistics/StatsLab/Analysis")
# load("snsf_data.RData") # Chiara's directory, comment out
#load("SNFS Data/snsf_data.RData") # carla's directory, comment out
load("/home/leslie/Desktop/StatsLab/snsf_data.RData") # leslie's directory, comment out when other is using
source("Cleaning Functions.R")
source("Data for Regression.R")

# install.packages("biostatUZH", repos="http://R-Forge.R-project.org")

library(biostatUZH)
library(psy)
library(psych)
library(ggplot2)
library(gridExtra)
library(coin)
library(ggmosaic)
library(effects)
library(xtable)
library(kableExtra)
library(ordinal)
library(MASS)

```

\newpage 
\tableofcontents
\newpage

# Introduction

The Swiss National Science Foundation (SNSF) is a research funding agency which disseminates yearly, on behalf of the Swiss Government, billions of CHF to the best researchers in Switzerland. This report contains a statistical analysis performed on three data sets provided by SNSF, containing information on the applications for funding received in 2016, the corresponding grades given by both internal and external evaluators, and whether the application was funded or not.

The analysis performed for SNSF had a two-fold aim, corresponding to the following two research questions:

(1) Is gender bias occurring at any stage of the SNSF evaluation process? Is the gender of the main applicant influencing the rating of the application?

(2) To what extent the different steps of the evaluation and the different criteria within each step determine the final funding decision?

The SNSF evaluation procedure is a multi-step process (involving external reviewers, internal referees, and an internal board) which takes into consideration both the track record of the applicant and the quality of the project (see Appendix for a more detailed description of the evaluation procedure).

Several studies (Witteman et al., 2017; Solans-Domenech et al., 2017) have shown that female applicants' projects get higher score when the application is blinded. Moreover, female applicants receive usually higher grades for projects and lower grades for track record. Hence, after investigating the gender dimension to identify possible biases in the evaluation procedure, the focus of the analysis will be the relative importance of of the criteria for funding (applicant's track record vs. quality of the proposal) and, also, of each step of the evaluation procedure (which opinion is more likely to determine the final decision - the external reviewer or the internal reviewer?). Possible interactions between the gender dimension and the second research question will also be investigated (for instance, by taking into account also the gender of evaluator or the percentage of female referees).

# Data Description

We have three data sets: Applications, External Reviewers and Internal Referees. They contain respectively information about the SNSF project funding applications, the evaluation of the applications by external peer reviewers and the evaluation of the proposals by external the internal referee and co-referee (when available). For a full description of the data & variables, please see the Appendix. 


# Cleaning the Data

We made the following decisions while cleaning the data:

* We only worked with complete applications, i.e. projects for which we have information from all the three data sets.
* To avoid a temporal trend, we are only considering application from 2016. 
* In both the external and internal step, we encountered applications which had several reviews per application. For the sake of our analysis, in these scenarios we computed the mean grade for each criteria, so that each application had a "single" score for each criteria assessed on. In all instances of combining scores, we did not round until the very end, and rounded up if the value was $\ge$ 0.5, otherwise we rounded down. 
* Since an application had different numbers of reviewers, we introduced a new variable, PercentFemale, which calculated the percent of female reviewers out of all reviewers of a single application (ranging from 0 to 1).
* All applications with a grade were converted to an ordinal factor, with levels from 1 (lowest) to 6 (highest).

We also made some additional decisions given the specific datasets:

### Applications

* We only consider the MainDiscipline2 (a factor with 21 levels), rather than MainDiscipline (a factor 118 levels) to improve interpretability. 
* There is one application for which we do not know the gender of the applicant, and therefore we decided to omit that observation from the analysis.


```{r Table Professorship and AcademicAge}
# For Professorship

tbl<-table(applications$ResponsibleApplicantProfessorshipType, useNA = "always")
NA.prof<-sum(is.na(applications$ResponsibleApplicantProfessorshipType))/dim(applications)[1]

Prof<-data.frame(tbl)
colnames(Prof)<-c("Type","Frequency")
# For AcademicAge
NA.AC<-sum(is.na(applications$ResponsibleApplicantAcademicAgeAtSubmission))/dim(applications)[1]
```
* We will also not consider the variable "CallTitle" because we do not think it has value to the model. We also will not consider "Professorship" or "AcademicAge" due to the fact that there are a considerable number of NA's on those variables (around `r round(NA.prof*100)`% of the observations).

```{r}
kable(Prof,"latex", booktabs = T) %>%
  kable_styling(position = "center")
```


### External Reviewers

* We did not consider reviewer observations that did not give a grade or gave a grade of "0". Reviewers always have the option to choose not to consider or to give the grade "0" when reviewing an application. Some might be mistakes, in others cases there might be a conflict of interest, or they might be very ambivalent about the project. 
* One of the questions evaluated in the applications is "Broader impact (forms part of the assessment of scientific relevance, originality and topicality)". For the time frame we are considering, in all the applications this grade was NA. Hence, we omit this variable from our model.
* ProposalCombined: We created a new variable to summarize the assessment of the scientific proposal in the external review step. This is a simple mean of the grade given for Suitability and Scientific Relevance. This helped to isolate the effect of the grade given to the scientific proposal, versus the applicant track record, as well as to ensure easier comparison with the internal review step.
* PercentFemale: As previously mentioned, we introduced a new variable calculating the percent of reviewers of each application that is female. 

### Internal Referees

* There were 22 observations (1 for the time frame we are dealing with) for which only demographic information was available, no grades were given. We decided to omit those observations.
* We decided not to consider the Referee role as a variable in our model, as the majority of the evaluations has only one referee.

```{r Table RefereeRole}
Ref<-data.frame(table(referee_grades$RefereeRole, useNA = "always"))
colnames(Ref)<-c("Type","Frequency")
kable(Ref,"latex", booktabs = T) %>%
  kable_styling(position = "center")

```
* PercentFemale: As previously mentioned, we introduced a new variable calculating the percent of reviewers of each application that is female. 

# Exploratory Analysis

```{r data to use for analysis, echo=F, message=F}


#### External datasets to use
external_regression_data <- prepare_data_external_log_regression(apps=final.apps, external=final.external)
ex.f <- external_regression_data[external_regression_data$Gender=="f",]
ex.m <- external_regression_data[external_regression_data$Gender=="m",]
ex.div1 <- external_regression_data[external_regression_data$Division=="Div 1",] # particularly bad for Div 1
ex.div2 <- external_regression_data[external_regression_data$Division=="Div 2",]
ex.div3 <- external_regression_data[external_regression_data$Division=="Div 3",]


#### internal datasets to use 
internal_regression_data<-prepare_data_internal_log_regression(apps=final.apps, internal=final.internal)
in.f <- internal_regression_data[internal_regression_data$Gender=="f",]
in.m <- internal_regression_data[internal_regression_data$Gender=="m",]
in.div1 <- internal_regression_data[internal_regression_data$Division=="Div 1",] # particularly bad for Div 1
in.div2 <- internal_regression_data[internal_regression_data$Division=="Div 2",]
in.div3 <- internal_regression_data[internal_regression_data$Division=="Div 3",]



```

In our exploratory analysis, we discovered a few interesting insights that relate to the findings we will discuss from our formal analysis. 

### Comparison of the Distribution of Grades between the External & Internal Review Step

Since the external & internal step both assess candidates on the same criteria (the strength of the scientific proposal, and the strength of the applicant), on the same ordinal scale (from poor to outstanding), we were interested to see if the distribution of grades are the same. We would expect different distributions for the Overall Grade vs the Ranking, since those are measures are on different scales, but we may expect to see a similar distribution of grades for criteria that use the same measurement system. After combining the Suitability & Scientific Relevance grades given to a candidate in the external review step into a single grade, we can compare the average grade given for the Scientific Proposal in the two steps, as well as the grade given for the Applicant Track Record across the two steps. 

We see that the External Reviewers are more generous with their grades; for the strength of the Scientific Proposal, 48% of proposals are considered "excellent" or "outstanding", versus only 28% in the internal review step. 

```{r scientific grade distribution external vs internal, out.width="50%", fig.ncol=2}

ProposalCombined <- external_regression_data$ProposalCombined
ex_proposal <- as.data.frame(prop.table(table(ProposalCombined)))
ex_proposal$Freq <- as.numeric(ex_proposal$Freq)
colnames(ex_proposal)[1] <- "ProposalScore"

p8 <- ggplot(ex_proposal, aes(x=ProposalScore, y= Freq)) +
  geom_histogram(stat="identity") +
  theme(plot.title = element_text(size = rel(1.5), hjust = 0.5, 
                                  margin = margin(t = 10, b = 20, unit = "pt"))) +
  ggtitle("External Scientific Proposal Grade Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5)) + 
  scale_y_continuous(limits = c(0, 0.45))

in_proposal <- as.data.frame(prop.table(table(internal_regression_data$ProjectAssessment)))
in_proposal$Freq <- as.numeric(in_proposal$Freq)
colnames(in_proposal)[1] <- "ProposalScore"
p9  <- ggplot(in_proposal, aes(x=ProposalScore, y= Freq)) +
  geom_histogram(stat="identity") +
  theme(plot.title = element_text(size = rel(1.5), hjust = 0.5, 
                                  margin = margin(t = 10, b = 20, unit = "pt"))) +
  ggtitle("Internal Scientific Proposal Grade Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5)) +
  scale_y_continuous(limits = c(0, 0.41))

p8
p9
# grid.arrange(p8, p9, ncol=2)
```

Similarly we see the same pattern with Applicant Track Record: 66% of Applicant Track records are considered "excellent" or "outstanding" by the External Reviewers, versus merely 50% by the Internal Reviewers. 

```{r, out.width="50%", fig.ncol=2}

ApplicantTrack <- external_regression_data$ApplicantTrack
ex_proposal <- as.data.frame(prop.table(table(ApplicantTrack)))
ex_proposal$Freq <- as.numeric(ex_proposal$Freq)
colnames(ex_proposal)[1] <- "ApplicantScore"
p8b <- ggplot(ex_proposal, aes(x=ApplicantScore, y= Freq)) +
  geom_histogram(stat="identity") +
  theme(plot.title = element_text(size = rel(1.5), hjust = 0.5, 
                                  margin = margin(t = 10, b = 20, unit = "pt"))) +
  ggtitle("External Applicant Track Grade Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5)) +
  scale_y_continuous(limits = c(0, 0.45))

in_proposal <- as.data.frame(prop.table(table(internal_regression_data$ApplicantTrack)))
in_proposal$Freq <- as.numeric(in_proposal$Freq)
colnames(in_proposal)[1] <- "ApplicantScore"
p9b  <- ggplot(in_proposal, aes(x=ApplicantScore, y= Freq)) +
  geom_histogram(stat="identity") +
  theme(plot.title = element_text(size = rel(1.5), hjust = 0.5, 
                                  margin = margin(t = 10, b = 20, unit = "pt"))) +
  ggtitle("Internal Applicant Track Grade Distribution") +
  geom_text(aes(label=round(Freq, 2),vjust=1.5)) +
  scale_y_continuous(limits = c(0, 0.43))

p8b
p9b
# grid.arrange(p8, p9, ncol=2)
```

After noticing this discrepancy, we wanted to quantify how different these  the grades were to one another. To assess the agreement between the two steps, for the same criteria, we used Cohen's Kappa. Cohen's Kappa measures the proportion of agreement between two raters assessing something on an ordinal scale, accounting for the fact that there will always be some proportion by random chance. An important specification of Cohen's Kappa is the weight given to the measurements. If the external & internal reviewers both assessed the Applicant Track Record as "excellent", that would be considered full agreement. However, we want to allocate partial credit if the rating is a level close to it. We used a linear weight up to distance 2, and after that gave no credit. (In this example, if one rater gave an "outstanding" or "very good", that would be considered a distance of one and be weighted by 0.8. If the second rater assessed the Applicant Track to be "good", which is a distance of two away from excellent, that would be weighted as 0.6). Anything with a distance of 3 or more (in this example, if the second rater gave a rating of "average"), we allocated no weight, as the difference between average and excellent is quite large.

Cohen's Kappa takes a value from 0 to 1, with 0 signifying there is no more agreement than what one would expect from random chance, and 1 indicating perfect agreement.  From this, we found that there was just "moderate" agreement (moderate indicates a kappa between 0.4-0.6) between the internal and external steps when using the weighted kappa, for the grades given for the Applicant Track Record:

```{r cohens kappa}
board_data <- prepare_data_board_log_regression(apps=final.apps, internal = final.internal, external = final.external)

custom.weights <- matrix(c(1, 0.8, 0.6, 0, 0, 0, 0, 1, 0.8, 0.6, 0, 0, 
         0.6, 0.8, 1, 0.8, 0.6, 0, 0, 0.6, 0.8, 1, 0.8, 0.6,
         0, 0, 0.6, 0.8, 1, 0.8, 0, 0, 0, 0.6, 0.8, 1), nrow=6, ncol=6, byrow=T)


kappa.applicant <- table(board_data$ExternalApplicantTrack, board_data$InternalApplicantTrack)

kappa.app <- cohen.kappa(kappa.applicant, w=custom.weights)
kappa.app


```

We observe the same moderate agreement when we look at the comparison of grades given to the scientific propsoal (ProposalCombined and ProjectAssessment):

```{r}

kappa.proposal <- table(board_data$ProposalCombined, board_data$ProjectAssessment)
kappa.pro <- cohen.kappa(kappa.proposal, w=custom.weights)
kappa.pro
```


### Impact of Internal Reviewers on Funding

We wanted to understand if this discrepancy between how grades had an impact on whether an application is funded. To do this, we visualized the summary grade given to an application, and whether that application is funded or not. As we can see here, there are several applications in the external step that recieve an OverallGrade of "excellent" or "outstanding" that end up not approved (color coral). This highlights that not only do the Internal Reviewers give tougher grades in general than the external step, but they also consider some "excellent" and "outstanding" applications by the external reviewers to be not of the quality that deserves funding. This trend is true in all divisions and both genders, please refer to the appendix to see the specific graphic. On the other hand, applications with top grades in the Ranking almost always are approved (color blue). 

Our conclusion for this is that the internal step is very consequential, and the difference in the rating they give translates into differences in whether an application gets funded or not. 

```{r, out.width="50%", fig.ncol=2}

plot_mirror_barplot <- function(dataset, variable1, variable2="IsApproved", plot_title="Plot Title", title_size=8) {
  table_data_frame <- as.data.frame(table(dataset[,variable1], dataset[,"IsApproved"]))
  colnames(table_data_frame) <- c(variable1, variable2, "Frequency")
  levels(table_data_frame[,variable1]) <- list("Outstanding"="6", "Excellent"="5", 
                                               "Very Good"="4", "Good"="3", "Average"="2", "Poor"="1")
  table_data_frame$Frequency[table_data_frame[,"IsApproved"]==0] <- -table_data_frame$Frequency[table_data_frame[,"IsApproved"]==0]
  
  ggplot(table_data_frame, aes_string(x=variable1, y="Frequency", fill=variable2)) + 
    geom_bar(stat="identity", position="identity") + 
    scale_y_continuous(breaks=seq(-100,100,by=50),labels=abs(seq(-100,100,by=50))) +
    coord_flip() +
    theme(plot.title = element_text(size = rel(1.5), hjust = 0.5, 
                                  margin = margin(t = 10, b = 20, unit = "pt"))) +
    ggtitle(plot_title) + 
    theme(plot.title = element_text(size = title_size)) 
    # scale_fill_manual(values = c("firebrick","darkseagreen4"))
    
}

plot_mirror_barplot2 <- function(dataset, variable1, variable2="IsApproved", plot_title="Plot Title", title_size=8) {
  table_data_frame <- as.data.frame(table(dataset[,variable1], dataset[,"IsApproved"]))
    
  colnames(table_data_frame) <- c(variable1, variable2, "Frequency")
  levels(table_data_frame[,variable1]) <- list("A"="6", "AB"="5", 
                                               "B"="4", "BC"="3", "C"="2", "D"="1")
                                               
  table_data_frame$Frequency[table_data_frame[,"IsApproved"]==0] <- -table_data_frame$Frequency[table_data_frame[,"IsApproved"]==0]
  
  ggplot(table_data_frame, aes_string(x=variable1, y="Frequency", fill=variable2)) + 
    geom_bar(stat="identity", position="identity") + 
    scale_y_discrete(breaks=c("A", "AB", "B","BC", "C", "D")) +
    scale_y_continuous(breaks=seq(-100,100,by=50),labels=abs(seq(-100,100,by=50))) +
    coord_flip() +
    theme(plot.title = element_text(size = rel(1.5), hjust = 0.5, 
                                  margin = margin(t = 10, b = 20, unit = "pt"))) +
    ggtitle(plot_title) + 
    theme(plot.title = element_text(size = title_size)) 
    # scale_fill_manual(values = c("firebrick","darkseagreen4"))
    
}

p4 <- plot_mirror_barplot(dataset=external_regression_data, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approve")

p5 <- plot_mirror_barplot2(dataset=internal_regression_data, 
                          variable1 = "Ranking", plot_title = "Ranking vs. Approved")

p4
p5
# grid.arrange(p4, p5, ncol=2)



```


### Distribution of Grades by the Gender of the Reviewer

The third interesting insight we found was when we investigated the impact of the gender of the person reviewing the data. We look at the relative frequencies of grades given by male and female reviewers, to applicants, regardless of gender. Within the external step in particular, we found that female reviewers give proportionally fewer "excellent" and "outstanding" grades, compared to their male counterparts. Within the internal step, we did not notice a particular difference, though we will consider the impact of the gender of the reviewer more rigorously in our analysis. 

```{r, out.width="50%", fig.ncol=2}

####################### EXTERNAL

tmp.external.data <- merge(final.apps[,c("Gender", "ProjectID")], external_reviews, by="ProjectID")

# just females
tmp.external.data.f <- tmp.external.data[tmp.external.data[,"Gender"]=="f",]

# just males
tmp.external.data.m <- tmp.external.data[tmp.external.data[,"Gender"]=="m",]

r.tab.2<-prop.table(table(tmp.external.data$OverallGrade, tmp.external.data$ReviewerGender),2)
r.tab.dataframe.2 <- as.data.frame(r.tab.2)    
colnames(r.tab.dataframe.2) <- c("OverallGrade", "ReviewerGender", "Freq")

# How men and women allocate overallgrades

p10 <- ggplot(r.tab.dataframe.2,aes(x=OverallGrade,y=Freq,fill=factor(ReviewerGender)))+
  geom_bar(stat="identity",position="dodge")+
  xlab("OverallGrade")+ylab("Proportion of grades, by Reviewer Gender") +
  theme(plot.title = element_text(size = rel(1.5), hjust = 0.5, 
                                  margin = margin(t = 10, b = 20, unit = "pt"))) +
  ggtitle("External Reviewers Distribution of OverallGrades") +
  scale_fill_manual(values=c("orchid4", "darkorange1")) + 
  scale_y_continuous(limits = c(0, 0.40))

####################### INTERNAL

tmp.internal.data <- merge(final.apps[,c("Gender", "ProjectID")], final.internal, by="ProjectID")

# just females
tmp.internal.data.f <- tmp.internal.data[tmp.internal.data[,"Gender.x"]=="f",]

# just males
tmp.internal.data.m <- tmp.internal.data[tmp.internal.data[,"Gender.x"]=="m",]

r.tab.2<-prop.table(table(tmp.internal.data$Ranking, tmp.internal.data$RefereeGender),2)
r.tab.dataframe.2 <- as.data.frame(r.tab.2)    
colnames(r.tab.dataframe.2) <- c("Ranking", "RefereeGender", "Freq")


p11 <- ggplot(r.tab.dataframe.2,aes(x=Ranking,y=Freq,fill=factor(RefereeGender)))+
  geom_bar(stat="identity",position="dodge")+
  xlab("Ranking")+ylab("Proportion of grades, by Reviewer Gender") +
  theme(plot.title = element_text(size = rel(1.5), hjust = 0.5, 
                                  margin = margin(t = 10, b = 20, unit = "pt"))) +
  ggtitle("Internal Reviewers Distribution of Rankings") +
  scale_fill_manual(values=c("orchid4", "darkorange1")) + 
  scale_y_continuous(limits = c(0, 0.4))

p10
p11

```

# Gender Bias

To see if gender has an influence in any of the steps of the evaluation process, we did several things. For the external and internal steps, we took took the following two approaches:

(@) **Logistic Regression:** We first fit a logistic regression with the function glm in R, where we used IsApproved (a binary variable) as a response and demographic information of the applicant, project information and the given grades as predictors. The aim of this regression is to see if gender has an influence on the final decision from the perspective of each step.

(@) **Ordinal Regression:** Next, as the final decision is determined by the different grades in the process, in order to see if gender has an influence on any of them, we fitted an Ordinal regression with the function clm of the package "ordinal" on each grade with demographic data and project information as predictors.

In the following sections we will review both analyses for the external, and then internal, steps of the process. 

## External Step

### **Logistic Regression**

The goal of this analysis is to see if gender has an influence on the final funding decision, based on the information provided in the external step. 

Regression data:  To perform the analysis, we combined in one data frame information about the applications (IsApproved, Age, Gender, Division, IsContinuation, PreviousRequest, InstType, log(AmountRequested), Semester) and about the grades given by the external reviewers (ApplicantTrack, ScientificRelevance, Suitability, OverallGrade, ProposalCombined, PercentFemale). We only had two continuous variables, and at this stage did not standardize them.
    
As there are almost no application approved that have grades smaller than "good", we decide to aggregate grades "poor", "average" and "good" to avoid perfect separation problems. All grades are considered as factors.
                         
```{r}
# Obtain Regression Data
external_regression_data<-prepare_data_external_log_regression(final.apps,final.external)

# combine grades 1,2,3 and log AmountRequested
external_regression_data$logAmount<-log(external_regression_data$AmountRequested)
data<- subset(external_regression_data,select = -c(ProjectID, OverallGrade, AmountRequested,ProposalCombined))
data$ApplicantTrack<-ifelse(data$ApplicantTrack<=3,3,data$ApplicantTrack)
data$ApplicantTrack<-factor(data$ApplicantTrack)
data$ScientificRelevance<-ifelse(data$ScientificRelevance<=3,3,data$ScientificRelevance)
data$ScientificRelevance<-factor(data$ScientificRelevance)
data$Suitability<-ifelse(data$Suitability<=3,3,data$Suitability)
data$Suitability<-factor(data$Suitability)

# Fit first model with all variables and interactions
  Model.log.ext <- glm(IsApproved ~ .+Gender:Division+
               Gender:PercentFemale+Gender:ApplicantTrack+InstType:Division ,data=data, 
             family="binomial")
  PsR2<-(1-exp((Model.log.ext$dev-Model.log.ext$null)/1623))/(1-exp(-Model.log.ext$null/1623))
  
  
```

We first fitted a logistic regression model with all the variables and the interactions between Gender and Division, PercentFemale and ApplicantTrack with the glm function in package *stats*. Also we considered the interaction between InstType and Division. we didn't considered OverallGrade as it is highly correlated with the grades of the applicant and the project. we achieved a pseudo-R^2  value of  `r round(PsR2,4)`, indicating that this percent of the variation in the approval of the applications can be explained by the model. 
^[$R^2=\frac{1-(exp{((D_{res}- D_{null})/n)}}{1-exp{(-D_{null}/n)}}$]



```{r}

log.ext.Model <- glm(IsApproved ~ ApplicantTrack + ScientificRelevance + Suitability + 
              PercentFemale + Age + Gender + Division + IsContinuation + InstType + 
              Semester + Division:InstType, data=data, family="binomial")

PsR2.small<-(1-exp((log.ext.Model$dev-log.ext.Model$null)/1623))/(1-exp(-log.ext.Model$null/1623))
```


When selecting the variables with the AIC criteria in order to work with a small and effective model, we end up with the following predictors: ApplicantTrack, ScientificRelevance, Suitability, PercentFemale, Age, Gender, Division, IsContinuation, InstType, Semester, Division:InstType. No interaction with Gender where significant. The pseudo R^2 for this model is `r round(PsR2.small,4)` , i.e. this smaller model explains basically the same variance of the data than the former one. None of them reveal that the information from the external reviewers explain the final decision correctly. This fact will be explored in more detail later on. ^[See Appendix pg. for the summary result of this regression.]


```{r get data for logistic external}
external_regression_data<-prepare_data_external_log_regression(final.apps,final.external)

external_regression_data$logAmount<-log(external_regression_data$AmountRequested)
data<- subset(external_regression_data,select = -c(ProjectID, OverallGrade, AmountRequested, 
                                                   ProposalCombined))

## COMBINE GRADES otherwise it doesn't work
data$ApplicantTrack<-ifelse(data$ApplicantTrack<=3,3,data$ApplicantTrack)
data$ApplicantTrack<-factor(data$ApplicantTrack)
data$ScientificRelevance<-ifelse(data$ScientificRelevance<=3,3,data$ScientificRelevance)
data$ScientificRelevance<-factor(data$ScientificRelevance)
data$Suitability<-ifelse(data$Suitability<=3,3,data$Suitability)
data$Suitability<-factor(data$Suitability)
```

```{r logistic external regression}

# Fit first model with all variables and interactions
  Model.log.ext <- glm(IsApproved ~ .+Gender:Division+
                Gender:PercentFemale+Gender:ApplicantTrack+InstType:Division ,data=data, 
                family="binomial")

  #summary(Model.log.ext)
  coef.log <- Model.log.ext$coefficients
  CI <- confint(Model.log.ext, type= "Wald")

Model.log.ext <- glm(IsApproved ~ ApplicantTrack + ScientificRelevance + Suitability + 
                                  PercentFemale + Age + Gender + Division + IsContinuation + InstType +
                                  Semester + Division:InstType, data=data, family="binomial")

PsR2.small<-(1-exp((Model.log.ext$dev-Model.log.ext$null)/1623))/(1-exp(-Model.log.ext$null/1623))
```


When selecting the variables with the AIC criteria in order to work with a small and effective model, we end up with the following predictors: ApplicantTrack, ScientificRelevance, Suitability, PercentFemale, Age, Gender, Division, IsContinuation, InstType, Semester, Division:InstType. No interaction with Gender where significant. The pseudo R^2 for this model is `r round(PsR2.small,4)` , i.e. this smaller model explains basically the same variance of the data than the former one. Non of them reveal that the information from the external reviewers explain the final decision correctly. This fact will be explored in more detail later on. 

In order to assess if gender is a relevant predictor for this regression, we computed the 95% confidence intervals for the coefficients of our model. We can see them in the following plot: those that are red include 0 and therefore they are not significant. Those in blue don't include zero and so they are significant at a 5% level.

```{r new CI plot logistic external}
colors <- rep('not significant', nrow(CI))
for (i in 1:nrow(CI)){
  if (CI[i,1] > 0 & 0 < CI[i,2]) {colors[i]='significant'}
  if (CI[i,1] < 0 & 0 > CI[i,2]) {colors[i]='significant'}
}
lower <- CI[,1]
upper <- CI[,2]
df <- data.frame(l = lower, u=upper, col=colors)
#str(df)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-5,8)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)), x=coef.log, colour=col))
```

The confidence intervals which do not include zero are those for the variables: IsContinuation, Suitability (all levels from 4 to 6), ScientificRelevance (all levels from 4 to 6), and ApplicantTrack(for levels 5 and 6). 

```{r coefficients logistic external, fig.cap="Exponentiated coefficients and confidence intervals"}


id<-  which(CI[,1]>0 & CI[,2]>0)
sign.coef <- coef.log[id]
sign.OR <- exp(sign.coef)

#sign.OR

kable(round(cbind(OR=sign.OR,exp(confint(Model.log.ext)[id,])), 2),"latex", booktabs = T)%>%
  kable_styling(position = "center")
```

The interpretation of the exponentiated coefficients is as following: the odds is the $\frac{P(Approved = 1 | Grade = x)}{P(Approved = 0 | Grade = x)}$.

* The odds with respect to being approved or not given an Applicant Track grade of 5 is `r round(sign.OR[1],2)` times larger than the odds for the reference level
* The odds with respect to being approved or not given an Applicant Track grade of 6 is `r round(sign.OR[2],2)` times larger than the odds for the reference level
* The odds with respect to being approved or not given a Scientific Relevance grade of 4  is `r round(sign.OR[3],2)` times larger than the odds for the reference level
* The odds with respect to being approved or not given a Scientific Relevance grade of 5 is `r round(sign.OR[4],2)` times larger than the odds for the reference level
* The odds with respect to being approved or not given a Scientific Relevance grade of 6 is `r round(sign.OR[5],2)` times larger than the odds for the reference level
* The odds with respect to being approved or not given a Suitability grade of 4 is `r round(sign.OR[6],2)` times larger than the odds for the reference level
* The odds with respect to being approved or not given a Suitability grade of 5 is `r round(sign.OR[7],2)` times larger than the odds for the reference level
* The odds with respect to being approved or not given a Suitability grade of 6 is `r round(sign.OR[8],2)` times larger than the odds for the reference level
* The odds with respect to being approved or not given that the project is a continuation is `r round(sign.OR[9],2)` times larger than the odds for the reference level




```{r gender CI logistic internal, echo=F, eval=F}
CIgender<-CI[which(rownames(CI)=="Genderf"),]
betaGender<-exp(coef.log[which(names(coef.log)=="Genderf")])
```

Notice that the coefficient for gender is not significant, since its confidence interval (-2.074,2.059) contains zero. The point estimate for gender is 0.094 and this means that the odds with respect to being approved or not given that the applicant is a women is 2.99 times larger than the odds for a male applicant.


```{r effect plots logistic external, fig.align="center", fig.height=3}
# Obtain all the effects        
  eff.fit <- allEffects(Model.log.ext)
  
# Obtain Gender effect as a separate data frame 
  eff<-Effect("Gender", Model.log.ext)
  eff<-eff$fit
  prob<- round((exp(eff)/(1+exp(eff))),3)
  
  plot(Effect("Gender",Model.log.ext),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
```

As we are interested in the effect of gender in each step of the evaluation process, we looked at the difference of the predicted probability of being accepted between male and female given the final model. Female have a probability `r prob[2,1]`, while male have `r prob[1,1]`. Although there is a small difference, our analysis suggest that this difference is not relevant.

\pagebreak

### **Ordinal Regression**

To look into the variables that influence the different grades in this part of the evaluation process, and see if the gender of the main applicant has an influence on it, we ran an ordinal regression with the function clm of the package ordinal in R, which fits cumulative link models (CLMs), for further details of this models see Appendix pg.

We did this for all grades given by the external reviewers:

1. The average of the grades given to the scientific proposal (ProposalCombined)

2. The applicant grade (ApplicantTrack)

3. The Overall Grade. 

```{r}

Model.Prop<- clm(ProposalCombined ~ Gender + Division + PercentFemale + IsContinuation + 
                    InstType + logAmount ,data=external_regression_data)

Prop.nogender<- clm(ProposalCombined ~  Division + PercentFemale + IsContinuation + InstType + logAmount ,data=external_regression_data)
                            
p.value<-anova(Model.Prop,Prop.nogender)[2,6]
```

(1) **Scientific Proposal**: After fitting a full model using the clm function with ProposalCombined as a response variable and different interactions, and then selecting from this model the significant variables with the AIC criteria and the help of the drop1() function in R. From the output of the drop1() fuction we manually looked into that variable that reduce the most the AIC if removed. We ended up with a model with the following predictors: Gender, Division, PercentFemale, IsContinuation, InstType and log(AmountRequested). Then we fitted the same model without Gender and compare it with the anova() function and the likelihood ratio test to the one with gender, we get a p.value of `r p.value`, meaning that for the grades given to the project, gender is not important. This is to be expected, as the project is being evaluated and not the applicant.


```{r effects prob project3, message=F, warning=F}
X<-Model.Prop$model[,-1]

# Average row
Div<-names(which.max(table(X$Division)))
PF<-mean(X$PercentFemale)
IC<-names(which.max(table(X$IsContinuation)))
LA<-mean(X$logAmount)
IT<-names(which.max(table(X$InstType)))


newdata<-data.frame(Gender="f", Division=Div, PercentFemale=PF,
         IsContinuation=IC,logAmount=LA, InstType=IT)
fitted <- predict(Model.Prop,newdata=newdata,type = "cum.prob")
f.cum.prob<-fitted$cprob2
fitted <- predict(Model.Prop,newdata=newdata)
f.prob<-fitted$fit

newdata<-data.frame(Gender="m", Division=Div, PercentFemale=PF,
         IsContinuation=IC,logAmount=LA, InstType=IT)
fitted <- predict(Model.Prop,newdata=newdata,type = "cum.prob")
m.cum.prob<-fitted$cprob2
fitted <- predict(Model.Prop,newdata=newdata)
m.prob<-fitted$fit

gender.prob.project<-rbind(m.prob, f.prob)
rownames(gender.prob.project)<-c("Male","Female")
colnames(gender.prob.project)<-c("poor","average","good", "very good",
                               "excellent","outstanding")

gender.prob.project<-t(gender.prob.project)
gender.prob.project<-cbind(round(gender.prob.project,3),
Difference=round((gender.prob.project[,1]-gender.prob.project[,2]),3))
gender.prob.project<-as.data.frame(gender.prob.project)

kable(gender.prob.project,"latex", booktabs = T, caption="Predicted probabilities of getting different grades") %>%
    kable_styling(position = "center" )
```

Overall the average difference is really small: `r round(mean(abs(gender.prob.project[,3])),5)`. This seems to suggest that there is no evidence of gender influencing the probability of achieving a certain grade. We also represented in the plots below the probability and cumulative probability curves of getting each grade for male and female: they follow more or less the same trend and the only difference, as we've seen from the table above, is that women are slightly more likely to get a "very good" rather than an "excellent".

```{r effects plot project2, message=F, warning=F}
p1 <- ggplot(gender.prob.project) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("Predicted Probability for Project Assessment")+
  theme(plot.title = element_text(size=10))

# Cumulative probability
gender.cum.prob.project<-rbind(m.cum.prob, f.cum.prob)
rownames(gender.cum.prob.project)<-c("Male","Female")
colnames(gender.cum.prob.project)<-c("poor","average","good", "very good",
                                   "excellent","outstanding")

gender.cum.prob.project<-t(gender.cum.prob.project)
gender.cum.prob.project<-cbind(round(gender.cum.prob.project,3),
 Difference=round((gender.cum.prob.project[,1]-gender.cum.prob.project[,2]),3))
gender.cum.prob.project<-as.data.frame(gender.cum.prob.project)

p2 <- ggplot(gender.cum.prob.project) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Cumulative Probability")+
  ggtitle("Cumulative Probability for Project Assessment")+
  theme(plot.title = element_text(size=10))

grid.arrange(p1, p2, ncol=2)
```


```{r new CI plot project external}
CI.project <- confint(Model.Prop)
cof<-Model.Prop$coefficients[6:14]
colors <- rep('not significant', nrow(CI.project))
for (i in 1:nrow(CI.project)){
  if (CI.project[i,1] > 0 & 0 < CI.project[i,2]) {colors[i]='significant'}
  if (CI.project[i,1] < 0 & 0 > CI.project[i,2]) {colors[i]='significant'}
}
lower <- CI.project[,1]
upper <- CI.project[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-1.5,1.5)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)),x=cof, colour=col))
```

From the plot above we can see that gender is not significant, since its confidence interval includes zero. The other significant predictors are Division, the percentage of female reviewers, if the project is a continuation, the Institution type and the log(AmountRequested).


```{r}

gender.prob<-Effect("Gender", mod=Model.Prop)
gender.prob<-gender.prob$prob
gender.prob<-data.frame(Male=gender.prob[1,],Female=gender.prob[2,])
rownames(gender.prob)<-c("poor","average","good", "very good",
                        "excellent","outstanding")

gender.prob<-cbind(round(gender.prob,3),Difference=round((gender.prob[,1]-gender.prob[,2]),3))
       
```

   
    
```{r}
  Model.App<- clm(ApplicantTrack ~ Gender + Division + PercentFemale + IsContinuation + 
                              InstType + logAmount +Gender:PercentFemale,
                             data=external_regression_data)
  App.nogender<- clm(ApplicantTrack ~ Division + PercentFemale + IsContinuation +
                               InstType + logAmount +Gender:PercentFemale,
                             data=external_regression_data)
                            
  p.value<-anova(Model.App,App.nogender)[2,6]
             
```

(2) **Applicant Track assessment**: The final model we used has ApplicantTrack as a response variable, and the following predictors: Gender, Division, PercentFemale, IsContinuation, InstType, log(AmountRequested) and the interaction between Gender and PercentFemale. Again we fitted the same model without Gender and compare it with the anova() function to the one with gender, we get a p.value of `r p.value`, meaning that for the grades given to the main applicant, gender needs to be considered in the model. In the next table we present part of the summary for this model, to see the full summary refer to the Appendix.


```{r clm int applicant track}
Model.App<- clm(ApplicantTrack ~ Gender + Division + PercentFemale + IsContinuation + 
                              InstType + logAmount +Gender:PercentFemale,
                             data=external_regression_data)
App.nogender<- clm(ApplicantTrack ~ Division + PercentFemale + IsContinuation +
                               InstType + logAmount +Gender:PercentFemale,
                             data=external_regression_data)
                            
p.value<-anova(Model.App,App.nogender)[2,6]
p.value <- round(p.value,4)
             
```

We computed the difference in predicted probability of getting a specific grade for both male and female.

```{r ex effects track, message=F, warning=F}
X<-Model.App$model[,-1]

# Average row
Div<-names(which.max(table(X$Division)))
PF<-mean(X$PercentFemale)
IC<-names(which.max(table(X$IsContinuation)))
LA<-mean(X$logAmount)
IT<-names(which.max(table(X$InstType)))


newdata<-data.frame(Gender="f", Division=Div, PercentFemale=PF,
         IsContinuation=IC,logAmount=LA, InstType=IT)
fitted <- predict(Model.App,newdata=newdata,type = "cum.prob")
f.cum.prob<-fitted$cprob2
fitted <- predict(Model.App,newdata=newdata)
f.prob<-fitted$fit

newdata<-data.frame(Gender="m", Division=Div, PercentFemale=PF,
         IsContinuation=IC,logAmount=LA, InstType=IT)
fitted <- predict(Model.App,newdata=newdata,type = "cum.prob")
m.cum.prob<-fitted$cprob2
fitted <- predict(Model.App,newdata=newdata)
m.prob<-fitted$fit

gender.prob.app<-rbind(m.prob, f.prob)
rownames(gender.prob.app)<-c("Male","Female")
colnames(gender.prob.app)<-c("poor","average","good", "very good",
                               "excellent","outstanding")

gender.prob.app<-t(gender.prob.app)
gender.prob.app<-cbind(round(gender.prob.app,3),
Difference=round((gender.prob.app[,1]-gender.prob.app[,2]),3))
gender.prob.app<-as.data.frame(gender.prob.app)

kable(gender.prob.app,"latex", booktabs = T, caption="Predicted probabilities of getting different Applicant Track grades") %>%
    kable_styling(position = "center")

p1 <- ggplot(gender.prob.app) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("Predicted Probability for Applicant Track")+
  theme(plot.title = element_text(size=10))

# Cumulative probability
gender.cum.prob.app<-rbind(m.cum.prob, f.cum.prob)
rownames(gender.cum.prob.app)<-c("Male","Female")
colnames(gender.cum.prob.app)<-c("poor","average","good", "very good",
                                   "excellent","outstanding")

gender.cum.prob.app<-t(gender.cum.prob.app)
gender.cum.prob.app<-cbind(round(gender.cum.prob.app,3),
 Difference=round((gender.cum.prob.app[,1]-gender.cum.prob.app[,2]),3))
gender.cum.prob.app<-as.data.frame(gender.cum.prob.app)

p2 <- ggplot(gender.cum.prob.app) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Cumulative Probability")+
  ggtitle("Cumulative Probability for Applicant Track")+
  theme(plot.title = element_text(size=10))
```

In the table above, we see the predicted probability of getting each grade for both male and female and the difference between the two. The average difference of the cumulative probability is here `r round(mean(abs(gender.prob.app[,3])),5)`, very close to zero. From the plot below we see that there is almost no difference between women and men probabilities. This seems to suggest that the small p-value from the likelihood ratio test is not really reliable to establish if gender has an influence on the applicant track assessment.

```{r ex call effect plots track, echo=F}
grid.arrange(p1,p2,ncol=2)
```


```{r new CI plot track external}
CI.track <- confint(Model.App)
cof<-Model.App$coefficients[6:15]
colors <- rep('not significant', nrow(CI.track))
for (i in 1:nrow(CI.track)){
  if (CI.track[i,1] > 0 & 0 < CI.track[i,2]) {colors[i]='significant'}
  if (CI.track[i,1] < 0 & 0 > CI.track[i,2]) {colors[i]='significant'}
}
lower <- CI.track[,1]
upper <- CI.track[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-2.5,2)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)),x=cof, colour=col))
```

Notice that the confidence interval referring to gender  do not include zero and so the corresponding coefficient is significant. The only variables that appear to be not significant in the determination of the Applicant Track grade, are the institution type and the Division. In this model, Division one is used as a base line, and from the results above, we can see that the only impact in grades, from the division point of view, is if the applicant come from Division two. Likewise, from the institution type point of view, there are different chances of reaching higher grades for applicants in institutions other than ETH and Uni.


```{r ex clm ranking, warning=F, message=F}

Model.Overall<-clm(OverallGrade ~ Gender + PercentFemale + Division + IsContinuation + 
               PreviousRequest + InstType + logAmount, data=external_regression_data)

Over.nogender<- clm(OverallGrade ~ PercentFemale + Division + IsContinuation + 
               PreviousRequest + InstType + logAmount, data=external_regression_data)
                            
p.value<-round(anova(Model.Overall,Over.nogender)[2,6],5)
                          
```

(3) **Overall Grade**: This last model has OverallGrade as a response and Gender, Division, PercentFemale, IsContinuation, InstType, PreviousRequest and logAmount as predictors. We are not considering here the grades given to the applicant track record and to the project, as we just want to see the influence of the demographic data and the project information in each grade. A comparison of this model with the same one without gender may suggest that gender is not significant to the model: p.value of `r p.value`. 

```{r ex effects prob ranking, message=F, warning=F}
X<-Model.Overall$model[,-1]
#Average Data
Div<-names(which.max(table(X$Division)))
PF<-mean(X$PercentFemale)
PR<-names(which.max(table(X$PreviousRequest)))
IC<-names(which.max(table(X$IsContinuation)))
LA<-mean(X$logAmount)
IT<-names(which.max(table(X$InstType)))

newdata<-data.frame(Gender="f", Division=Div, PercentFemale=PF,
        PreviousRequest=PR, IsContinuation=IC,logAmount=LA, InstType=IT)
fitted <- predict(Model.Overall,newdata=newdata, type = "cum.prob")
f.cum.prob<-fitted$cprob2
fitted <- predict(Model.Overall,newdata=newdata)
f.prob<-fitted$fit

newdata<-data.frame(Gender="m",Division="Div 1", PercentFemale=0, 
        PreviousRequest="1",IsContinuation="0",logAmount=13.02518,InstType="Uni")
fitted <- predict(Model.Overall,newdata=newdata, type = "cum.prob")
m.cum.prob<-fitted$cprob2
fitted <- predict(Model.Overall,newdata=newdata)
m.prob<-fitted$fit

gender.prob.Overall<-rbind(m.prob, f.prob)
rownames(gender.prob.Overall)<-c("Male","Female")
colnames(gender.prob.Overall)<-c("poor","average","good", "very good",
                               "excellent","outstanding")

gender.prob.Overall<-t(gender.prob.Overall)
gender.prob.Overall<-cbind(round(gender.prob.Overall,3),
Difference=round((gender.prob.Overall[,1]-gender.prob.Overall[,2]),3))
gender.prob.Overall<-as.data.frame(gender.prob.Overall)

kable(gender.prob.Overall,"latex", booktabs = T, caption="Predicted probabilities of getting different Overall Grades") %>%
    kable_styling(position = "center")

p1 <- ggplot(gender.prob.Overall) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("Predicted Probability for Ranking")+
  theme(plot.title = element_text(size=10))

# Cumulative probability
gender.cum.prob.Overall<-rbind(m.cum.prob, f.cum.prob)
rownames(gender.cum.prob.Overall)<-c("Male","Female")
colnames(gender.cum.prob.Overall)<-c("poor","average","good", "very good",
                                   "excellent","outstanding")

gender.cum.prob.Overall<-t(gender.cum.prob.Overall)
gender.cum.prob.Overall<-cbind(round(gender.cum.prob.Overall,3),
 Difference=round((gender.cum.prob.Overall[,1]-gender.cum.prob.Overall[,2]),3))
gender.cum.prob.Overall<-as.data.frame(gender.cum.prob.Overall)

p2 <- ggplot(gender.cum.prob.Overall) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Cumulative Probability")+
  ggtitle("Cumulative Probability for Ranking")+
  theme(plot.title = element_text(size=10))
```

The predicted probabilities of achieving certain grade for male and female is shown in the next table. The average difference of the cumulative probability is here as well close to zero (`r round(mean(abs(gender.prob.Overall[,3])),5)`). Notice that the only difference is that female applicants are mole likely to get a "good" grade rather than a "very good", compared to male applicants. From the cumulative probability plot below, we see that the trend is the same for both genders and that the difference is not relevant.
    
```{r, echo=F}
grid.arrange(p1,p2,ncol=2)
```



```{r new CI plot ranking external}
CI.Rank <- confint(Model.Overall)
cof<-Model.Overall$coefficients[6:15]
colors <- rep('not significant', nrow(CI.Rank))
for (i in 1:nrow(CI.Rank)){
  if (CI.Rank[i,1] > 0 & 0 < CI.Rank[i,2]) {colors[i]='significant'}
  if (CI.Rank[i,1] < 0 & 0 > CI.Rank[i,2]) {colors[i]='significant'}
}
lower <- CI.Rank[,1]
upper <- CI.Rank[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-2,1.5)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)), x=cof, colour=col))
```

```{r gender ex clm Overall coef, echo=F}
# summary(Model.Overall)
# Genderf -0.31
# exp(-0.31) 0.7334
```


Here gender seems to be not significant, since its confidence interval include zero. However the difference between the upper bound and zero is really small. The other significant variables in this model are the percentage of female referees, log(AmountRequested), Is Continuation, the institution type and Division.

Here again gender seems to be not significant, since its confidence interval includes zero. However the difference between the upper bound and zero is really small. The other significant variables in this model are the percentage of female referees, PreviousRequest, and the institution type.

## Internal Step

### **Logistic Regression**

We now repeat the logistic regression for the internal step, to understand if gender has an influence on the final funding decision, based on the information provided in the internal step. 

Regression data:  as explained before, in order to perform the analysis, we combined in a single data frame all the information about applications (IsApproved, Age, Gender, Division, IsContinuation, PreviousRequest, InstType, log(AmountRequested), Semester) and about grades given by the internal referees (ApplicantTrack, ProjectAssessment, Ranking, PercentFemale). 
    
We had again a perfect separation problem, due to the fact that there are very few approved applications with grades worse than "good". We aggregated the grades which were in category "poor", "average" and "good" in a unique category for both the applicant track record and the project assessment. All grades are considered as factors. Since the proportion of female referees who evaluate the application takes only values 0, 0.5 and 1, we consider it as a factor with 3 levels.

We didn't standardized the continuous variables, in order to be able to interpret the coefficients estimated from the model.

```{r get data for logistic internal}
internal_regression_data<-prepare_data_internal_log_regression(final.apps,final.internal)
internal_regression_data$ApplicantTrack <- factor(internal_regression_data$ApplicantTrack, ordered = F)
internal_regression_data$ProjectAssessment <- factor(internal_regression_data$ProjectAssessment, ordered = F)
internal_regression_data$logAmount <- log(internal_regression_data$AmountRequested)
#internal_regression_data$Semester <- revalue(internal_regression_data$Semester, c("ottobre"="Oct", "aprile"="Apr"))
internal_regression_data$Semester <- revalue(internal_regression_data$Semester, c("Oktober"="Oct", "April"="Apr"))
data<- subset(internal_regression_data,select = -c(ProjectID, Ranking, AmountRequested,
                                                   NumberExternalReviewers, NumberInternalReviewers))

## COMBINE GRADES otherwise it doesn't work
data$ApplicantTrack <- ifelse(as.numeric(data$ApplicantTrack) < 4,3,data$ApplicantTrack)
data$ApplicantTrack <- factor(data$ApplicantTrack)
data$ProjectAssessment <- ifelse(as.numeric(data$ProjectAssessment) <4,3,data$ProjectAssessment)
data$ProjectAssessment <- factor(data$ProjectAssessment)
```

```{r full logistic regression, echo=F}
# Fit the full model with all variables and interactions
  Model <- glm(IsApproved ~. +Gender:Division+ Gender:PercentFemale+ Gender:ApplicantTrack+ 
                         InstType:Division, data=data, family="binomial")
  PsR2<-(1-exp((Model$dev-Model$null)/1623))/(1-exp(-Model$null/1623))
```

We first used the 'glm' function in R to fit a full model with all the available variables and the interactions between Gender and Division, Gender and PercentFemale, Gender and ApplicantTrack. Moreover, we considered the interaction between InstType and Division. We didn't include the Ranking grades into the model, since they are highly correlated with the single grades for the applicant and the project. We achieved a pseudo-R^2 value of  `r round(PsR2,4)`, meaning that the variation in the binary variable Y (approved or not) can be explained for more than half by this model including only the internal step information.

<!-- ```{r anova logistic internal} -->
<!-- anova(Model, test="Chisq") -->
<!-- ``` -->

<!-- From the output of the anova function, we see that many of the variables in the full model are not relevant, since their p-values are clearly bigger than 0.05. Notice that at this ateg of the analysis the variable gender seems to have an impact on the probability of being approved. -->

Then, we did variable selection using the AIC criteria again with the drop1() function, in order to obtain a small and effective model as we did for the external step. The remaining predictors are Gender, Age, Semester, IsContinuation, PercentFemale, ApplicantTrack, ProjectAssessment and log(AmountRequested). Interactions were removed from the model because not significant.

```{r logistic internal regression}
Model.log.int <- glm(IsApproved ~ Gender + Age + Semester + IsContinuation + PercentFemale + 
    ApplicantTrack + ProjectAssessment + logAmount, data=data, family="binomial")

# summary(Model.log.int)
coef.log <- Model.log.int$coefficients
CI <- confint(Model.log.int)

PsR2.small<-(1-exp((Model.log.int$dev-Model.log.int$null)/1623))/(1-exp(-Model.log.int$null/1623)) #0.69135
```

The pseudo R^2 for this model is `r round(PsR2.small,4)` , i.e. this reduced model explains basically the same proportion of variance of the data as the previous model. Even if we removed some variables, the model still explains almost 70% of the variation of the variable IsApproved. It therefore seems that the internal grades are significant predictors for the final funding decision. 

In order to assess if gender is a relevant predictor for this regression, we computed the 95% confidence intervals for the coefficients of our model. We can see them in the following plot: those that are red include 0 and therefore they are not significant. Those in blue don't include zero and so they are significant at a 5% level.

```{r new CI plot logistic internal}

colors <- rep('not significant', nrow(CI))
for (i in 1:nrow(CI)){
  if (CI[i,1] > 0 & 0 < CI[i,2]) {colors[i]='significant'}
  if (CI[i,1] < 0 & 0 > CI[i,2]) {colors[i]='significant'}
}
lower <- CI[,1]
upper <- CI[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-5,8)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)), x=coef.log, colour=col))
```

The confidence intervals which don't include zero are those for the variables: IsContinuation, ApplicantTrack (all levels from 4 to 6) and ProjectAssessment (all levels from 4 to 6). Notice that the coefficient for gender is not significant, since its confidence interval (`r round(df[which(rownames(df)=="Genderf"),1],4)`,`r round(df[which(rownames(df)=="Genderf"),1],4)`) contains zero. The point estimate for gender is `r coef.log[2]` and this means that the odds with respect to being approved or not given that the applicant is a woman is `r exp(coef.log[2])` times larger than the odds for a man (which is the reference level).

In order to interpret the coefficient estimates, we exponentiated them and we obtained the following values:

```{r coefficients logistic internal}

id<-  which(CI[,1]>0 & CI[,2]>0)
sign.coef <- coef.log[id]
sign.OR <- exp(sign.coef)

#sign.OR

kable(round(cbind(OR=sign.OR,exp(confint(Model.log.ext)[id,])), 2),"latex", booktabs = T, caption="Odd Ratio and corresponding confidence intervals, Internal Logistic Regression")%>%
  kable_styling(position = "center")

```

* The odds with respect to being approved or not given that the project is a continuation is `r round(sign.OR[1],2)` times larger than for a project which is not a continuation, keeping all other variables constant;
* The odds with respect to being approved or not given that the Applicant Track grade is 4 is `r round(sign.OR[2],2)` larger than the odds for the reference level (Applicant Track grade from 1 to 3), keeping all other variables constant;
* The odds with respect to being approved or not given that the Applicant Track grade is 5 is `r round(sign.OR[3],2)` larger than the odds for the reference level (Applicant Track grade from 1 to 3), keeping all other variables constant;
* The odds with respect to being approved or not given that the Applicant Track grade is 6 is `r round(sign.OR[4],2)` larger than the odds for the reference level (Applicant Track grade from 1 to 3), keeping all other variables constant;
* The odds with respect to being approved or not given that the Project Assessment grade is 4 is `r round(sign.OR[5],2)` larger than the odds for the reference level (Project Assessment grade from 1 to 3), keeping all other variables constant;
* The odds with respect to being approved or not given that the Project Assessment grade is 5 is `r round(sign.OR[6],2)` larger than the odds for the reference level (Project Assessment grade from 1 to 3), keeping all other variables constant;
* The odds with respect to being approved or not given that the Project Assessment grade is 6 is `r round(sign.OR[7],2)` larger than the odds for the reference level (Project Assessment grade from 1 to 3), keeping all other variables constant.

```{r effect plots logistic internal, fig.align="center", fig.height=3}
# Obtain all the effects        
  eff.fit <- allEffects(Model.log.int)
  
# Obtain Gender effect as a separate data frame 
  eff<-Effect("Gender", Model.log.int)
  eff<-eff$fit
  prob<- round((exp(eff)/(1+exp(eff))),3)
```

As said before, our focus is the effect of gender in each step of the evaluation process. We checked the difference in the predicted probability of being accepted between male and female: women have probability `r prob[2,1]` of the project being approved, while men have `r prob[1,1]`. Surprisingly, the predicted probability of being funded seems to be higher for female applicants, even if the difference is too small to be relevant. We can clearly see that from the Gender effect plot, where the confidence intervals are overlapping and the line is almost horizontal. From this initial analysis we can say that it doesn't seem that referees are biased against women.

```{r effect gender logistic}
  plot(Effect("Gender",Model.log.int),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
```

### **Ordinal Regression**

To see which variables influence the different grades in the second step of the evaluation process and check if the gender of the main applicant has an influence on it, we ran an ordinal regression with the function 'clm' of the package 'ordinal' in R. 

We did this for all grades given by the internal reviewers:

1. The grades given to the scientific proposal (ProjectAssessment)

2. The applicant grade (ApplicantTrack)

3. The Ranking Grade. 


```{r clm project assessment}
internal_regression_data<-prepare_data_internal_log_regression(final.apps,final.internal)
#internal_regression_data$Semester <- revalue(internal_regression_data$Semester, c("ottobre"="Oct", "aprile"="Apr"))
internal_regression_data$Semester <- revalue(internal_regression_data$Semester, c("Oktober"="Oct", "April"="Apr"))
internal_regression_data$logAmount <- log(internal_regression_data$AmountRequested)
internal_regression_data$PercentFemale <- factor(internal_regression_data$PercentFemale)

# Final Model after variable selection
Model.project <- clm(ProjectAssessment ~ Gender + Division + PercentFemale + Age + 
               IsContinuation + InstType + logAmount, data=internal_regression_data)

Model2.project <- clm(ProjectAssessment ~ Division + PercentFemale + Age + 
                IsContinuation + InstType + logAmount, data=internal_regression_data)

# Compare the models 
pvalue <- anova(Model2.project, Model.project)[2,6]
```

(1) **Scientific Proposal**: we fitted the full model with ProjectAssessment as response variable, using the clm function. Then we selected the significant variables with the AIC criteria and the help of the drop1() function in R. We performed a stepwise procedure by hand, removing each time the variable that was minimizing the most the AIC. We ended up with a model with the following predictors: Gender, Division, PercentFemale, Age, IsContinuation, InstType and log(AmountRequested). We also fitted the same model without Gender (using the clm function again) and compared it to the previous one using the likelihood ratio test implemented in the anova function: we got a p.value of `r pvalue`, meaning that Gender seem to be a significant predictor for the grades given to the project by the internal referees. 

Below you see the predicted confidence interval for all the coefficients in our model: all the variables are significant (except for one level of Division and one for PercentFemale). Notice that the upper bound of the gender confidence interval is really close to zero. From this results, we cannot say that there is clear evidence of gender bias.

```{r new CI plot project internal}
CI.project <- confint(Model.project)
coef.project <- coef(Model.project)[6:16]
colors <- rep('not significant', nrow(CI.project))
for (i in 1:nrow(CI.project)){
  if (CI.project[i,1] > 0 & 0 < CI.project[i,2]) {colors[i]='significant'}
  if (CI.project[i,1] < 0 & 0 > CI.project[i,2]) {colors[i]='significant'}
}
lower <- CI.project[,1]
upper <- CI.project[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-1.5,1.5)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)), x=coef.project, colour=col))
```

Since we have gender as a predictor in our model, we can show the effect it has on the grades. For this purpose, we estimated the predicted probability of falling in the different categories for each gender and the difference between men and women. The result is presented in the following table.

```{r effects prob project, message=F, warning=F}
X<-Model.project$model[,-1]

newdata<-data.frame(Gender="f", Division="Div 1", PercentFemale='0',
        Age=48, IsContinuation="0",logAmount=13.02518, InstType="Uni")
fitted <- predict(Model.project,newdata=newdata, type = "cum.prob")
f.cum.prob<-fitted$cprob2
fitted <- predict(Model.project,newdata=newdata)
f.prob<-fitted$fit

newdata<-data.frame(Gender="m",Division="Div 1", PercentFemale="0", 
        Age=48,IsContinuation="0",logAmount=13.02518,InstType="Uni")
fitted <- predict(Model.project,newdata=newdata, type = "cum.prob")
m.cum.prob<-fitted$cprob2
fitted <- predict(Model.project,newdata=newdata)
m.prob<-fitted$fit

gender.prob.project<-rbind(m.prob, f.prob)
rownames(gender.prob.project)<-c("Male","Female")
colnames(gender.prob.project)<-c("poor","average","good", "very good",
                               "excellent","outstanding")

gender.prob.project<-t(gender.prob.project)
gender.prob.project<-cbind(round(gender.prob.project,3),
Difference=round((gender.prob.project[,1]-gender.prob.project[,2]),3))
gender.prob.project<-as.data.frame(gender.prob.project)

kable(gender.prob.project,"latex", booktabs = T, caption="Predicted probabilities of getting different Scientific Proposal Grades") %>%
    kable_styling(position = "center")

```

Overall the average difference is really small: `r round(mean(abs(gender.prob.project[,3])),5)`. Even though gender is a signifiant predictor, we can see from the predicted probabilities that gender does not have a meaningful impact on the probability of achieving a certain grade. 

```{r effects plot project, message=F, warning=F}
p1 <- ggplot(gender.prob.project) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("Predicted Probability for Project Assessment")+
  theme(plot.title = element_text(size=10))

# Cumulative probability
gender.cum.prob.project<-rbind(m.cum.prob, f.cum.prob)
rownames(gender.cum.prob.project)<-c("Male","Female")
colnames(gender.cum.prob.project)<-c("poor","average","good", "very good",
                                   "excellent","outstanding")

gender.cum.prob.project<-t(gender.cum.prob.project)
gender.cum.prob.project<-cbind(round(gender.cum.prob.project,3),
 Difference=round((gender.cum.prob.project[,1]-gender.cum.prob.project[,2]),3))
gender.cum.prob.project<-as.data.frame(gender.cum.prob.project)

p2 <- ggplot(gender.cum.prob.project) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Cumulative Probability")+
  ggtitle("Cumulative Probability for Project Assessment")+
  theme(plot.title = element_text(size=10))

grid.arrange(p1, p2, ncol=2)
```

This conclusion is reinforced when we look at the plots above, which show the probability and cumulative probability curves of getting each grade for male and female.  They follow more or less the same trend and the only difference, as we've seen from the table above, is that women are slightly more likely to get a "very good" rather than an "excellent". We therefore conclude that gender is a significant, but not practically relevant, predictor of the different grades.


```{r clm applicant track}
Model.track <- clm(ApplicantTrack ~ Gender + Division + PercentFemale + IsContinuation + 
               InstType + Semester + logAmount + Gender:Division + Division:PercentFemale, 
             data=internal_regression_data)

Model2.track <- clm(ApplicantTrack ~ Division + PercentFemale + IsContinuation + 
               InstType + Semester + logAmount + Division:PercentFemale, data=internal_regression_data)

pvalue <- round(anova(Model.track,Model2.track)[2,6],5)
```

(2) **Applicant Track assessment**: The model we used has ApplicantTrack as a response variable and the following predictors: Gender, Division, PercentFemale, IsContinuation, InstType, log(AmountRequested), Semester, the interaction between Gender and Division and the interaction between Division and PercentFemale. Again we fitted the same model without Gender and compare it to the one with gender using the likelihood ratio test included in the anova() function. We got a p.value of $`r pvalue`$, meaning that for the grades given to the main applicant track record, gender should be included in the model. We see from the Confidence Interval plot that the interaction Gender:Division is significant. The other significant variables seem to be: Division, Institution type, IsContinuation and the percentage of female reviewers. Finally, the interaction between Division and the percentage of female seems to have some importance too. However, once again, gender is signficant but not practically relevant, as evidenced by the negligible differences between genders in the table below.


```{r new CI plot track internal}
coef.track <- coef(Model.track)[6:22]
CI.track <- confint(Model.track)

colors <- rep('not significant', nrow(CI.track))
for (i in 1:nrow(CI.track)){
  if (CI.track[i,1] > 0 & 0 < CI.track[i,2]) {colors[i]='significant'}
  if (CI.track[i,1] < 0 & 0 > CI.track[i,2]) {colors[i]='significant'}
}
lower <- CI.track[,1]
upper <- CI.track[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-2.5,2)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)), x=coef.track, colour=col))
```


```{r effects track, message=F, warning=F}
X<-Model.track$model[,-1]
newdata<-data.frame(Gender="f", Division="Div 1", PercentFemale='0',
        Semester="Apr", IsContinuation="0",logAmount=13.02518, InstType="Uni")
fitted <- predict(Model.track,newdata=newdata, type = "cum.prob")
f.cum.prob<-fitted$cprob2
fitted <- predict(Model.track,newdata=newdata)
f.prob<-fitted$fit

newdata<-data.frame(Gender="m",Division="Div 1", PercentFemale='0', 
        Semester="Apr",IsContinuation="0",logAmount=13.02518,InstType="Uni")
fitted <- predict(Model.track,newdata=newdata, type = "cum.prob")
m.cum.prob<-fitted$cprob2
fitted <- predict(Model.track,newdata=newdata)
m.prob<-fitted$fit

gender.prob.track<-rbind(m.prob, f.prob)
rownames(gender.prob.track)<-c("Male","Female")
colnames(gender.prob.track)<-c("poor","average","good", "very good",
                               "excellent","outstanding")

gender.prob.track<-t(gender.prob.track)
gender.prob.track<-cbind(round(gender.prob.track,3),
Difference=round((gender.prob.track[,1]-gender.prob.track[,2]),3))
gender.prob.track<-as.data.frame(gender.prob.track)

kable(gender.prob.track,"latex", booktabs = T, caption="Predicted probabilities of getting different Applicant Track grades") %>%
    kable_styling(position = "center")

p1 <- ggplot(gender.prob.track) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("Predicted Probability for Applicant Track")+
  theme(plot.title = element_text(size=10))

# Cumulative probability
gender.cum.prob.track<-rbind(m.cum.prob, f.cum.prob)
rownames(gender.cum.prob.track)<-c("Male","Female")
colnames(gender.cum.prob.track)<-c("poor","average","good", "very good",
                                   "excellent","outstanding")

gender.cum.prob.track<-t(gender.cum.prob.track)
gender.cum.prob.track<-cbind(round(gender.cum.prob.track,3),
 Difference=round((gender.cum.prob.track[,1]-gender.cum.prob.track[,2]),3))
gender.cum.prob.track<-as.data.frame(gender.cum.prob.track)

p2 <- ggplot(gender.cum.prob.track) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Cumulative Probability")+
  ggtitle("Cumulative Probability for Applicant Track")+
  theme(plot.title = element_text(size=10))
```

We computed the predicted probability of getting each grade for both male and female and the difference between the two. The average difference of the cumulative probability is here `r round(mean(abs(gender.prob.track[,3])),5)`, very close to zero. From the plot below we see that there is almost no difference between women and men probabilities. 

```{r call effect plots track, echo=F}
grid.arrange(p1,p2,ncol=2)
```


```{r clm ranking, warning=F, message=F}

Model.Rank <- clm(Ranking ~  Gender + PercentFemale + Division + IsContinuation + 
               PreviousRequest + InstType + logAmount, data=internal_regression_data)

Model2.Rank <- clm(Ranking ~  PercentFemale + Division + IsContinuation + 
                PreviousRequest + InstType + logAmount, data=internal_regression_data)

pvalue <- anova(Model2.Rank, Model.Rank)[2,6]
                          
```

(3) **Ranking**: This last model has Ranking as a response and Gender, Division, PercentFemale, IsContinuation, InstType, PreviousRequest and logAmount as predictors. We are not considering here the grades given to the applicant track record and to the project, as we just want to see the influence of the demographic data and the project information in each grade. A comparison of this model with the same one without gender may suggest that gender is significant to the model: p.value of `r pvalue`.

We can also see from the plot below that the upper bound of the confidence interval of gender is not so far from 0. This also suggests that the previous p-value is not completely trustable.

```{r new CI plot ranking internal}
coef.rank <- coef(Model.Rank)[6:16]
CI.Rank <- confint(Model.Rank)
colors <- rep('not significant', nrow(CI.Rank))
for (i in 1:nrow(CI.Rank)){
  if (CI.Rank[i,1] > 0 & 0 < CI.Rank[i,2]) {colors[i]='significant'}
  if (CI.Rank[i,1] < 0 & 0 > CI.Rank[i,2]) {colors[i]='significant'}
}
lower <- CI.Rank[,1]
upper <- CI.Rank[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-2,1.5)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)), x=coef.rank, colour=col))
```


```{r effects prob ranking, message=F, warning=F}
X<-Model.Rank$model[,-1]
newdata<-data.frame(Gender="f", Division="Div 1", PercentFemale='0',
        PreviousRequest="1", IsContinuation="0",logAmount=13.02518, InstType="Uni")
fitted <- predict(Model.Rank,newdata=newdata, type = "cum.prob")
f.cum.prob<-fitted$cprob2
fitted <- predict(Model.Rank,newdata=newdata)
f.prob<-fitted$fit

newdata<-data.frame(Gender="m",Division="Div 1", PercentFemale='0', 
        PreviousRequest="1",IsContinuation="0",logAmount=13.02518,InstType="Uni")
fitted <- predict(Model.Rank,newdata=newdata, type = "cum.prob")
m.cum.prob<-fitted$cprob2
fitted <- predict(Model.Rank,newdata=newdata)
m.prob<-fitted$fit

gender.prob.Rank<-rbind(m.prob, f.prob)
rownames(gender.prob.Rank)<-c("Male","Female")
colnames(gender.prob.Rank)<-c("D","C","BC", "B",
                                   "AB","A")

gender.prob.Rank<-t(gender.prob.Rank)
gender.prob.Rank<-cbind(round(gender.prob.Rank,3),
Difference=round((gender.prob.Rank[,1]-gender.prob.Rank[,2]),3))
gender.prob.Rank<-as.data.frame(gender.prob.Rank)

kable(gender.prob.Rank,"latex", booktabs = T, caption="Predicted probabilities of getting different Ranking grades") %>%
    kable_styling(position = "center")


p1 <- ggplot(gender.prob.Rank) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("Predicted Probability for Ranking")+
  theme(plot.title = element_text(size=10))

# Cumulative probability
gender.cum.prob.Rank<-rbind(m.cum.prob, f.cum.prob)
rownames(gender.cum.prob.Rank)<-c("Male","Female")
colnames(gender.cum.prob.Rank)<-c("D","C","BC", "B",
                                   "AB","A")

gender.cum.prob.Rank<-t(gender.cum.prob.Rank)
gender.cum.prob.Rank<-cbind(round(gender.cum.prob.Rank,3),
 Difference=round((gender.cum.prob.Rank[,1]-gender.cum.prob.Rank[,2]),3))
gender.cum.prob.Rank<-as.data.frame(gender.cum.prob.Rank)

p2 <- ggplot(gender.cum.prob.Rank) +
  geom_line(aes(1:6, Male, colour = "Male")) +
  geom_line(aes(1:6, Female, colour = "Female"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Cumulative Probability")+
  ggtitle("Cumulative Probability for Ranking")+
  theme(plot.title = element_text(size=10))
```

Once again the differences between male and female predicted probabilities are not too large, so we do not find gender to be a meaningful predictor of the grade. However, we do notice that between male and female for the top three grades (A, AB, B), there is a cumulative difference of 7.7pp.
    
```{r, echo=F}
grid.arrange(p1,p2,ncol=2)
```
 

## Results

### External Step

The external step model is not a good explanation of the variation in the approval of applications (around 42%). We nevertheless look into the influence of gender at this stage, but couldn't find evidence of its effect on the final decision.

When looking at the different grades in this step, we found that gender did not have a significant influence on Applicant Track, ProposalCombined, or OverallGrade. 

### Internal Step

The internal model is a quite good explanation of the variation in the approval of applications (around 70% of the variability is explained). When looking at each criteria of evaluation, we found that gender seems to have a small influence in the grade given, however this effect is not practically relevant.

Considering all the analysis that we have done so far, we can say that there is no evidence of gender bias in the funding decision at the Swiss National Science Foundation. Gender is generally not an important variable in the regressions we performed and the difference in the predicted probability between male and female applicants is not practically relevant.


# Relative Importance of the Different Steps

Our second research question was to assess the relative importance of each step in the process, and the relative importance of each criteria within each step. To answer this question, we took the following approach: 

(@) **Most Important Step: Logistic Regression:** We first fit a logistic regression with IsApproved as our binary response variable using the glm function, with the demographic data and the summary grades from the external and internal step to assess which step of the process was most important to determining the final funding decision. 

(@) **Most Important Criteria within Each Step:** We then assessed which criteria was most important within each step to predict the overall grade given to an application (OverallGrade in the External step, Ranking in the Internal step). We did this by fitting an Ordinal Regression (with the OverallGrade (for external) and Ranking (for internal) as the response) with standardized coefficients, and then comparing the magnitude of each predictor. We use this approach for first the external and then the internal step. 

## Most Important Step: Logistic Regression

```{r board regression}

board_data1 <- prepare_data_board_log_regression(apps=final.apps, internal = final.internal, external = final.external)

board_data1$Ranking <- ifelse(board_data1$Ranking %in% c(1,2,3), 3, board_data1$Ranking)
board_data1$Ranking <- factor(board_data1$Ranking, ordered=T)
board_data1$OverallGrade <- ifelse(board_data1$OverallGrade %in% c(1,2,3), 3, board_data1$OverallGrade)
board_data1$OverallGrade <- factor(board_data1$OverallGrade, ordered=T)

board_log_regression <- glm(board_data1$IsApproved ~ Gender + Division + Age + IsContinuation + InstType + log(AmountRequested) + 
                              Ranking + OverallGrade + Gender:Division + PercentFemale + 
                              PercentFemale:Gender, family="binomial", data = board_data1)

calc_pseudo_r <- function(log_regression_object) {
  n <- dim(log_regression_object$data)[1]
  pseudo <- (1 - exp((log_regression_object$dev - log_regression_object$null)/n)) / (1-exp(-log_regression_object$null/n))
  return(pseudo)
}

full.board.psuedo.r <- calc_pseudo_r(board_log_regression)

```

To approach the question of which step in the process is most important, we first fit a logistic regression with IsApproved as our binary response variable using the glm function in R. We fit a full model with all potential demographic predictors and interactions across the different steps of the process, and the summary grade given to an application in the external (OverallGrade) and internal (Ranking) step. To address the first part of the question (relative importance of each step in the process), we used only the summary grade in each step due to the correlation between the individual grades given within each step and the summary grade given. With the full model (predictors: Gender, Division, Age, IsContinuation, InstType, log(AmountRequested), PercentFemale, Ranking, OverallGrade, Gender:Division, PercentFemale:Gender), we achieved a pseudo-R^2 value of  `r round(full.board.psuedo.r,4)`, indicating that percent of the variation in Y can be explained by the model. 

```{r}
board_log_regression_small <- glm(board_data1$IsApproved ~ Age + IsContinuation + Ranking + OverallGrade, family="binomial", data = board_data1)

board_r_small <- calc_pseudo_r(board_log_regression_small)
```

As our goal was to explain the most important factors, we then did backwards variable selection using the AIC. This left us with a model with only 4 predictors: Ranking, OverallGrade, Age, and IsContinuation. The pseudo R^2 measure of this model is `r round(board_r_small,4)`, which indicates that this simplified model nearly explains exactly as much variance in the data as the full model, and so we can be content to use just the small model.

Now that we've reduced our model to 4 predictors, we wanted to understand exactly how important each of those predictors are to the final funding decision. To do this, we looked at the confidence intervals of the coefficients to see which had the largest impact. To do this, we needed to first standardize our continuous variables (Age). When we plot the confidence intervals of our coefficients, we can see that the internal Ranking has by far the largest coefficient, and thus the biggest impact on the final funding decision.     

```{r variable importance board regression}

board_data2 <- prepare_data_board_log_regression(apps=final.apps, internal = final.internal, external = final.external)
board_data2$Age <- scale(board_data2$Age)
board_data2$Ranking <- ifelse(board_data2$Ranking %in% c(1,2,3), 3, board_data2$Ranking)
board_data2$Ranking <- factor(board_data2$Ranking, ordered=F)
board_data2$OverallGrade <- ifelse(board_data2$OverallGrade %in% c(1,2,3), 3, board_data2$OverallGrade)
board_data2$OverallGrade <- factor(board_data2$OverallGrade, ordered=F)

fit7 <- glm(board_data2$IsApproved ~ Age + IsContinuation + Ranking + OverallGrade, family="binomial", data = board_data2)


CI <- confint(fit7)
coef.log <- fit7$coefficients


```


```{r new CI plot logistic board}

colors <- rep('not significant', nrow(CI))
for (i in 1:nrow(CI)){
  if (CI[i,1] > 0 & 0 < CI[i,2]) {colors[i]='significant'}
  if (CI[i,1] < 0 & 0 > CI[i,2]) {colors[i]='significant'}
}
lower <- CI[,1]
upper <- CI[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-5,8)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)),x=coef.log, colour=col))
```

## Most Important Criteria Within Each Step

The second aspect of this question was to identify what was the most important criteria within each step.

### External step

In order to check which variable has the biggest influence on the final grade given by the external reviewers, we fitted an ordinal regression using the "OverallGrade" grade as multinomial response and the Applicant Track grade, the ProposalCombined grade and some demographic data as predictors. We started as always fitting the full model and we did variable selection using the AIC. The explanatory variables included in the final model are: ApplicantTrack, ProposalCombined, Gender, Division, PercentFemale, IsContinuation, PreviousRequest and Semester.

```{r data for importance external step}

# Obtain Regression Data
external_regression_data<-prepare_data_external_log_regression(final.apps,final.external)
# get grades as factors
external_regression_data$logAmount<-log(external_regression_data$AmountRequested)
external_regression_data$Semester <- revalue(external_regression_data$Semester, c("Oktober"="Oct", "April"="Apr"))
data <- subset(external_regression_data,select = -c(ProjectID, AmountRequested))

data$ApplicantTrack<-factor(data$ApplicantTrack, ordered=F)

data$ScientificRelevance<-factor(data$ScientificRelevance, ordered=F)

data$Suitability<-factor(data$Suitability, ordered=F)

data$ProposalCombined<-factor(data$ProposalCombined, ordered=F)

ex_data <- data

```

We also computed the predicted probabilities for each of the Overall grade, varying the Applicant Track grade (plot on the left) and the ProposalCombined grade (plot on the right). It is clear that the same variation in grade implies different changes in probabilities: when the project grade changes, the probability variation is much bigger. This confirms what we found before: the quality of the project has a greater influence on the final ranking, compared to the track record.

```{r relative importance external}
Model.ex.step<-clm(OverallGrade ~ Gender + Division + PercentFemale +
                        ProposalCombined + ApplicantTrack + IsContinuation +
                        PreviousRequest + Semester, data=ex_data)
# summary(Model.ex.step)

```

```{r new CI plot importance external}
CI.ex.step <- confint(Model.ex.step)
cof<-Model.ex.step$coefficients[6:22]
colors <- rep('not significant', nrow(CI.ex.step))
for (i in 1:nrow(CI.ex.step)){
  if (CI.ex.step[i,1] > 0 & 0 < CI.ex.step[i,2]) {colors[i]='significant'}
  if (CI.ex.step[i,1] < 0 & 0 > CI.ex.step[i,2]) {colors[i]='significant'}
}
lower <- CI.ex.step[,1]
upper <- CI.ex.step[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-7,22)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)),x=cof, colour=col))
```

From the plot above we can see that the variable whose coefficients are at the biggest distance from 0 is the ProposalCombined, when the grade is 5 or 6. The grades given to the track record of the applicant are significant too, since the confidence intervals don't include 0 but we can see that the effect is smaller.

```{r ex effect plot for track}
X <- Model.ex.step$model[,-1]

#Predicted probabilities for track=1
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale=0,
          PreviousRequest="1", IsContinuation="0",ProposalCombined="4",
          ApplicantTrack="1",Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
at1.prob<-fitted$fit

#Predicted probabilities for track=2
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale=0,
          PreviousRequest="1", IsContinuation="0",ProposalCombined="4",
          ApplicantTrack="2",Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
at2.prob<-fitted$fit

#Predicted probabilities for track=3
newdata<-data.frame(Gender="f", Division="Div 1",  PercentFemale=0,
          PreviousRequest="1", IsContinuation="0",ProposalCombined="4",
          ApplicantTrack="3",Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
at3.prob<-fitted$fit

#Predicted probabilities for track=4
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale=0,
          PreviousRequest="1", IsContinuation="0",ProposalCombined="4",
          ApplicantTrack="4",Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
at4.prob<-fitted$fit

#Predicted probabilities for track=5
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale=0,
          PreviousRequest="1", IsContinuation="0",ProposalCombined="4",
          ApplicantTrack="5",Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
at5.prob<-fitted$fit

#Predicted probabilities for track=6
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale=0,
          PreviousRequest="1", IsContinuation="0",ProposalCombined="4",
          ApplicantTrack="6",Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
at6.prob<-fitted$fit

gender.prob.track<-rbind(at1.prob,at2.prob,at3.prob,at4.prob,at5.prob,at6.prob)
rownames(gender.prob.track)<-c("Track1","Track2",'Track3','Track4',
                               'Track5','Track6')
colnames(gender.prob.track)<-c("poor","average","good", "very good",
                               "excellent","outstanding")
gender.prob.track<-as.data.frame(t(gender.prob.track))

p1<- ggplot(gender.prob.track) +
  geom_line(aes(1:6, Track1, colour = "Track1")) +
  geom_line(aes(1:6, Track2, colour = "Track2"))+
  geom_line(aes(1:6, Track3, colour = "Track3"))+
  geom_line(aes(1:6, Track4, colour = "Track4"))+
  geom_line(aes(1:6, Track5, colour = "Track5"))+
  geom_line(aes(1:6, Track6, colour = "Track6"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("OverallGrade Probability vs Track Record")
```


```{r ex effect plots for project}
#Predicted probabilities for Project=1
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale=0,
          PreviousRequest="1", IsContinuation="0", ProposalCombined="1",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
p1.prob<-fitted$fit

#Predicted probabilities for Project=2
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale=0,
          PreviousRequest="1", IsContinuation="0", ProposalCombined="2",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
p2.prob<-fitted$fit

#Predicted probabilities for Project=3
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale=0,
          PreviousRequest="1", IsContinuation="0", ProposalCombined="3",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
p3.prob<-fitted$fit

#Predicted probabilities for Project=4
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale=0,
          PreviousRequest="1", IsContinuation="0", ProposalCombined="4",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
p4.prob<-fitted$fit

#Predicted probabilities for Project=5
newdata<-data.frame(Gender="f", Division="Div 1", PercentFemale=0,
          PreviousRequest="1", IsContinuation="0", ProposalCombined="5",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
p5.prob<-fitted$fit

#Predicted probabilities for Project=6
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale=0,
          PreviousRequest="1", IsContinuation="0", ProposalCombined="6",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.ex.step,newdata=newdata)
p6.prob<-fitted$fit

gender.prob.track<-rbind(p1.prob,p2.prob,p3.prob,p4.prob,p5.prob,p6.prob)
rownames(gender.prob.track)<-c("Project1","Project2",'Project3','Project4',
                               'Project5','Project6')
colnames(gender.prob.track)<-c("poor","average","good", "very good",
                               "excellent","outstanding")
gender.prob.track<-t(gender.prob.track)
gender.prob.track<-as.data.frame(gender.prob.track)

p2 <- ggplot(gender.prob.track) +
  geom_line(aes(1:6, Project1, colour = "Project1")) +
  geom_line(aes(1:6, Project2, colour = "Project2"))+
  geom_line(aes(1:6, Project3, colour = "Project3"))+
  geom_line(aes(1:6, Project4, colour = "Project4"))+
  geom_line(aes(1:6, Project5, colour = "Project5"))+
  geom_line(aes(1:6, Project6, colour = "Project6"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("OverallGrade Probability vs Project grade")

grid.arrange(p1,p2,ncol=2)
```

We also computed the predicted probabilities for each of the OverallGrade grades, varying the Applicant Track grade (plot on the left) and the ProposalCombined grade (plot on the right). It is clear that the same variation in grade implies different changes in probabilities: when the project grade changes, the probability variation is much bigger. Notably, in the right graph, the highest probability to the final grade corresponds with the grade given to the ProposalCombined for grades 3, 4, 5 and 6. On the left, receiving a 3, 4 or 5 for the ApplicantTrack record all correspond to most likely receiving an overall grade of 4. This confirms what we found before: the quality of the project has a greater influence on the final Overall Grade, compared to the track record.

### Internal step

In order to check which variable has the biggest influence on the final grade given by the internal referees, we fitted an ordinal regression using the "Ranking" grade as multinomial response and the Applicant Track grade, the Project Assessment grade and some demographic data as predictors. We started as always fitting the full model and we did variable selection using the AIC. The explanatory variables included in the final model are: ApplicantTrack, ProjectAssessment, Gender, Division, PercentFemale, IsContinuation, PreviousRequest and Semester.

```{r data for importance internal step}
internal_regression_data<-prepare_data_internal_log_regression(final.apps,final.internal)
#internal_regression_data$Semester <- revalue(internal_regression_data$Semester, c("ottobre"="Oct", "aprile"="Apr"))
internal_regression_data$Semester <- revalue(internal_regression_data$Semester, c("Oktober"="Oct", "April"="Apr"))
internal_regression_data$logAmount <- log(internal_regression_data$AmountRequested)
data<- subset(internal_regression_data,select = -c(ProjectID, AmountRequested))

# Get grades as factors
data$ApplicantTrack <- factor(data$ApplicantTrack, ordered = F)
data$ProjectAssessment <- factor(data$ProjectAssessment, ordered = F)
data$PercentFemale <- factor(data$PercentFemale)
```

We also computed the predicted probabilities for each of the Ranking grade, varying the Applicant Track grade (plot on the left) and the Project Assessment grade (plot on the right). It is clear that the same variation in grade implies different changes in probabilities: when the project grade changes, the probability variation is much bigger. This confirms what we found before: the quality of the project has a greater influence on the final ranking, compared to the track record.

```{r relative importance internal}
Model.int.step<-clm(Ranking ~ Gender + Division + PercentFemale +
                        ProjectAssessment + ApplicantTrack + IsContinuation +
                        PreviousRequest + Semester, data=data)
# summary(Model.int.step)
```

```{r new CI plot importance internal}
coef.int.step <- coef(Model.int.step)[6:23]
CI.int.step <- confint(Model.int.step)
colors <- rep('not significant', nrow(CI.int.step))
for (i in 1:nrow(CI.int.step)){
  if (CI.int.step[i,1] > 0 & 0 < CI.int.step[i,2]) {colors[i]='significant'}
  if (CI.int.step[i,1] < 0 & 0 > CI.int.step[i,2]) {colors[i]='significant'}
}
lower <- CI.int.step[,1]
upper <- CI.int.step[,2]
df <- data.frame(l = lower, u=upper, col=colors)
axis.names <- rownames(df)

ggplot(data=df)+
  geom_segment(aes(y=as.factor(rownames(df)),yend=as.factor(rownames(df)),x=l, xend=u, colour=col))+
  xlim(-2,22)+ 
  ylab("")+ xlab("")+ggtitle("Confidence Intervals")+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  theme(axis.text.y = element_text(angle=0, hjust=0))+
  geom_vline(xintercept = 0, linetype="dotted")+
  geom_point(aes(y=as.factor(rownames(df)), x=coef.int.step, colour=col))
```

From the plot above we can see that the variable whose confidence intervals are at the biggest distance from 0 is the Project Assessment. The grades given to the track record of the applicant are significant too, since the confidence intervals don't include 0 but we can see that the effect is smaller.

```{r effect plot for track}
X <- Model.int.step$model[,-1]

#Predicted probabilities for track=1
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale="0",
          PreviousRequest="1", IsContinuation="0",ProjectAssessment="4",
          ApplicantTrack="1",Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
at1.prob<-fitted$fit

#Predicted probabilities for track=2
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale="0",
          PreviousRequest="1", IsContinuation="0",ProjectAssessment="4",
          ApplicantTrack="2",Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
at2.prob<-fitted$fit

#Predicted probabilities for track=3
newdata<-data.frame(Gender="f", Division="Div 1",  PercentFemale="0",
          PreviousRequest="1", IsContinuation="0",ProjectAssessment="4",
          ApplicantTrack="3",Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
at3.prob<-fitted$fit

#Predicted probabilities for track=4
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale="0",
          PreviousRequest="1", IsContinuation="0",ProjectAssessment="4",
          ApplicantTrack="4",Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
at4.prob<-fitted$fit

#Predicted probabilities for track=5
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale="0",
          PreviousRequest="1", IsContinuation="0",ProjectAssessment="4",
          ApplicantTrack="5",Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
at5.prob<-fitted$fit

#Predicted probabilities for track=6
newdata<-data.frame(Gender="m", Division="Div 1",  PercentFemale="0",
          PreviousRequest="1", IsContinuation="0",ProjectAssessment="4",
          ApplicantTrack="6",Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
at6.prob<-fitted$fit

gender.prob.track<-rbind(at1.prob,at2.prob,at3.prob,at4.prob,at5.prob,at6.prob)
rownames(gender.prob.track)<-c("Track1","Track2",'Track3','Track4',
                               'Track5','Track6')
colnames(gender.prob.track)<-c("poor","average","good", "very good",
                               "excellent","outstanding")
gender.prob.track<-as.data.frame(t(gender.prob.track))

p1<- ggplot(gender.prob.track) +
  geom_line(aes(1:6, Track1, colour = "Track1")) +
  geom_line(aes(1:6, Track2, colour = "Track2"))+
  geom_line(aes(1:6, Track3, colour = "Track3"))+
  geom_line(aes(1:6, Track4, colour = "Track4"))+
  geom_line(aes(1:6, Track5, colour = "Track5"))+
  geom_line(aes(1:6, Track6, colour = "Track6"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("Ranking Probability vs Track Record")
```


```{r effect plots for project}
#Predicted probabilities for Project=1
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale="0",
          PreviousRequest="1", IsContinuation="0", ProjectAssessment="1",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
p1.prob<-fitted$fit

#Predicted probabilities for Project=2
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale="0",
          PreviousRequest="1", IsContinuation="0", ProjectAssessment="2",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
p2.prob<-fitted$fit

#Predicted probabilities for Project=3
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale="0",
          PreviousRequest="1", IsContinuation="0", ProjectAssessment="3",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
p3.prob<-fitted$fit

#Predicted probabilities for Project=4
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale="0",
          PreviousRequest="1", IsContinuation="0", ProjectAssessment="4",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
p4.prob<-fitted$fit

#Predicted probabilities for Project=5
newdata<-data.frame(Gender="f", Division="Div 1", PercentFemale="0",
          PreviousRequest="1", IsContinuation="0", ProjectAssessment="5",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
p5.prob<-fitted$fit

#Predicted probabilities for Project=6
newdata<-data.frame(Gender="m", Division="Div 1", PercentFemale="0",
          PreviousRequest="1", IsContinuation="0", ProjectAssessment="6",
          ApplicantTrack="4", Semester="Apr")
fitted <- predict(Model.int.step,newdata=newdata)
p6.prob<-fitted$fit

gender.prob.track<-rbind(p1.prob,p2.prob,p3.prob,p4.prob,p5.prob,p6.prob)
rownames(gender.prob.track)<-c("Project1","Project2",'Project3','Project4',
                               'Project5','Project6')
colnames(gender.prob.track)<-c("poor","average","good", "very good",
                               "excellent","outstanding")
gender.prob.track<-t(gender.prob.track)
gender.prob.track<-as.data.frame(gender.prob.track)

p2 <- ggplot(gender.prob.track) +
  geom_line(aes(1:6, Project1, colour = "Project1")) +
  geom_line(aes(1:6, Project2, colour = "Project2"))+
  geom_line(aes(1:6, Project3, colour = "Project3"))+
  geom_line(aes(1:6, Project4, colour = "Project4"))+
  geom_line(aes(1:6, Project5, colour = "Project5"))+
  geom_line(aes(1:6, Project6, colour = "Project6"))+
  theme(legend.title=element_blank(),legend.position = "bottom")+
  xlab("")+ylab("Predicted Probability")+
  ggtitle("Ranking Probability vs Project grade")

grid.arrange(p1,p2,ncol=2)
```

We also computed the predicted probabilities for each of the Ranking grade, varying the Applicant Track grade (plot on the left) and the Project Assessment grade (plot on the right). It is clear that the same variation in grade implies different changes in probabilities: when the project grade changes, the probability variation is much bigger. This confirms what we found before: the quality of the project has a greater influence on the final ranking, compared to the track record.


# Conclusion

# Appendix

## Detailed Data Description

### Applications
  * **AmountRequested**: Rounded to the next 10k CHF
  * **AmountGranted**: Rounded to the next 10k CHF
  * **IsApproved**: 1 if the application was approved, 0 if it was rejected
  * **GradeFinal**: Comparative ranking of the application as determined by the evaluation body (the division of the National Research Council). A: "belongs to the 10% best percent"; AB: "10% are worse, 75% are better"; B: "50% are worse, 25% are better"; BC: " 25% are worse, 50% are better"; C "10% are worse, 75% are better"; D: "90% of the applications are better"
  
  * **Division**: Evaluation Body in which the application was evaluated. Division 1 evaluates Social Sciences and Humanities; Division 2 Mathematics, Natural Sciences and Engineering; Division 3 Biology and Medicine
  
  * **MainDiscipline**: as chosen by the applicant from the SNSF discipline list
  * **MainDisciplineLevel2**: category in the SNF discipline list grouping disciplines into fields of research
  * **CallTitle**: Call for proposals under which the application was submitted. Applications from the same Call are evaluated together, i.e. in competition to each other
  * **CallEndDate**: Submission deadline of the Call
  * **ResponsibleApplicantAcademicAgeAtSubmission**: Years since the applicant's PhD at time of submission; data only available since mid 2016
  * **ResponsibleApplicantAgeAtSubmission**: Biological age of the applicant at time of submission; data only available since mid 2016
  * **ResponsibleApplicantProfessorshipType**: employment situation of the applicant at time of submission; data only available since mid 2016
  * **Gender**: of the main applicant
  * **NationalityIsoCode**: Nationality of the main applicant
  * **IsHasPreviousProjectRequested**: 0 if it is the applicant's first application at the SNSF, 1 if not
  * **InstType**: Type of institution where the applicant is employed
  * **IsContinuation**: 1 if the project is a thematic continuation of a previously approved project, 0 if not
  * **ProjectID**: Anonymized identifier of the application

### Referee Grades
  * **Question**: Evaluation criterion
  * **QuestionRating**: The (co-)referee's assessment of the evaluation criterion
  * **OverallRanking**: The (co-)referee's overall comparative ranking of the application. A: "belongs to the 10% best percent"; same scale as the GradeFinal
  * **RefereeRole**: Some applications have one referee evaluation, some have two. The role indicates who was the primary and who was the secondary referee (also called co-referee) 
  * **RefereeGender**
  * **IDs**: Anonymized identifiers of the application, the referee and the evaluation by the referee

### Reviews
  * **Question**: Evaluation criterion
  * **QuestionRating**: The external reviewer's assessment of the evaluation criterion
  * **OverallGrade**: The external reviewer's overall assessment of the application
  * **SourcePerson**: Who suggested the reviewer?
  * **Gender**
  * **Country**: where the reviewer is located. Not always known
  * **EmailEnding**: ending of the reviewer's email address. Might be used as an approximation of the country where the reviewer is located in cases where this data is missing
  * **IDs**: Anonymized identifiers of the application, the reviewer and the review
   
## Exploratory Analysis

### OverallGrade vs. IsApproved, by Division


```{r}
p5 <- plot_mirror_barplot(dataset=ex.div1, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Division 1")
p6 <- plot_mirror_barplot(dataset=ex.div2, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Division 2")
p7 <- plot_mirror_barplot(dataset=ex.div3, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Division 3")


p5
p6
p7


```

### OverallGrade vs. IsApproved, by Gender


```{r}

p8 <- plot_mirror_barplot(dataset=ex.f, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Female Applicants")
p9 <- plot_mirror_barplot(dataset=ex.m, 
                          variable1 = "OverallGrade", plot_title = "OverallGrade vs. Approved; Male Applicants")

p8
p9
``` 
\newpage

## External Logistic Regression

* Summary of the final model
```{r}
summary(Model.log.ext)
```
```{r plot-effects, out.width='50%',fig.show="hold",fig.ncol=2, fig.cap="Effects of the external logistic regression", eval=FALSE}

plot(Effect("ApplicantTrack",Model.log.ext),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("ScientificRelevance",Model.log.ext), ### COMMENTED BC ERROR
        confint=list(style="band",alpha=0.3,col="grey"),
        lines=list(col=1))
plot(Effect("Suitability",Model.log.ext),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("PercentFemale",Model.log.ext),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("Age",Model.log.ext),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("Division",Model.log.ext),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("IsContinuation",Model.log.ext),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
plot(Effect("Semester",Model.log.ext),
       confint=list(style="band",alpha=0.3,col="grey"),
       lines=list(col=1))
```

### Ordinal external Regression

#### Project grades (ProposalCombined)

Summary of the final model:
    ```{r}
summary(Model.Prop)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.Prop$beta,confint(Model.Prop))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```
\pagebreak

#### Applicant Track
Summary of the final model:
```{r}
summary(Model.App)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.App$beta,confint(Model.App))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```


#### Overall Grade
Summary of the final model:
```{r}
summary(Model.Overall)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.Overall$beta,confint(Model.Overall))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```

## Internal Logistic Regression

* Summary of the final model
```{r}
summary(Model.log.int)
```

* Effect plots

```{r plot-effects-int, out.width='50%',fig.show="hold",fig.ncol=2, fig.cap="Effects of the external logistic regression"}
# LESLIE FIXXXX!!!!!
# plot(Effect("ApplicantTrack",Model.log.int),
#        confint=list(style="band",alpha=0.3,col="grey"),
#        lines=list(col=1))
# plot(Effect("ProjectAssessment",Model.log.int),
#        confint=list(style="band",alpha=0.3,col="grey"),
#        lines=list(col=1))
# plot(Effect("PercentFemale",Model.log.int),
#        confint=list(style="band",alpha=0.3,col="grey"),
#        lines=list(col=1))
# plot(Effect("Age",Model.log.int),
#        confint=list(style="band",alpha=0.3,col="grey"),
#        lines=list(col=1))
# plot(Effect("IsContinuation",Model.log.int),
#        confint=list(style="band",alpha=0.3,col="grey"),
#        lines=list(col=1))
# plot(Effect("Semester",Model.log.int),
#        confint=list(style="band",alpha=0.3,col="grey"),
#        lines=list(col=1))
```

### Ordinal internal Regressions

#### Project Assessment

Summary of the final model:
    ```{r}
summary(Model.project)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.project$beta,confint(Model.project))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```
\pagebreak

#### Applicant Track
Summary of the final model:
```{r}
summary(Model.track)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.track$beta,confint(Model.track))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```


#### Ranking
Summary of the final model:
```{r}
summary(Model.Rank)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.Rank$beta,confint(Model.Rank))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```

\pagebreak

## Relative importance within Internal step
Summary of the final model:
```{r}
summary(Model.int.step)
```

Odd Ratios and Confidence intervals:

```{r}
kable(round(exp(cbind(OR=Model.int.step$beta,confint(Model.int.step))), 2),"latex", booktabs = T) %>%
  kable_styling(position = "center")
```
