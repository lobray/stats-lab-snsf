ind.x <- select.x(x.train, y.train, q)
x.train <- x.train[,ind.x]
x.test <- x.test[,ind.x]
}
y.pred <- knn1(x.train, x.test, y.train)
error[i] <- sum(y.pred != y.test)
}
return(sum(error/n))
}
# assess performance of 1 NN classifier via K-fold cross validation,
#    after pre-selection.
# replicate this nr.cv times (so nr.cv times to determine the folds)
cv.wrong <- replicate(nr.cv, cv.knn1(x.new,y,K=10,preselect=FALSE))
# Cross-validation:
# Package for knn classification:
library(class)
# Select variables with highest marginal correlations.
# Inputs:
#   x: predictors
#   y: dependent variable
#   q: nr of variables to be selected
# Outputs:
#   indices of q columns of x with highest marginal correlation with y
select.x <- function(x,y,q){
# some simple checks on input values:
stopifnot(is.matrix(x), nrow(x)==length(y), q>=1, q<=ncol(x))
# compute cor(x[,i],y) for i=1,..,p:
cor.vec <- apply(x,2,cor,y=y)
# determine indices of variables, so that the absolute value of
#   their correlation with y is sorted in decreasing order:
ind <- order(abs(cor.vec), decreasing=TRUE)
# return indices of q variables with largest absolute correlation
return(ind[1:q])
}
# Conduct K-fold cross-validation for 1 NN classifier.
# Inputs:
#   x: predictors
#   y: dependent variable
#   K: nr of folds in cross-validation
#   preselect: boolean indicating whether pre-selection should be
#              conducted *within* cross validation. Defaults to FALSE.
#   q: nr of variables to be pre-selected. Defaults to 10.
# Outputs:
#   error rate.
cv.knn1 <- function(x,y,K,preselect=FALSE,q=10){
# quick checks of input:
stopifnot(is.matrix(x), nrow(x)==length(y), 1<=K, K<=nrow(x),
q>=1, q<=ncol(x))
# randomly shuffle the rows:
n <- length(y)
ind.x <- sample(n, replace=FALSE)
x <- x[ind.x,]
y <- y[ind.x]
# create K (roughly) equally sized folds:
folds <- cut(seq(1,n),breaks=K,labels=FALSE)
# perform K fold cross validation:
error <- integer(K)
for(i in 1:K){
# Segment data by fold using the which() function
ind.test <- which(folds==i, arr.ind=TRUE)
x.test <- x[ind.test,]
y.test <- y[ind.test]
x.train <- x[-ind.test,]
y.train <- y[-ind.test]
# if preselect==TRUE, conduct selection of x variables inside CV
if (preselect==TRUE){
ind.x <- select.x(x.train, y.train, q)
x.train <- x.train[,ind.x]
x.test <- x.test[,ind.x]
}
y.pred <- knn1(x.train, x.test, y.train)
error[i] <- sum(y.pred != y.test)
}
return(sum(error/n))
}
###
set.seed(123)
n <- 50       # sample size
p <- 5000     # nr of predictors
q <- 20       # nr of pre-selected predictors
K <- 10       # nr of folds in cross validation
nr.cv <- 50   # nr of K-fold cross validations that are performed
#   we do this to see how much the results depend
#   on the way in which the folds were chosen
nsim <- 50    # nr of data sets that are simulated
#   we do this to see how much the results vary over
#   different data sets
### Conduct cross-validation the right and wrong way for one data set:
# create high-dimensional data
x <- matrix(rnorm(n*p),nrow=n)
y <- c(rep(0,n/2),rep(1,n/2))
# note that x contains no information about y!
# pre-select q columns of x with strongest marginal correlation with y
x.new <- x[,select.x(x,y,q)]
# assess performance of 1 NN classifier via K-fold cross validation,
#    after pre-selection.
# replicate this nr.cv times (so nr.cv times to determine the folds)
cv.wrong <- replicate(nr.cv, cv.knn1(x.new,y,K=10,preselect=FALSE))
plot(cv.wrong, ylim=c(0,1), ylab="CV error rate",
xlab="Iteration of K-fold CV, keeping data fixed",
main="CV estimate of error rate; feature selection before CV")
abline(h=mean(cv.wrong),lty=2)
abline(h=0.5,col="blue")
legend("topleft",c("truth","mean of estimated error rates"),
col=c("blue","black"),lty=c(1,2))
# assess performance of 1 NN classifier, using pre-selection within CV
cv.correct <- replicate(nr.cv, cv.knn1(x,y,K=10,preselect=TRUE,q=20))
# assess performance of 1 NN classifier, using pre-selection within CV
cv.correct <- replicate(nr.cv, cv.knn1(x,y,K=10,preselect=TRUE,q=20))
knitr::opts_chunk$set(echo = TRUE)
setwd("~/StatLab")
load("snsf_data.RData")
source("Cleaning Functions.R")
source("Data for Regression.R")
library(vcd)
library(corrplot)
library(caTools)
library(ROCR)
library(pROC)
library(car)
rm(applications, external_reviews, referee_grades)
rm(apps, internal_reviews, reviews)
internal_regression_data<-prepare_data_internal_log_regression(final.apps,final.internal)
internal_regression_data$Semester <- revalue(internal_regression_data$Semester, c("ottobre"="Oct", "aprile"="Apr"))
levels(internal_regression_data$Semester)
split_data <- function(data,Div="All",SplitRatio=0.8){
data<- data[,-1]
if (Div == "All"){
final.data<- data
}
else {
final.data<- subset(data,Division==Div, select = -(Division))
}
#spliting the data into train and test set
set.seed(123)
if (SplitRatio<1){
split<-sample.split(final.data$IsApproved, SplitRatio = SplitRatio)
Train<-subset(final.data, split=="TRUE")
Test <-subset(final.data, split=="FALSE")
} else {
Test<-Train<-final.data
}
return(list(data=final.data,Train.data=Train,Test.data=Test))
}
data_for_model <- split_data(internal_regression_data)
InternalReviewModel<- function(data,Div="All",
SplitRatio=0.8, cutoff=0.5 ){
Train <- data$Train.data
Test <- data$Test.data
# fitting the model
Model <- glm(Train$IsApproved ~. -Ranking+ Age:ApplicantTrack+Gender:Division+ApplicantTrack:Gender-InstType ,data=Train,
family="binomial")
# Testing the Treshold
par(mfrow=c(1,2))
predictor<-predict(Model, Test, type="response")
ROCRPred<- prediction(predictor,Test$IsApproved)
ROCRPerf<- performance(ROCRPred,"tpr","fpr")
plot(ROCRPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1))
# with respect to accuracy
ROCRACC<- performance(ROCRPred,"acc")
plot(ROCRACC)
# Confusion Matrices
AccTable<-table(ActualValue=Test$IsApproved,Prediction=predictor>=cutoff)
accuracy<-(sum(diag(AccTable))/sum(AccTable))
# Return
print(paste("Regresion for Internal Reviews.   ", "Division: ", Div))
par(mfrow=c(1,1))
Res.data<-list(Train,Test)
return(list(Regression=Model,
Model= summary(Model),
`Confidence Intervals`=confint(Model),
`Confusion Matrix`=AccTable,
`Percentage of data used for Training`=paste(SplitRatio*100,"%"),
Accuracy=paste(round(accuracy,2)*100,"%")))
}
fit <- InternalReviewModel(data_for_model)
fit
c.gender <- coef(fit)[2]
fit <- InternalReviewModel(data_for_model)
fit
c.gender <- coef(fit)[2]
c.gender <- coeff(fit)[2]
c.gender <- coefficients(fit)[2]
c.gender <- coef(fit$Regression)[2]
fit <- InternalReviewModel(data_for_model)
fit
c.gender <- coef(fit$Regression)[2]
fit <- InternalReviewModel(data_for_model)
fit
c.gender <- coef(fit$Regression)[2]
c.cont <- coef(fit$Regression)[7]
c.appltrack <- coef(fit$Regression)[11]
c.project <- coef(fit$Regression)[10]
internal_regression_data<-prepare_data_internal_log_regression(final.apps,final.internal)
internal_regression_data$ProjectAssessment <- factor(internal_regression_data$ProjectAssessment)
Model <- polr(ProjectAssessment ~ Gender+Division+InstType+IsContinuation+Semester+
log(AmountRequested)+PercentFemale,
method="logistic", data=internal_regression_data)
library(MASS)
Model <- polr(ProjectAssessment ~ Gender+Division+InstType+IsContinuation+Semester+
log(AmountRequested)+PercentFemale,
method="logistic", data=internal_regression_data)
summary(Model)
split<-sample.split(internal_regression_data$ProjectAssessment, SplitRatio = 0.8)
Train<-subset(internal_regression_data, split=="TRUE")
Test <-subset(internal_regression_data, split=="FALSE")
InternalMultiModel<- function(data=internal_regression_data, train=Train,
test=Test,SplitRatio = 0.8){
# fitting the model
Model <- polr(ProjectAssessment ~ Gender+Division+InstType+IsContinuation+
log(AmountRequested)+PercentFemale,
method="logistic", data=Train)
predictor<-predict(Model, Test, type="class")
# Confusion Matrices
AccTable<-table(ActualValue=Test$ProjectAssessment,Prediction=predictor)
accuracy<-(sum(diag(AccTable))/sum(AccTable))
# Return
print(paste("Regresion for Internal Reviews.   "))
return(list(Model= summary(Model),
#`Confidence Intervals`=confint(Model),
`Confusion Matrix`=AccTable,
`Percentage of data used for Training`=paste(SplitRatio*100,"%"),
Accuracy=paste(round(accuracy,2)*100,"%")))
}
(Model_fitted <- InternalMultiModel())
drop1(Model)
split<-sample.split(internal_regression_data$ProjectAssessment, SplitRatio = 0.8)
Train<-subset(internal_regression_data, split=="TRUE")
Test <-subset(internal_regression_data, split=="FALSE")
InternalMultiModel<- function(data=internal_regression_data, train=Train,
test=Test,SplitRatio = 0.8){
# fitting the model
#Model <- polr(ProjectAssessment ~ Gender+Division+InstType+IsContinuation+
#                log(AmountRequested)+PercentFemale,
#                method="logistic", data=Train)
#drop1(Model)
Model <- polr(ProjectAssessment ~ Division+InstType+IsContinuation+
log(AmountRequested)+PercentFemale,
method="logistic", data=Train)
predictor<-predict(Model, Test, type="class")
# Confusion Matrices
AccTable<-table(ActualValue=Test$ProjectAssessment,Prediction=predictor)
accuracy<-(sum(diag(AccTable))/sum(AccTable))
# Return
print(paste("Regresion for Internal Reviews.   "))
return(list(Model= summary(Model),
#`Confidence Intervals`=confint(Model),
`Confusion Matrix`=AccTable,
`Percentage of data used for Training`=paste(SplitRatio*100,"%"),
Accuracy=paste(round(accuracy,2)*100,"%")))
}
(Model_fitted <- InternalMultiModel())
knitr::opts_chunk$set(echo = TRUE)
setwd("~/StatLab")
load("snsf_data.RData")
source("Cleaning Functions.R")
source("Data for Regression.R")
library(vcd)
library(corrplot)
library(caTools)
library(ROCR)
library(pROC)
library(car)
rm(applications, external_reviews, referee_grades)
rm(apps, internal_reviews, reviews)
library(vcd)
library(corrplot)
library(caTools)
library(ROCR)
library(pROC)
library(car)
rm(applications, external_reviews, referee_grades)
rm(apps, internal_reviews, reviews)
internal_regression_data<-prepare_data_internal_log_regression(final.apps,final.internal)
setwd("~/StatLab")
load("snsf_data.RData")
source("Cleaning Functions.R")
source("Data for Regression.R")
setwd("~/StatLab")
load("snsf_data.RData")
source("Cleaning Functions.R")
source("Data for Regression.R")
library(vcd)
library(corrplot)
library(caTools)
library(ROCR)
library(pROC)
library(car)
internal_regression_data<-prepare_data_internal_log_regression(final.apps,final.internal)
rm(applications, external_reviews, referee_grades)
rm(apps, internal_reviews, reviews)
internal_regression_data$Semester <- revalue(internal_regression_data$Semester, c("ottobre"="Oct", "aprile"="Apr"))
levels(internal_regression_data$Semester)
split_data <- function(data,Div="All",SplitRatio=0.8){
data<- data[,-1]
if (Div == "All"){
final.data<- data
}
else {
final.data<- subset(data,Division==Div, select = -(Division))
}
#spliting the data into train and test set
set.seed(123)
if (SplitRatio<1){
split<-sample.split(final.data$IsApproved, SplitRatio = SplitRatio)
Train<-subset(final.data, split=="TRUE")
Test <-subset(final.data, split=="FALSE")
} else {
Test<-Train<-final.data
}
return(list(data=final.data,Train.data=Train,Test.data=Test))
}
data_for_model <- split_data(internal_regression_data)
InternalReviewModel<- function(data,Div="All",
SplitRatio=0.8, cutoff=0.5 ){
Train <- data$Train.data
Test <- data$Test.data
# fitting the model
Model <- glm(Train$IsApproved ~. -Ranking+ Age:ApplicantTrack+Gender:Division+ApplicantTrack:Gender-InstType ,data=Train,
family="binomial")
# Testing the Treshold
par(mfrow=c(1,2))
predictor<-predict(Model, Test, type="response")
ROCRPred<- prediction(predictor,Test$IsApproved)
ROCRPerf<- performance(ROCRPred,"tpr","fpr")
plot(ROCRPerf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1))
# with respect to accuracy
ROCRACC<- performance(ROCRPred,"acc")
plot(ROCRACC)
# Confusion Matrices
AccTable<-table(ActualValue=Test$IsApproved,Prediction=predictor>=cutoff)
accuracy<-(sum(diag(AccTable))/sum(AccTable))
# Return
print(paste("Regresion for Internal Reviews.   ", "Division: ", Div))
par(mfrow=c(1,1))
Res.data<-list(Train,Test)
return(list(Regression=Model,
Model= summary(Model),
`Confidence Intervals`=confint(Model),
`Confusion Matrix`=AccTable,
`Percentage of data used for Training`=paste(SplitRatio*100,"%"),
Accuracy=paste(round(accuracy,2)*100,"%")))
}
fit <- InternalReviewModel(data_for_model)
fit
c.gender <- coef(fit$Regression)[2]
c.cont <- coef(fit$Regression)[7]
c.appltrack <- coef(fit$Regression)[11]
c.project <- coef(fit$Regression)[10]
c.gender <- coef(fit$Regression)[2]
c.cont <- coef(fit$Regression)[7]
c.appltrack <- coef(fit$Regression)[11]
c.project <- coef(fit$Regression)[10]
coef(fit$Regression)[2]
c.gender <- exp(coef(fit$Regression)[2])
exp(coef(fit$Regression)[2])
(1-exp(coef(fit$Regression)[2]))*100
coef(fit$Regression)[2]
coef(fit$Regression)[7]
exp(coef(fit$Regression)[7])
c.cont <- (exp(coef(fit$Regression)[7])-1)*100
fit
c.appltrack <- coef(fit$Regression)[11]
exp(coef(fit$Regression)[11])
c.appltrack <- exp(coef(fit$Regression)[11])
fit
c.project <- coef(fit$Regression)[12]
fit
c.project <- exp(coef(fit$Regression)[12])
Train <- data_for_model$Train.data
fvl <- predict(Model, type="link")
Model<-fit$Regression
drop1(Model, test="Chisq")
Train <- data_for_model$Train.data
fvl <- predict(Model, type="link")
fpr <- predict(Model, type="response")
IsApproved<-Train$IsApproved
plot(fvl, IsApproved, type="n", xlab="linear predictor", ylab="Approval Result")
points(fvl[IsApproved==0], IsApproved[IsApproved==0])
points(fvl[IsApproved==1], IsApproved[IsApproved==1], col="red")
lines(sort(fvl+1), sort(fpr+1), lty=3)
title("Result vs. Linear Predictor")
par(mfrow=c(1,2))
Train <- data_for_model$Train.data
fvl <- predict(Model, type="link")
fpr <- predict(Model, type="response")
IsApproved<-Train$IsApproved
plot(fvl, IsApproved, type="n", xlab="linear predictor", ylab="Approval Result")
points(fvl[IsApproved==0], IsApproved[IsApproved==0])
points(fvl[IsApproved==1], IsApproved[IsApproved==1], col="red")
lines(sort(fvl+1), sort(fpr+1), lty=3)
title("Result vs. Linear Predictor")
xx <- fvl
yy <- residuals(Model, type="deviance")
plot(xx, yy, pch=20, main="Tukey-Anscombe Plot")
lines(loess.smooth(xx, yy, family="gaussian"), col="red")
abline(h=0, lty=3, col="grey")
pseudoR <- (1-exp((Model$dev-Model$null)/nrow(Train)))/(1-exp(-Model$null/nrow(Train)))
pseudoR <- (1-exp((Model$dev-Model$null)/nrow(Train)))/(1-exp(-Model$null/nrow(Train)))
install.packages("effects")
library(effects)
library(effects)
plot(allEffects(fit))
plot(allEffects(Model))
library(effects)
plot(Effects(c=("IsApproved","Gender")))
plot(Effects(c("IsApproved","Gender")))
plot(Effect(c("IsApproved","Gender")))
plot(Effect(c("IsApproved","Gender"), mod=Model))
plot(Effect(c("ApplicantTrack","Gender"), mod=Model))
plot(Effect("Gender", mod=Model))
plot(Effect(c("Gender","ApplicantTrack"), mod=Model))
plot(Effect(c("ApplicantTrack"), mod=Model))
plot(Effect("ProjectAssessment", mod=Model))
plot(Effect(c("Gender","ApplicantTrack"), mod=Model))
plot(Effect(c("Gender","ProjectAssessment"), mod=Model))
plot(Effect("AmountRequested", mod=Model))
plot(Effect("log(AmountRequested)", mod=Model))
plot(Effect("AmountRequested", mod=Model))
plot(Effect("Division", mod=Model))
plot(Effect("Age",mod=Model))
plot(Effect("InstType", mod=Model))
plot(Effect("PercentFemale",mod=Model))
plot(Effect("IsContinuation",mod=Model))
plot(Effect("PreviousRequest",mod=Model))
plot(Effect("Semester"),mod=Model)
plot(Effect("Semester",mod=Model))
setwd("~/StatLab")
load("snsf_data.RData")
source("Cleaning Functions.R")
source("Data for Regression.R")
library(ggplot2)
rm(applications,reviews,referee_grades, test)
ggplot(internal_regression_data,aes(x=ApplicantTrack,y=Ranking,fill=factor(Gender)))+
geom_bar(stat="identity",position="dodge")+
scale_fill_discrete(name="OverallGrade",
breaks=c(1, 2),
labels=c("Male", "Female"))+
xlab("OverallGrade")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
ggtitle("Difference in how Female & Male Allocate OverallGrades to Male Applicants")
internal_regression_data<-prepare_data_internal_log_regression(final.apps,final.internal)
ggplot(internal_regression_data,aes(x=ApplicantTrack,y=Ranking,fill=factor(Gender)))+
geom_bar(stat="identity",position="dodge")+
scale_fill_discrete(name="OverallGrade",
breaks=c(1, 2),
labels=c("Male", "Female"))+
xlab("OverallGrade")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
ggtitle("Difference in how Female & Male Allocate OverallGrades to Male Applicants")
ggplot(internal_regression_data,aes(x=ApplicantTrack,y=Ranking,fill=factor(Gender)))+
scale_fill_discrete(name="OverallGrade",
breaks=c(1, 2),
labels=c("Male", "Female"))+
xlab("OverallGrade")+ylab("Proportion of OverallGrades allocated by Male & Female Reviewers") +
ggtitle("Difference in how Female & Male Allocate OverallGrades to Male Applicants")
ggplot(internal_regression_data, aes(x=ApplicantTrack, y=Ranking)) + geom_point()
+ geom_point(size=2, shape=23)
+ geom_point(color=factor(Gender))
+ geom_point(color=factor(internal_regression_data$Gender))
ggplot(internal_regression_data, aes(x = ApplicantTrack, y = Ranking, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
ggplot(internal_regression_data, aes(x = ApplicantTrack, y = Ranking, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Difference in how Female & Male Allocate OverallGrades to Male Applicants")
ggplot(internal_regression_data, aes(x = ApplicantTrack, y = Ranking, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Applicant Track and Ranking")
## Project Assessment
ggplot(internal_regression_data, aes(x = ProjectAssessment, y = Ranking, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Project Assessment and Ranking")
load("snsf_data.RData")
external_regression_data<-prepare_data_internal_log_regression(final.apps,final.external)
external_regression_data<-prepare_data_external_log_regression(final.apps,final.external)
external_regression_data<-prepare_data_external_log_regression(final.apps,final.external)
## Applicant Track
ggplot(internal_regression_data, aes(x = ApplicantTrack, y = OverallGrade, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Applicant Track and Ranking")
## Applicant Track
ggplot(external_regression_data, aes(x = ApplicantTrack, y = OverallGrade, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Applicant Track and Ranking")
## Suitability
ggplot(external_regression_data, aes(x = Suitability, y = OverallGrade, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Project Assessment and Ranking")
## Scientific Relevance
ggplot(external_regression_data, aes(x = ScientificRelevance, y = OverallGrade, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Project Assessment and Ranking")
## ProposalCombined
ggplot(external_regression_data, aes(x = ProposalCombined, y = OverallGrade, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Project Assessment and Ranking")
str(external_regression_data)
# Get the Regression data
external_regression_data<-prepare_data_external_log_regression(final.apps,final.external)
external_regression_data$ProposalCombined <- factor(external_regression_data$ProposalCombined, ordered = T)
setwd("~/GitHub/stats-lab-snsf")
source("Cleaning Functions.R")
source("Data for Regression.R")
external_regression_data<-prepare_data_external_log_regression(final.apps,final.external)
## ProposalCombined
ggplot(external_regression_data, aes(x = ProposalCombined, y = OverallGrade, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Project Assessment and Ranking")
## Scientific Relevance
ggplot(external_regression_data, aes(x = ScientificRelevance, y = OverallGrade, col=Gender)) +
geom_jitter(alpha = .5) +  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
ggtitle("Agreement between Project Assessment and Ranking")
knitr::opts_chunk$set(echo = TRUE)
pseudoR <- (1-exp((Model$dev-Model$null)/1623))/(1-exp(-Model$null/1623))
